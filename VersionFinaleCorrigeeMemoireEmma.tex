\documentclass[11pt,a4paper,oneside]{book}
\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{algorithm}
\usepackage[french]{minitoc}
\usepackage{lmodern}
\usepackage{fancybox}
\usepackage[thmmarks,amsmath]{ntheorem} 
\usepackage{tocbibind}
\usepackage{lscape}
\usepackage{tikz}
\usepackage{frame}
\usepackage{fancyhdr}
\usepackage{algorithmic}
\usepackage{titlesec}
\usepackage{listings}
\usetikzlibrary{shadows,shapes,arrows,positioning,shapes.geometric}
\usepackage{caption,subcaption}
\usepackage{graphicx}
\usepackage{hyperref}
%\usepackage[final]{pdfpages} 
\usepackage{tablists}
\floatname{algorithm}{\textsc{Algorithme}}
\usepackage[babel=true,kerning=true]{microtype}
\usepackage[left=3cm,right=3cm,bottom=3cm,top=3cm]{geometry}
\usepackage{xcolor}

\pagestyle{fancy}
\fancyhead{}
\setcounter{section}{0}
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}
\newcounter{subsubsubsection}[subsubsection]
\hypersetup{colorlinks=true, linkcolor=violet}

\author{Emmanuel Kagarabi, L2 Maths}
\title{Projet de mémoire}

\usepackage{sectsty}
%\sectionfont{\color{violet}{}}
%\subsectionfont{\color{cyan}{}}
%\subsubsectionfont{\color{brown}{}}

\titleformat{\chapter}[frame]% 
{\titleline[r]{}\normalfont}%
{\filright{}% 
	\textsc{Chapitre \thechapter~}}%
{12pt}{\Huge\bfseries\filcenter}{}


{\theorembodyfont{\sffamily} 
	\theoremheaderfont{\scshape} 
	\newtheorem{prop}{Proposition}[chapter]
	\newtheorem{theo}{Théorème}[chapter]
	\newtheorem{cor}{Corollaire}[chapter]
	\newtheorem{lem}{Lemme}[chapter]
	%\newtheorem{algorithm}{Algorithme}[chapter]}

{\theoremstyle{plain}
	\RequirePackage{latexsym}
	\theorembodyfont{\itshape}
	\theoremheaderfont{\scshape}
	\theoremseparator{}
	\newtheorem{defi}{Définition}[chapter]
	\newtheorem{propp}{Propriété}[chapter]
	\newtheorem{rem}{Remarque}[chapter]
	\newtheorem{notation}{Notation}[chapter]
	\newtheorem{ex}{Exemple}[chapter]}


{\theoremstyle{nonumberplain}
	\RequirePackage{amssymb}
	\theorembodyfont{\normalfont}
	\theoremheaderfont{\scshape}
	\theoremsymbol{\ensuremath{_\blacksquare}}
	\newtheorem{proof}{Démonstration}
	\theoremclass{LaTeX}}

\newcommand{\encad}[1]{%
 \fbox{\begin{minipage}[t]{\linewidth}%
  #1\end{minipage}}}

\def\B{\mathfrak B}
\def\N{\mathbb N}
\def\Z{\mathbb Z}
\def\G{\mathfrak G}
\def\R{\mathbb R}
\def\U{\mathfrak U}
\def\er{\mathcal R}
\def\A{\mathcal A}
\def\cey{\mathcal C}
\def\C{\mathbb C}
\def\K{\mathbb K}
\def\M{\mathcal M}
\def\L{\mathcal L}
\def\op{\mathcal O}
\def\W{\mathcal W}
\def\I{\mathbf I}
\def\V{\mathcal V}
\def\Bb{\mathcal B}
\def\vey{\mathbf V}
\def\eL{\mathscr L}

\def\tensor{\overset{d}{\underset{j=1}{\otimes}}}
\def\ov{\mathbf 0}
\def\at{\otimes_{a}}
\def\gtensor{\overset{d}{\underset{j=1}{\bigotimes}}}
\def\atensor{_{a}\overset{d}{\underset{j=1}{\bigotimes}}}
\def\Ed{\overset{d}{\underset{j=1}{\times}}}
\def\agktensor{_{a}\overset{d}{\underset{k=1}{\bigotimes}}}
\def\Rn{{\mathbb R}^n}
\def\Rnn{{\mathbb R}^{n \times n}}
\def\ktensor{\overset{d}{\underset{k=1}{\otimes}}}
\def\a{\mathbf a}
\def\b{\mathbf b}
\def\s{\mathbf s}
\def\v{\mathbf v}
\def\u{\mathbf u}
\def\e{\mathbf e}
\def\x{\mathbf x}
\def\y{\mathbf y}
\def\z{\mathbf Z}
\def\mo{\rm mod.}
\def\w{\mathbf w}
\def\G{\mathbf G}
\def\RItd{\mathbb{R}^{I_1 \times I_2 \times \ldots \times I_d}}
\def\gktensor{\overset{d}{\underset{k=1}{\bigotimes}}}
\def\RItn{\mathbb{R}^{I_1 \times I_2 \times \ldots \times I_N}}

\newcommand{\f}[2]{\dfrac{#1}{#2}}
\newcommand{\pf}[2]{\frac{#1}{#2}}

\def\argmin{\mathop{\rm argmin }}
\def\spam{\rm Vect}
\def\liminf{\mathop{\rm liminf }}
\def\limsup{\mathop{\rm limsup }}
\newcommand{\mbf}[1]{\mathbf{#1}}
\newcommand{\figref}[1]{\textsc{Figure}~\ref{#1}}
\newcommand{\tabref}[1]{\textsc{Table}~\ref{#1}}
\newcommand{\secref}[1]{Section~\ref{#1}}
\newcommand{\algorithmref}[1]{\textsc{Algorithme}~\ref{#1}}
\newcommand{\defref}[1]{\textsc{Définition}~\ref{#1}}
\newcommand{\propref}[1]{\textsc{Proposition}~\ref{#1}}
\newcommand{\exoref}[1]{\textsc{Exemple}~\ref{#1}}
\newcommand{\theoref}[1]{\textsc{Théorème}~\ref{#1}}
\newcommand{\proppref}[1]{\textsc{Propriété}~\ref{#1}}
\newcommand{\lemref}[1]{\textsc{Lemme}~\ref{#1}}
\newcommand{\corref}[1]{\textsc{Corollaire}~\ref{#1}}
\newcommand{\remref}[1]{\textsc{Remarque}~\ref{#1}}


\renewcommand\labelitemi{\circ}
\definecolor{dkgreen}{rgb}{0,0.55,0}
\definecolor{dkred}{rgb}{0.70,0,0}
\definecolor{dkblue}{rgb}{0,0,0.65}
\definecolor{dktoto}{rgb}{0.80,0.80,0}
\definecolor{dktata}{rgb}{0.80,0,0.80}
\definecolor{dkgray}{rgb}{0.80,0.80,0.80}
\newcommand{\cgr}[1]{\textcolor{dkgreen}{#1}}
\newcommand{\cre}[1]{\textcolor{dkred}{#1}}
\newcommand{\cdb}[1]{\textcolor{dkblue}{#1}}
\newcommand{\cbl}[1]{\textcolor{dkblue}{#1}}
\newcommand{\cbk}[1]{\textcolor{black}{#1}}
\newcommand{\ctoto}[1]{\textcolor{dktoto}{#1}}
\newcommand{\ctata}[1]{\textcolor{dktata}{#1}}
\newcommand{\crel}[1]{\textcolor{red}{#1}}

	\newcommand{\cuboid}[7]{
	% bottom
	\draw[#7] (#1, #2, #3) -- ++(#4, 0, 0) -- ++(0, 0, #6) -- ++(-#4, 0, 0) -- cycle;
	% back
	\draw[#7] (#1, #2, #3) -- ++(#4, 0, 0) -- ++(0, #5, 0) -- ++(-#4, 0, 0) -- cycle;
	% left
	\draw[#7] (#1, #2, #3) -- ++(0, 0, #6) -- ++(0, #5, 0) -- ++(0, 0, -#6) -- cycle;
	% right
	\draw[#7] (#1+#4, #2, #3) -- ++(0, 0, #6) -- ++(0, #5, 0) -- ++(0, 0, -#6) -- cycle;
	% front
	\draw[#7] (#1, #2, #3+#6) -- ++(#4, 0, 0) -- ++(0, #5, 0) -- ++(-#4, 0, 0) -- cycle;
	% top
	\draw[#7] (#1, #2+#5, #3) -- ++(#4, 0, 0) -- ++(0, 0, #6) -- ++(-#4, 0, 0) -- cycle;
}

\newcommand{\sliceM}[6]{
	%front to back and vice versa
	\draw[#6] (#1, #2,#3) -- ++(#4, 0,0) -- ++(0, #5,0) -- ++(-#4, 0,0) -- cycle;
}
\newcommand{\sliceMm}[6]{
	% left to right and vice versa
	\draw[#6] (#1, #2,#3) -- ++(0,#4 ,0) -- ++(0,0,#5) -- ++(0,-#4,0) -- cycle;
}
\newcommand{\sliceMe}[6]{
	% bottom to top and vice versa
	\draw[#6] (#1, #2,#3) -- ++(#4,0 ,0) -- ++(0,0,#5) -- ++(-#4,0,0) -- cycle;
}
\newcommand{\quadri}[9]{
	% le 1er et le dernier pt ont meme abscisse
	\draw [#9](#1,#2,0)to[bend left=#8](#3,#4,0);
	\draw [#9](#3,#4,0)to[bend right=#8](#5,#6,0);
	\draw [#9](#5,#6,0)to[bend left=#8](#7,#2,0);
	\draw [#9](#7,#2,0)to[bend right=#8](#1,#2,0);
}

\newcommand{\xangle}{15}
\newcommand{\yangle}{135}
\newcommand{\zangle}{90}

\newcommand{\xlength}{0.8}
\newcommand{\ylength}{0.8}
\newcommand{\zlength}{0.8}

\newcommand{\dimension}{2}% actually dimension-1
\newcolumntype{M}{>{$}c<{$}}
\pgfmathsetmacro{\xx}{\xlength*cos(\xangle)}
\pgfmathsetmacro{\xy}{\xlength*sin(\xangle)}
\pgfmathsetmacro{\yx}{\ylength*cos(\yangle)}
\pgfmathsetmacro{\yy}{\ylength*sin(\yangle)}
\pgfmathsetmacro{\zx}{\zlength*cos(\zangle)}
\pgfmathsetmacro{\zy}{\zlength*sin(\zangle)}


\def\cT{\mathcal T}
\def\If{I_{f}}
%\newcommand{\mbf}[1]{\mathbf{#1}}
\newcommand\myeq{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily def}}}{=}}}

\newcommand\notdef{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily def}}}{=}}}


%\bibliographystyle{acm}
\bibliographystyle{plain-fr}

\begin{document}
%\maketitle
%\renewcommand{\contentsname}{Sommaire}
%\renewcommand{\headrulewidth}{pt}
%\renewcommand{\footrulewidth}{0.5pt}
\fancyhead{}
\fancyhead[LE]{\textit{\nouppercase{\leftmark}}}
\fancyhead[LO]{\textit{\nouppercase{\rightmark}}}
\fancyfoot[R]{}
\rhead{\thepage}
\cfoot{}

\begin{titlepage}
	
	\begin{center}
		\large \textrm{\textsc{République Démocratique du Congo}}\\
		\vspace{0.2cm}
		\large
		\textcolor{black}{\textsc{Enseignement Supérieur et Universitaire}}	\\
		\vspace{0.2cm}
		\large Institut Supérieur Pédagogique de Bukavu\\
		\Large
		\vspace{0.2cm}
		\textsc{I.S.P/Bukavu}
		\begin{figure}[!h]
			\centering
			\includegraphics[width=0.3\linewidth]{logoISPBKV}
			\label{fig:logoispbkv}
		\end{figure}
		
		
		\Large
		\textsc{BP: 854/Bukavu}\\
		\vspace{0.2cm}
		{\fontfamily{pzc}{\selectfont%
				\textcolor{black}{\textsc{Section des Sciences Exactes}}}}\emph{\\}
		\vspace{0.2cm}
		{\fontfamily{pzc}{\selectfont%
				Département de Mathématique-Physique}}\\
		\vspace{0.3cm}
		
		\hrulefill	
		
		\emph{\\}
		
	{\sffamily \bfseries\Huge
	\noindent\textcolor{violet}{Sur les traitements des tenseurs numériques}}
			\emph{\\}
			
		\hrulefill
		
		
		\normalsize
		
		
		\vspace{0.4cm}
		\Large
		\begin{flushleft}
			\hspace{7cm}
			{\fontfamily{pzc}{\selectfont%
					\textcolor{black}{Par}} }\\
			\hspace{6cm}
			\textsc{Nyandu Kagarabi } Emmanuel\\
			
			\vspace{0.3cm}
			\hspace{6cm}
			{\fontfamily{pzc}{\selectfont%
					\textcolor{black}{Mémoire présenté  et défendu pour l’obtention  } }}\\
			\hspace{6cm}
			{\fontfamily{pzc}{\selectfont%
					du diplôme	de licencié en Pédagogie Appliquée
			}}\\
			\vspace{0.3cm}
			\hspace{6cm}
			Option: Mathématique\\
			\vspace{0.2cm}
			\hspace{6cm}
			{\fontfamily{pzc}{\selectfont%
					\textcolor{black}{Dirigé par }}}\\
			\hspace{6cm}
			Justin \textsc{Buhendwa Nyenyezi}\\
			\hspace{7cm}
			{\ttfamily Professeur Associé}
			
			\vspace{0.2cm}
			\hspace{6cm}
			{\fontfamily{pzc}{\selectfont%
					\textcolor{black}{Codirigé par }}}\\
			\hspace{6cm}
			 \textsc{Minani Iragi} Emmanuel\\
			\hspace{7cm}
			{\ttfamily Docteur}
		\end{flushleft}
		
		\vspace{0.2cm}
		{\fontfamily{pzc}{\selectfont%
				\textcolor{cyan}{Année académique 2020-2021}}} \\
		\color{black}
	\end{center}
\end{titlepage}

\frontmatter
\tableofcontents
\listoffigures
\listoftables

\chapter*{Épigraphe}
\addcontentsline{toc}{chapter}{Epigraphe}
\par\textquotedblleft En mathématiques, on ne comprend pas les choses, on s'y habitue\textquotedblright. John Von Neuman.

\par\textquotedblleft
La mathématique est l'art de donner le même nom à des choses différentes\textquotedblright. Henry Poincaré.

\par\textquotedblleft
Soit A un succès dans la vie. Alors $ A=x+y+z $, où $ x = travailler $, $ y = s'amuser $ et
$ z = se\, taire $\textquotedblright. Albert Einstein.

\par\textquotedblleft La théorie, c'est quand on sait tout et que rien ne fonctionne. La pratique, c'est quand tout fonctionne et que personne  ne sait pas pourquoi. Ici, nous avons réuni théorie et pratique: Rien ne fonctionne... et personne ne sait pourquoi!\textquotedblright. Albert Einstein.

\par\textquotedblleft Si une théorie est assez puissante pour traduire l'arithmétique, elle soit incohérente, soit incomplète\textquotedblright. Kurt G\"{o}del.

\par\textquotedblleft Les mathématiques ne prouvent rien, elles démontrent. Les mathématiciens ne démontrent rien, ils prouvent qu'on pourrait démontrer. Les autres sciences ne prouvent ni ne démontrent rien, elles découvrent. Parfois les hommes de science l'oublient et croient trouver la vérité au bout de leurs raisonnements ou de leurs expériences. Ils deviennent alors les absolutistes, c'est-à-dire les fascistes de la science. Étudier les fondements des théories et mettre leurs limites en évidence est aussi travailler pour la démocratie\textquotedblright. Cours de méthodologie mathématique de Jacques Mersch.

\par\textquotedblleft
L'honneur est donc pour vous, qui croyez. Mais, pour les incrédules, la pierre qu'ont rejetée ceux qui bâtissaient est devenue la principale de l'angle, et une pierre d'achoppement et un rocher de scandale\textquotedblright. 1Pierre 2: 7-8. 

\par\textquotedblleft Ne refuse pas un bienfait à celui qui y a droit, quand tu peux le faire, ne dis pas à ton prochain: Vas et reviens demain je te donnerai! quand tu as de quoi donner\textquotedblright. Proverbes 3: 27-28. 

\par\textquotedblleft
 Dieu, qui connaît le mieux les capacités des hommes, cache ses mystères aux sages et aux prudents de ce monde, et les révèle aux petits enfants\textquotedblright. Isaac Newton.
 
 \textquotedblleft Le voyage de la découverte ne consiste pas à chercher de nouveaux paysages mais à avoir de nouveaux yeux\textquotedblright. Marcel Proust.
 
 
 \par\textquotedblleft La philosophie est écrite dans ce grand livre, l'univers, qui ne cesse pas d'être ouvert devant nos yeux. Mais ce livre ne peut se lire si on ne comprend pas le langage et on ne connaît pas les caractères avec lesquels il est écrit. Or, la langue est celle des mathématiques, et les caractères sont triangles, cercles et d'autres figures géométriques. Si on ne les connaît pas, c'est humainement impossible d'en comprendre même pas un seul mot. Sans eux, on ne peut qu'aller à la dérive dans un labyrinthe obscur et  inextricable \textquotedblright. Galileo Galilei, dit Galilée.
 

\chapter*{Dédicace}
\addcontentsline{toc}{chapter}{Dédicace}
\begin{center}
À tout respect et amour je dédie ce travail à mes chers parents, ma famille et tous mes amis!!!.
\end{center}

\chapter*{In memorium}
\addcontentsline{toc}{chapter}{In memorium}
\begin{center}
À notre mère Berthe CIKURU que le très haut a précocement arrachée à notre attachement avant de jouir du fruit de son travail, son affection et sa tendresse. Quoi qu’endormie à l’attente du christ, que la terre des nos ancêtres lui demeure douce!!!.
\end{center}
\chapter*{Remerciements}
\addcontentsline{toc}{chapter}{Remerciements}
%\vspace{-1cm}
Bénis soit l'Éternel qui, nous a comblé de sa grâce tout au long de notre
formation et a rendu favorable notre évolution jusqu'à ce niveau d'études.\\
\par Le présent travail est certes, le fruit de plusieurs personnes qui, de près ou de
loin y ont contribué; ce qui nous pousse à remercier tous ceux dont l’assistance
intellectuelle, morale et surtout matérielle a permis sa réalisation.\\
\par De façon exceptionnelle, nous remercions vivement le Professeur Associé Justin \textsc{Buhendwa
Nyenyezi} et le Docteur \textsc{Minani Iragi} Emmanuel d'avoir accepté de diriger et de codiriger respectivement ce travail malgré leurs emplois de temps très surchargés.
Qu’à travers eux, tous les enseignants des départements de Mathématique-Physique
et Physique-Technologie trouvent ici l’expression de notre profonde gratitude.\\
\par Nos remerciements pour leurs efforts et
sacrifices consentis en ma faveur, à nos parents \textsc{Kagarabi}
Joseph et \textsc{Nzigire Buroko} Espérance ainsi que nos frères \textsc{Aksanti
Kagarabi} Edouard et \textsc{Birabaluge Kagarabi} Jean Paul et sœurs Nabintu Sylvie,
Bernadette, Nyota Angélique, Faida  Pascasie,
Ishara Rosette, Nzigire  Christine, Nsimire 
Berline, Hosanna, Binja Ghyslaine, Nadine.\\
\par Notre gratitude s'adresse également pour leur soutien et encouragement à notre cohorte académique ici représentée par Cikanyi Bahimuzi Julien, Bulonza Rukara Adelin, Mugoli Francine, Bunani Christian, Byamungu Claude, Namegabe Ibanez, Akili Moise, Akilimali Paul, Azene
Willy, Ishara François, Matabaro Hormisdas,
Asifiwe Ruphin, Byenda Daniel, Cekumi  Eustache et ainsi qu'à nos amis et collaborateurs représentés par Mugisho
Balaga Sammy, Eka Mihigo Félicité, Jospin Bagula, Philippe Mihanda, Magendo Wanny, Doris Ruhebuza, Prisca Damas, Aganze Mario, Ahadi Buroko, Ushindi Nickjay, Clotilde Byumanine, Mbonekube
Rogger, Rachel Matabaro, Mulume Pascal, Ahadi Christian, Masirika Adrien, Daniella Kazamwendo, Fabrice Minani, Patient Rugamba, Patient Rukara, Ushindi Rukara, Elie Mugaruka, Christian Akili, Ifundo Samuel, Innoncent, ...\\
\par Nous tenons à remercier particulièrement la famille du Prof. Samuel MATABISHI, celle de Maman Elcy BILALI et celle de Mère Bénédicte Sulia de l'aide m'apportée quand j'en avais plus besoin, Dieu vous bénisse!!!.\\
\par Enfin, que tous ceux dont les noms ne sont pas ci-haut cités ne se sentent pas
oubliés mais qu’ils trouvent à travers cette rédaction l’expression de notre profonde
affection et gratitude envers eux.\begin{flushright}
	 \textsc{Nyandu Kagarabi} Emmanuel
\end{flushright}



\mainmatter
\chapter*{Introduction générale}
\addcontentsline{toc}{chapter}{Introduction générale}
%\section{Contexte}
%Une modélisation multidimensionnelle peut être adoptée dans
%un grand nombre de problèmes se rattachant à des domaines
%aussi variés que la sociologie, l'analyse de données ou le \textit{traitement
%	du signal}. En physique et traitement du signal, les
%enregistrements numériques multidimensionnels sont modélisés par des tenseurs. Chaque mode d'un tenseur représente une
%grandeur physique telle que l'espace (\textit{longueur, largeur, hauteur}),
%le temps, le canal de couleur (\textit{longueur d'onde}), à laquelle
%est associée un espace vectoriel dont la dimension est égale au
%nombre d'échantillons numériques effectués dans cette dimension.
%Par exemple, une image en couleur se modélise par un
%tenseur trimodal: \textit{deux modes sont associés aux lignes et aux
%	colonnes, et un troisième au canal de couleur }(RVB). De même,
%en sismique, ou en acoustique sous-marine, lorsqu'une antenne
%rectiligne est employée, une modélisation trimodale des données peut être adoptée: \textit{un mode est associé à l'axe spatial, un
%	mode au temps, et un dernier à la polarisation de l'onde} .
%
%Les traitements des données multidimensionnelles procèdent
%généralement à un découpage du tenseur en vecteurs, ou matrices
%d'observations, de sorte que les méthodes du second ordre
%soient applicables. Les données traitées sont ensuite fusionnées pour retrouver la dimension du tenseur initial.
%Ce processus de découpage des données multidimensionnelles
%provoque inévitablement une perte d'information par rapport
%à la quantité globale d'information contenue dans le tenseur.
%Ainsi, il est intéressant de conserver le tenseur de
%données comme entité indivisible de manière à disposer potentiellement
%de plus d'information que ce que l'on pourrait
%obtenir par le découpage du tenseur de données. Par cette approche, on cherche à améliorer le traitement des données
%obtenu par les méthodes de second ordre. Cette nouvelle approche
%implique l'utilisation de nouveaux outils d'algèbre tensorielle,
%et tout particulièrement de décomposition tensorielle. Cette méthode repose sur la décomposition tensorielle de
%Tucker aussi connue sous le nom de Higher Order Singular
%Value Decomposition (HOSVD) ou approximation de rang-$ (R_{1},\dots,R_{N}) $ inférieur. Cette décomposition représente
%la généralisation de la Décomposition en Valeurs Singulières
%(DVS) des matrices aux tenseurs \cite[p.1]{MUTIfrance}.
%\section{Problématique}
%\section{ Hypothèses}
%\section{Importance du sujet}
%\section{Objectifs du travail}\cite{hackbusch2012tensor}
%\section{Choix et intérêt du sujet}
%\section{Délimitation du sujet}
%\section{Méthodologie de travail}
%\section{Subdivision du travail}
%\section{Difficultés rencontrées}
La plupart des problèmes de l'ingénieur et des problèmes des sciences appliquées sont actuellement analysés et résolus par des méthodes numériques diverses.
Le choix d'une méthode numérique pour un problème réel dépend de la manière dont il est modélisé mathématiquement. Or ces derniers temps, de nombreux problèmes dans les domaines de la neuro-informatique,
de la reconnaissance des formes et des images, du traitement
du signal et de l'apprentissage automatique génèrent des
quantités massives de données multidimensionnelles
présentant de multiples aspects et un niveau élevé de
complexité \cite[p.1]{cichocki2014era}.
% les données à traiter numériquement deviennent de plus en plus nombreuses et de plus en plus diverses. 
 Ceci entraîne un coût calculatoire exorbitant pour le traitement numérique. Une des raisons du coût exorbitant du processus de traitement numérique des données en dimension élevée est connue comme étant la \textquotedblleft \textit{malédiction de la dimensionalité}\textquotedblright\,
ou encore \textquotedblleft\textit{curse of dimensionality}\textquotedblright\, en anglais \cite[p.155]{cichocki2015tensor}. Pour remédier à ce problème, une approche tensorielle s'avère le meilleur choix, comme on peut le lire dans \cite{hackbusch2012tensor}.\\
\par  Les tenseurs sont des objets de l'algèbre multilinéaire généralisant les vecteurs et les matrices de l'algèbre linéaire. Ils ont pour philosophie de base, la représentation des propriétés des objets, indépendamment de l'espace dans lequel ces objets sont décrits \cite[p.11]{Maurice2010}. 
De ce fait, ils sont, depuis des décennies, très utilisés dans l'analyse et le traitement des données et des phénomènes multidimensionnels.\\
\par Cependant, certaines propriétés des objets de l'algèbre linéaire, telles que le rang d'une matrice, ne se généralisent pas de manière unique dans l'approche tensorielle, et donc en dimension supérieure à deux, avec comme conséquence que certaines pratiques telles que la décomposition en valeurs singulières et d'autres factorisations matricielles étaient moins exploitées en calcul tensoriel \cite[p.120]{Nyenyezi2018}. De plus, plusieurs opérations tensorielles (telles que par exemple le produit tenseur-tenseur élémentaire), produisent un tenseur dont le rang est supérieur à la somme des rangs des composants.\\

 Des travaux relativement récents, dont ceux de Wolfgang Hackbusch, auteur de \cite{hackbusch2012tensor}, ont proposé des techniques de compression des données multidimensionnelles permettant de surmonter ces difficultés. Inspirées par les méthodes de faible rang appliquées à la \textquotedblleft\textit{hiérarchisation}\textquotedblright\, des matrices, ces techniques s'appuient sur des représentations adéquates des données multidimensionnelles en structures creuses et échelonnées. Le but est de pouvoir atteindre une représentation séparable qui permette de traiter parallèlement les calculs dans chaque direction/sous-espace. D'après \cite[p.11]{hackbusch2012tensor}, sous certaines conditions, une telle représentation séparable combinée à des techniques de compression et de troncature permet de passer d'une complexité $\mathcal{O}(n^{d})$ à une complexité $\mathcal{O}(n{d})$, ou encore d'une complexité linéaire $\mathcal{O}(n)$ à une complexité quasi logarithmique $\mathcal{O}(d\log{}n)$.\\

%\pagestyle{empty}

Néanmoins, les ingénieurs et informaticiens qui se sont accaparés des algorithmes développés, ont négligé les fondements mathématiques qui sous-tendent ces algorithmes. Les inconvénients sont énormes. D'une part, en ignorant ces fondements, il n'est pas possible d'exploiter le maximum de ces algorithmes ni  de les étendre. D'autre part, le potentiel mathématique et algorithmique de ces fondements est négligé pourtant son développement pourrait apporter d'autres performances dans les sciences appliquées.\\

Le présent travail, contribue à répondre aux questions de recherche suivantes: \textquotedblleft Quels sont les fondements mathématiques qui sous-tendent les techniques tensorielles numériques actuellement exploitées en sciences de l'ingénieur et en informatique?. Quelles sont les différentes représentations numériques des tenseurs et quelles en sont les forces et les faiblesses? \textquotedblright. \\

L'objectif du travail est triple. Premièrement, il vise à expliciter les fondements mathématiques des tenseurs numériques et leurs différentes propriétés algébriques. Deuxièmement, il vise à présenter les différentes représentations des tenseurs numériques et les techniques de manipulation de ceux-ci. Troisièmement, il s'agira de présenter une application faisant intervenir les tenseurs numériques.\\
\thispagestyle{myheadings}\markboth{}{\textit{Introduction générale}}

L'intérêt de ce travail est double. Il est d'abord pédagogique en ce sens qu'il présente simplement les concepts mathématiques d'un degré assez élevé et vise sa compréhension à un plus grand public. Il est ensuite une contribution à la vulgarisation des sciences fondamentales à travers leurs utilités dans le monde industriel et dans les sciences appliquées.\\

Ce travail se limitera à présenter, essentiellement, les aspects algébriques des tenseurs numériques et n'en abordera pas les aspects topologiques. De même, les aspects algorithmiques seront abordés mais pas détaillés.\\

Nous utilisons, dans ce travail une méthode principalement documentaire. Par cette méthode, il est possible de sélectionner, d'explorer et de structurer les données tirées des autres travaux scientifiques relatifs à ce sujet de travail. Un raisonnement déductif rigoureux est ensuite appliqué pour expliciter les principaux résultats de ce travail.\\

Dans la suite, le chapitre \ref{chap1} présentera les fondements des espaces tensoriels où d'une part, nous rappelons quelques notions sur les espaces vectoriels et d'autre part, nous présentons les espaces tensoriels ainsi que leurs propriétés caractéristiques.
Le chapitre \ref{chap2} abordera la préoccupation des représentations et traitements des tenseurs numériques; il sera question de présenter non seulement la façon dont les tenseurs sont représentés mais aussi les techniques couramment utilisées dans le calcul tensoriel. Pour finir, le chapitre \ref{chap3} présentera un traitement multidimensionnel du signal par décomposition tensorielle. Dans ce chapitre, il sera question de présenter l'apport des décompositions tensorielles dans le traitement des signaux multidimensionnels.\\

Les années 2020 et 2021 laisseront dans la mémoire de l'humanité le souvenir indélébile d’une période désastreuse. La pandémie à Corona-virus a
bouleversé la vie des sociétés humaines sur la planète. La désorganisation des sociétés et des économies
a un impact sans précédent sur la vie quotidienne. Nous avons été victime de traverser une telle période. De plus, une autre difficulté a été le manque d'une documentation actualisée sur les mathématiques appliquées dans les bibliothèques locales. Ce qui nous a
poussé à payer de l'internet et à utiliser beaucoup plus des versions électroniques.

\chapter{Fondements algébriques des espaces tensoriels}\label{chap1}
\minitoc
%\section*{Prélude}
%Le mot \textquotedblleft tenseur\textquotedblright\, est issu de l'anglais d'origine latine \textquotedblleft tensor\textquotedblright, mot introduit en 1846 par William Rowan
%Hamilton pour décrire la norme dans un système algébrique (finalement nommé algèbre de Clifford). Le mot
%a été utilisé avec son sens actuel par Woldemar Voigt en 1899.
%Le calcul différentiel tensoriel a été développé vers 1890 sous le nom de calcul différentiel absolu, et fut
%rendu accessible à beaucoup de mathématiciens par la publication par Tullio Levi-Civita en 1900 du texte
%classique de même nom (en italien, suivi de traductions). Au XX\up{e} siècle, le sujet devient connu sous le nom
%d'analyse tensorielle, et acquiert une reconnaissance plus large avec l'introduction de la théorie de la
%relativité générale d'Albert Einstein, autour de 1915.
%
%La relativité générale est complètement formulée dans le langage des tenseurs. Einstein a appris à les utiliser,
%avec quelque difficulté, avec l'aide du géomètre Marcel Grassmann ou peut-être de Levi-Civita lui-même.
\section*{Introduction}
En mathématiques, l’algèbre multilinéaire étend les méthodes de l’algèbre linéaire. Tout comme l’algèbre
linéaire est bâtie sur le concept de \textit{vecteur} et développe la théorie des espaces vectoriels, l’algèbre
multilinéaire est bâtie sur le concept de \textit{tenseur} et développe la théorie des espaces tensoriels\footnote{\url{ https://fr.wikipedia.org/w/index.php?title=Tenseur&oldid=16871830.}}. 
Ce chapitre est une transition entre les espaces vectoriels et tensoriels. Dans sa suite, la section \ref{sec:1.1evectoriels} va porter sur des généralités relatives aux espaces vectoriels où l'attention sera focalisée sur certains résultats pouvant intervenir par la suite dans le calcul tensoriel. Quant à la section \ref{sec:1.2tensor}, elle présentera le produit tensoriel des espaces vectoriels et c'est à partir d'ici que les espaces tensoriels feront face.\\

 Dans le reste de ce travail, sauf indication spécifique, nous utiliserons les majuscules calligraphiées, les majuscules grasses, les majuscules, les minuscules grasses et les minuscules pour désigner, respectivement, un espace tensoriel d'ordre supérieur, un tenseur d'ordre supérieur, une matrice ou un espace vectoriel, un vecteur et un scalaire. Par exemple, on pourra désigner un espace tensoriel par $ \V $, un tenseur par $ \mbf{A} $, une matrice par A, un espace vectoriel par V, un vecteur par $ \mbf{v} $, et un scalaire par $ a $.
\section{Généralités sur les espaces vectoriels}\label{sec:1.1evectoriels}
L'essentiel des résultats à rappeler dans cette section a été puisé principalement des ouvrages \cite{balac2003algèbre}, \cite{Michel2013},  \cite{hackbusch2012tensor}, \cite{BalbineAIMS} et \cite{Annick2014}.
\subsection{Définitions}
Considérons un corps commutatif $ \K $. En particulier, dans le reste du travail, $ \K\in\{\R,\C \}$.

Soient V un ensemble muni d'une loi de composition interne notée additivement (symbole \og +\fg{}, définie par $ (\u,\v)\rightarrow \u+\v $) et d'une loi externe, à opérateurs dans $ \K $, notée multiplicativement et définie de $ \K\times V $ vers V, par $ (\lambda,\u)\rightarrow \lambda\cdot \u. $ 
\begin{defi}[Espace vectoriel]
	\emph{\\}
	Un espace vectoriel V sur $ \K $  est un ensemble, muni d’une opération d’addition vectorielle:
	\begin{equation*}
	\u \text{ et }\v\in V\Rightarrow \u+\v\in V,
	\end{equation*}
	et d’une opération de multiplication scalaire:
		\begin{equation*}
		\v\in V \text{ et }\lambda\in \K\Rightarrow \lambda\cdot \v\in V
	\end{equation*}
	qui satisfait les axiomes suivants:	
	\begin{itemize}
		\item[$ (1) $]
		$ (V,+) $ est un groupe commutatif. 
		\item[$ (2) $]
		La loi externe possède les propriétés suivantes: $ \forall\, \lambda,\mu\in \K,\, \forall \,\u,\v\in V $:
		\begin{itemize}
			\item[$(a)$]
			$ \lambda\cdot(\u+\v)=\lambda\cdot \u+\lambda\cdot \v $,
			\item[$ (b)$]
			$ (\lambda+\mu)\cdot \u=\lambda\cdot \u+\mu\cdot \u $,
			\item[$(c) $]
			$ (\lambda\cdot\mu)\cdot \u=\lambda\cdot(\mu\cdot \u)$,
			\item[$(d) $]
			$ 1_{\K}\cdot \u=\u. $
		\end{itemize}
	
	\end{itemize}
\end{defi}
\par Les éléments de V sont appelés des \textit{vecteurs} et ceux de $\K $ des \textit{scalaires}. Le corps $ \K $ est
appelé  corps de base de l’espace vectoriel V. Si $ (V,+,\cdot) $ est un espace vectoriel sur $ \K $, alors on dit que $V$ est un $ \K $-espace vectoriel. On note $ \mbf{0}_{V} $ ou s'il n'y a aucune confusion  à craindre \textbf{0}, l'élément neutre ou le vecteur nul de V pour l'addition. 

\begin{defi}[Sous-espace engendré]\label{Déf2.6}
	\emph{\\}
Soient V un $ \K $-espace vectoriel et $ F=(\v_{i})_{1\leq i\leq m} $ une famille
finie de vecteurs de V. On appelle sous-espace engendré par F et on
note $ \spam(F) $ ou $ \spam(\v_{1},\dots, \v_{m}) $  l'ensemble des combinaisons linéaires des
vecteurs $ \v_{1},\dots, \v_{m} $. Autrement dit,
\begin{equation}
\spam(\v_{1},\dots, \v_{m})=\{a_{1}\v_{1}+\cdots+a_{m}\v_{m}/(a_{1},\dots,a_{m})\in\K^{m}\}.
\end{equation}
Inversement, la famille $ (\v_{i})_{1\leq i\leq m} $ est dite génératrice du sous-espace vectoriel
$  \spam(\v_{1},\cdots, \v_{m}) $ de V.
\end{defi} 
Le sous-espace $\spam(\v_{1},\dots, \v_{m}) $ est constitué par les vecteurs de V qui
se décomposent suivant les vecteurs $\v_{1},\dots, \v_{m}$. En d'autres termes, dire que $ \x $
appartient à $\spam(\v_{1},\dots, \v_{m}) $ signifie qu'il existe $(a_{1},\dots,a_{m})\in\K^{m} $ tel que $ \x=a_{1}\v_{1}+\dots+a_{m}\v_{m} $. 
\begin{defi}[Partie libre – Partie liée]
	\emph{\\}
	Soit V, un espace vectoriel sur $ \K $. Les m vecteurs $ \v_{1} $, $\dots$, $ \v_{m}\in V $ sont dits
	linéairement indépendants si
	\begin{equation}
		\sum_{i=1}^{m}a_{i}\v_{i}=\mbf{0}_{V},\, a_{i}\in \K\Rightarrow a_{i}=0_{\K},\, 1\leq i\leq m.
	\end{equation}
	
	Les vecteurs $ \v_{1} $, $\dots$, $ \v_{m}$ constituent alors une partie libre de V. Si l'égalité:
	\begin{equation}
		\sum_{i=1}^{m}a_{i}\v_{i}=\mbf{0}_{V},\, a_{i}\in \K
	\end{equation}
	est possible avec des scalaires $ a_{i} $ non tous nuls, les vecteurs $ \v_{1} $, $\dots$, $ \v_{m}$ sont dits linéairement dépendants et constituent une partie liée de V.
\end{defi}
En associant les concepts de partie libre et de partie génératrice, on obtient le concept important de
base, comme le précise la définition suivante.
\begin{defi}[Base]
	\emph{\\}
	Soit V, un espace vectoriel sur $\K$. Un ensemble de vecteurs $ \v_{1} $, $\dots$, $ \v_{m}\in V$ est une
	base algébrique (ou plus simplement une
	base) de V s'il constitue à la fois une partie libre et une partie génératrice de V.
\end{defi}
\begin{defi}[Dimension]
	\emph{\\}
	Soit V, un espace vectoriel sur $\K$ engendré de manière finie. Le nombre commun de vecteurs des bases de V est appelé la dimension de l'espace vectoriel.
\end{defi}
Un espace vectoriel V sur le corps $\K$ est dit de dimension \textit{finie} s'il existe une base ne contenant qu'un
nombre fini d'éléments ou si cet espace est réduit à $\{0\}$. Sinon l'espace vectoriel sera dit de dimension
\textit{infinie}.
\begin{rem}
\emph{\\}
Pour un espace vectoriel V sur $ \K $ de dimension $ n $, la dimension a trois significations
équivalentes:
\begin{enumerate}
\item
Le nombre de vecteurs d’une base.
\item
Le nombre minimum de vecteurs d’une partie génératrice.
\item
Le nombre maximum de vecteurs d’une partie libre.
\end{enumerate}

\end{rem}

\subsection{Applications linéaires}
\subsubsection{Définitions}
\begin{defi}[Application linéaire]
	\emph{\\}
Soient V et W deux espaces vectoriels sur $ \K $. Une application  $ f :V\rightarrow W$ est dite linéaire si $ \forall \v,\w\in V $,  $ \alpha \in \K $, on a:
	\begin{eqnarray}
		&(1)& f(\v+\w) = f(\v) + f(\w),\nonumber\\
		&(2)& f(\alpha \v )=\alpha f(\v).\nonumber	
	\end{eqnarray}	
\end{defi}
\begin{defi}[Dual d'un espace vectoriel]
	\emph{\\}
	Soit V un espace vectoriel sur $ \K $. On appelle forme linéaire sur V, ou \textit{covecteur} de V, toute application linéaire de V à valeurs dans le corps $ \K $.  L’espace $ \L_{\K} (V,\K) $ des formes
	linéaires sur V est appelé \textit{dual} de l’espace V, et est noté $ V^{*} $.
	
	\begin{equation}
	V^{*}=\{f: V\longrightarrow\K,\, \mbox{ f linéaire}\}.
	\end{equation}
\end{defi}
\subsubsection{Matrice d'une application linéaire}\label{matrice}
Considérons deux espaces vectoriels V et W sur $ \K $ des dimensions finies, respectivement $ n $ et $ m $. Soient $ B=\{\v_{1},\cdots,\v_{n}\} $ une base de V et $ B'=\{\w_{1},\cdots,\w_{m}\} $ une base de W. Si $ f:V\rightarrow W $ est une application linéaire, alors l'image de tout vecteur $ \x $ de V est connue dès que l'on connait les images des vecteurs de la base $ B $. En effet, $ \x $ se décompose de manière unique dans $ B $: 
$$\x=\sum_{j=1}^{n}x_{j}\v_{j}. \text{ Ainsi, } f(\x)=f\left(\sum_{j=1}^{n}x_{j}\v_{j}\right)=\sum_{j=1}^{n}x_{j}f(\v_{j}),\;x_{j}\in\K.$$ Chacun des vecteurs $ f(\v_{j}) $ se décompose dans la base $ B' $ comme
$$f(\v_{j})=\sum_{i=1}^{m}a_{ij}\w_{j} \text{ et donc } f(\x)=\sum_{j=1}^{n}x_{j}f(\v_{j})=\sum_{j=1}^{n}x_{j}\sum_{i=1}^{m}a_{ij}\w_{j}=\sum_{i=1}^{m}\sum_{j=1}^{n}x_{j}a_{ij}\w_{j}. $$ Les coefficients $ a_{ij}$ sont entièrement définis par f et les deux bases $B $ et $ B' $.
\begin{defi}
\emph{\\}
Soient $ f:V\rightarrow W $ une application linéaire, $ B=\{\v_{1},\cdots,\v_{n}\} $ une base de V et $ B'=\{\w_{1},\cdots,\w_{m}\} $ une base de W. On appelle matrice de f relative aux bases $B$ et $B'$ la matrice 
\begin{equation}\label{matrix}
M(f)_{B,B'}=\begin{pmatrix}
	a_{11}&	a_{12}&\cdots&a_{1n}\\
	a_{21}&	a_{22}&\cdots&a_{2n}\\
	\vdots&\vdots&\ddots&\vdots\\
	a_{m1}&	a_{m2}&\cdots&a_{mn}\\
\end{pmatrix}
\end{equation}
où les coefficients $ a_{ij}$ sont définis par $ f(\v_{j})=\sum_{i=1}^{m}a_{ij}\w_{j} $, l'indice $ i $ désignant la ligne et $ j $ la colonne. On note simplement $ M=(a_{ij}) $. A toute application linéaire correspond une seule matrice (ou représentation matricielle). La réciproque est aussi vraie, c’est-à-dire qu’à toute matrice correspond une seule application linéaire.
\end{defi}
\subsection{Rappel de quelques résultats matriciels}
Les matrices constituent un cas particulier des tenseurs, raison pour laquelle une grande attention sera focalisée ici. Par concision, les résultats ne seront pas démontrés, leurs démonstrations sont à retrouver dans les ouvrages précédemment cités.
\subsubsection{Généralités}
\label{ssec:matrixgen}
\begin{defi}[Matrice]
	\emph{\\}
	Une matrice rectangulaire de dimensions $ m\times n $ sur $ \K $, définie par \eqref{matrix}, est  un tableau à m
	lignes et à n colonnes d'éléments $ a_{ij}\in \K $ où $ 1\leq i\leq m$ et $ 1\leq j\leq n.$
\end{defi}

\begin{defi}[Adjointe d'une matrice]\emph{\\}
Soit $ A=(a_{ij})\in\C^{m\times n} $. L'adjointe de A est la matrice notée et définie par $$ A^{*}=(\overline{A})^{T}=(\overline{a}_{ji}). $$ Dans le cas où $ A^{*}=A $, on dit que A est hermitienne.
\end{defi}
L’opération la plus importante en algèbre linéaire est probablement le produit matrice-vecteur \cite[p.2311]{oseledets2011tensor}. Nous la présentons via la définition suivante.
\begin{defi}[Multiplication matrice-vecteur]\emph{\\}
Soit $ \v $ un vecteur colonne de dimension $ n $ et soit A une matrice de taille $ m\times n $. Alors le produit matrice-vecteur $ \mbf{b} = A\v $ est le vecteur colonne de dimension $ m $ défini comme suit:
\begin{equation}\label{Matvec}
b_{i}=\sum_{j=1}^{n}a_{ij}v_{j},
\end{equation}
pour $ i = 1,2,...,m $, où $b_{i} $ désigne la ième entrée de $ \b $, $ a_{ij} $ désigne l'entrée correspondant à la ième ligne et à la jième colonne de A et $ v_{j} $ désigne la jième entrée de $ \v $.
Le coût en termes d'opérations est $mn(n-1) $.
\end{defi}
\begin{defi}[Produit matrice-matrice]\emph{\\}
 Étant données respectivement deux matrices $ A\in\K^{m\times p} $  et $ B\in\K^{p\times n} $, le produit matriciel $ C=AB\in\K^{m\times n} $  résultant de leur multiplication est tel que, chaque colonne de C est une combinaison linéaire des colonnes de A et les entrées sont définies comme suit:
 \begin{equation}\label{prodmat}
 c_{ij}=\sum_{k=1}^{p}a_{ik}b_{kj},
 \end{equation}
où $ c_{ij} $, $ a_{ik} $ et $ b_{kj} $ sont des entrées de C, A et B respectivement.

Ce produit matriciel AB n'est défini que lorsque le nombre de colonnes de A est égal au nombre de lignes de B. Le coût calculatoire est donné par $ mnp(p - 1) $.
\end{defi}
\begin{defi}[Inverse d'une matrice]\emph{\\}
	Une matrice carrée A d’ordre $ n $ est inversible s’il existe une matrice carrée $ A^{-1} $ d’ordre
	$ n $ telle que $ AA^{-1} = A^{-1}A = I_{n} $. La matrice $ A^{-1} $ est appelée l’inverse de A.
\end{defi}
\begin{defi}[Pseudoinverse]
\emph{\\}
Soit $ A\in\C^{m\times n} $ une matrice. Une matrice $ B\in\C^{n\times m} $ est appelée inverse généralisé ou pseudoinverse de A s'il satisfait les conditions suivantes:

\begin{tabular}{ll}
$ (1) $ &$ ABA=A $,\\
 $ (2) $&$ BAB=B.$\\
\end{tabular}

\end{defi}
La définition à présenter ci-dessous (tirée de \cite[p.4]{Moore2011}) généralise en fait la notion d'inverse d'une matrice. Elle pourra intervenir surtout dans la section \ref{sec:2.2traitements} lorsqu'il s'agira d'établir le lien entre les produits de Khatri-Rao, de Kronecker et de Hadamard pour les matrices.
\begin{defi}[Pseudoinverse de Moore-Penrose]\label{MOORE}\emph{\\}
Considérons une matrice $ A\in\C^{m\times n} $. Une matrice $ A^{\dagger}\in\C^{n\times m} $ est appelée Pseudoinverse de Moore-Penrose de A s'il satisfait les conditions suivantes:
\begin{itemize}
\item[$ (1) $]$ AA^{\dagger}A=A $,
\item[$ (2) $]$ A^{\dagger}AA^{\dagger}=A^{\dagger}, $
\item[$ (3) $]$ AA^{\dagger} \in\C^{m\times m}$ et $ A^{\dagger}A\in \C^{n\times n} $ sont hermitiennes.
\end{itemize}
\end{defi}
Si $ A^{-1} $ existe, alors on a $ A^{\dagger} = A^{-1} $. Par ailleurs, si $ (AA^{*})^{-1} $ existe, alors
\begin{equation}
	A^{\dagger}=A^{*}(AA^{*})^{-1}.
\end{equation} De même, si $ (A^{*}A)^{-1} $ existe, on a
\begin{equation}
A^{\dagger}=\left(A^{*}A\right)^{-1}A^{*}.
\end{equation}
\begin{defi}[Matrice unitaire, matrice orthogonale]
	\emph{\\}
	Soit $ A\in\C^{n\times n} $. On dit que A est:
	\begin{list}{$ \ast $}{}
		\item
		unitaire si $ A^{*}A=AA^{*}=I_{n} $, c'est-à-dire, $ A^{*}=A^{-1} $,
		\item
		orthogonale si $ A^{T}A=AA^{T}=I_{n}$, c'est-à-dire, $ A^{T}=A^{-1} $.
	\end{list}
\end{defi}


\begin{defi}[valeurs propres et vecteurs propres d'une matrice]
	\emph{\\}
	Un vecteur $ \v $ non nul de $ \K^{n} $ est appelé vecteur propre d’une matrice $ A\in\K^{n\times n} $ s’il existe
	un scalaire $ \lambda\in\K $ tel que
	\begin{equation}
	A\v=\lambda\v.
	\end{equation}
	Le scalaire $ \lambda $ est appelé valeur propre de A associée à $ \v $.
\end{defi}
L'ensemble de valeurs propres de A est appelé \textit{spectre} de A et est noté $ S_{p}(A) $. Les $ \lambda $ sont les racines dans $ \K $ du polynôme caractéristique $ P_{A}(\lambda)=\det(A-\lambda I_{n}) $ de degré $ n $.
\begin{defi}[Valeurs singulières]\label{SVD}
	\emph{\\}
	Les valeurs singulières d’une matrice $ A\in \K^{m\times n} $ sont les racines carrées des valeurs propres
	non-nulles de $ A^{*}A $ et $ AA^{*} $.
	$$\sigma_{j}=\sqrt{\lambda_{j}(A^{*}A)}=\sqrt{\lambda_{j}(AA^{*})}. $$
	%En notant par exemple $ \lambda_{1},\lambda_{2},\dots,\lambda_{n} $ les valeurs propres de $ A^{*}A $ ou de $ AA^{*} $  de sorte que $ \lambda_{1}\geq \lambda_{2}\geq\cdots\ge \lambda_{n}> 0$ et en posant $ \sigma_{i}=\sqrt{\lambda_{i}} $ tel que $ \sigma_{1}\geq \sigma_{2}\geq\cdots\geq \sigma_{n}> 0 $. Les nombres $\sigma_{1},\sigma_{2},\cdots,\sigma_{r}$ ainsi définis sont appelés valeurs singulières de A.\\
\end{defi}
\begin{defi}[Image-noyau d'une matrice]
	\emph{\\} Soit $ A\in\K^{m\times n} $. On appelle image de A le sous-espace vectoriel de $ \K^{m} $ notée $ Im(A) $ défini par:
	\begin{equation}
	Im(A)=\{\v\in\K^{m}/\exists \u\in \K^{n}, A\u=\v\}.
	\end{equation}
	Le noyau  de A est le sous-espace vectoriel de $ \K^{n} $ noté et défini par
	\begin{equation}
	\ker(A)=\mathcal{N}(A)=\{\u\in\K^{n}/A\u=0\}.
	\end{equation}
\end{defi}

En Analyse numérique, en Statistique et dans d’autres disciplines, les inconnues de
certains problèmes sont des vecteurs ou des matrices. De nombreuses méthodes consistent
à construire un élément d’un sous espace vectoriel le plus proche d’un autre élément
qui lui, est connu. Il est donc nécessaire de savoir mesurer des distances entre vecteurs
ou entre matrices. Pour cela on peut définir une norme sur l’espace vectoriel auquel ils
appartiennent. Avant de définir la norme vectorielle, définissons le produit scalaire:
\begin{defi}[Produit scalaire]
	\emph{\\}
	Soit V un $ \K $-espace vectoriel. L'application $ <,>:V\times V\rightarrow\K $: $ (\v,\w)\rightarrow<\v,\w> $ est un produit scalaire  sur V si et seulement si elle vérifie les conditions suivantes:
	\begin{enumerate}
		\item[$ (1) $]$ <\v,\w>=\overline{<\w,\v>} $.
		\item[$ (2) $]$ <\alpha\v_{1}+\beta\v_{2},\w>=\alpha<\v_{1},\w>+\beta<\v_{2},\w> $, $ \forall \alpha,\beta\in\K $.
		\item[$ (2) $] $ <\v,\v>\geq 0 $.
		\item[$ (4) $]$ <\v,\v>=0\Leftrightarrow \v=\mbf{0}_{V} $.
	\end{enumerate}
\end{defi}
\begin{ex}\emph{\\}
	Le produit scalaire sur $ \R^{n} $ est donné par  
	\begin{equation}\label{scalar}
	\langle\u,\v\rangle=\u^{T}\v=\sum_{i=1}^{n}u_{i}v_{i}=\v^{T}\u\in\R.
	\end{equation}
Cette opération nécessite $ n $ multiplications et $ (n - 1) $ additions. On dit que le coût calculatoire du produit scalaire  est $n(n-1) $.
\end{ex}On dit que deux vecteurs $ \u,\v\in\R^{n} $ sont \textit{orthogonaux}, si $ \langle \u,\v\rangle=0 $. Par ailleurs, une famille de vecteurs $ \{\v_{i}\}_{1\leq i\leq n} $ est orthogonale, si les vecteurs sont orthogonaux par paire, c'est-à-dire, $ \langle \v_{i},\v_{j}\rangle=0 $, $ \forall1\leq  i,j\leq n $ avec $ i\neq j $.
\begin{defi}[Produit extérieur]
	\emph{\\}
	Étant donnés  deux vecteurs $ \u\in\R^{m},\v\in\R^{n} $, leur produit extérieur est
	\begin{equation}\label{produitexterieur}
	\u\v^{T}=\begin{bmatrix}
	u_{1}\\\vdots\\ u_{m}
	\end{bmatrix}\begin{bmatrix}
	v_{1}&\cdots&v_{n}
	\end{bmatrix}=\begin{bmatrix}
	u_{1}v_{1}&\cdots&u_{1}v_{n}\\
	\vdots&\vdots&\vdots\\
	u_{m}v_{1}&\cdots&u_{m}v_{n}\\
	\end{bmatrix}\in\R^{m\times n}.
	\end{equation}
\end{defi}
\begin{defi}[Norme vectorielle]
	\emph{\\}
	Une norme sur un $ \K $-espace vectoriel V est une application $ \parallel.\parallel $ de V dans $\R_{+}  $ qui vérifie les propriétés suivantes:
	\begin{enumerate}
		\item[$ (1) $]$ \forall \v\in V $, $ \parallel\v\parallel\geq 0 $ (positivité). De plus,  $ \parallel \v \parallel=0\Leftrightarrow\v=0. $
		\item[$ (2) $]$ \forall\v\in V $, $ \forall \lambda\in\K $, $ \parallel\lambda\v\parallel=|\lambda|\parallel\v\parallel $ (homogénéité)
		\item[$ (3) $]$ \forall \v,\w\in V $, $ \parallel\v+\w \parallel\leq \parallel\v\parallel+\parallel\w\parallel$ (inégalité triangulaire).
	\end{enumerate} Un espace vectoriel muni d'une norme est appelé espace vectoriel normé.
\end{defi}
\begin{ex}
	\emph{\\}
 Étant donné un vecteur $ \v\in\R^{n} $, sa norme euclidienne est notée et définie par
\begin{eqnarray}
\parallel\v\parallel_{2}=\left(\sum_{i=1}^{n}v_{i}^{2}\right)^{\pf{1}{2}}.
\end{eqnarray}

\end{ex}

\begin{defi}[Norme matricielle]
\emph{\\}
 Une norme matricielle est une application $ \parallel.\parallel:\K^{m\times n}\longrightarrow\R_{+} $ qui satisfait les analogues des trois propriétés de la norme vectorielle. 
\begin{ex}\emph{\\}
Étant donnée une matrice $ A\in\K^{m\times n} $, voici deux des normes couramment utilisées:
\begin{eqnarray}
&\star&\text{Norme spectrale: }\parallel A\parallel_{2}=\underset{\parallel\v\parallel=1}{\max}\parallel A\v\parallel_{2}=\sigma_{\max}(A),\\
&&\quad \text{où }\sigma_{\max}(A) \text{ est la grande valeur singulière de la matrice A et,}\nonumber\\
&\star&\text{Norme de Frobenius: }\parallel\!A\,\parallel_{F}=\,\left(\sum_{i=1}^{m}\sum_{j=1}^{n}|a_{ij}|^{2}\right)^{\pf{1}{2}}=\sqrt{tr(A^{*}A)}=\sqrt{\sum_{i=1}^{r}\sigma_{i}^{2}},\label{frobmatrix}\quad\\
&&\quad \text{où }\sigma_{1},\dots,\sigma_{r} \text{ sont r valeurs singulières non nulles de A}.\nonumber
\end{eqnarray}
\end{ex}
\end{defi}

En algèbre linéaire, la notion de rang matriciel est définie de manière unique. Néanmoins, en algèbre multilinéaire, non seulement sa détermination  pose problème mais aussi il en existe autant de types comme on le présentera dans la section \ref{sec:2.3rang}.

\begin{defi}[Rang matriciel]\label{defrang}
	\emph{\\}
	Considérons une matrice $A\in \K^{I\times J}$ et notons $ r$ le rang de A\footnote{Les entrées d'une matrice $A\in \K^{I\times J}$ sont notées par $ a_{ij}(i\in I, j\in J) $ avec I et J deux ensembles finis d'indices. En particulier, pour $ I=\{1,\cdots,m\} $ et $ J=\{1,\dots,n\} $, $ \K^{I\times J}=\K^{m\times n} $ \cite[p.21]{hackbusch2012tensor}.}. Les déclarations suivantes sont équivalentes et  peuvent être
	utilisées pour définir  $ r $:
	
	\begin{enumerate}
		\item[(a)]
		$ r=\dim Im(A) $,
		\item[(b)]
		$ r=\dim Im(A^{T}) $,
		\item[(c)]
		r est le nombre maximal des lignes linéairement indépendantes dans A,
		\item[(d)]
		r est le nombre maximal des colonnes linéairement indépendantes dans A,
		\item[(e)]
		r est le nombre des valeurs singulières positives de A \cite[p.24]{hackbusch2012tensor}.
	\end{enumerate}
\end{defi}
  Le rang de $ A\in \K^{I\times J} $ est borné par le \textit{rang maximal}
\begin{equation}
r_{\max}=\min\{\#I,\#J\},
\end{equation}
et cette limite est atteinte pour les matrices dites de \textit{rang plein}. 

\subsubsection{Quelques décompositions matricielles}\label{ssec:matrixdecomp}
Beaucoup de décompositions matricielles ont été développées dans l'histoire de mathématiques. Néanmoins, ici, nous n'allons présenter qu'un échantillon de deux qui seront utilisées dans la section \ref{sec:2.4formats}. Il s'agit en effet des factorisations QR et SVD. 
\subsubsection*{(a) Décomposition QR}
La décomposition QR (ou \og factorisation QR\fg) est un outil utile pour l'orthogonalisation  et peut être considérée comme une formulation algébrique de l'orthogonalisation de Gram-Schmidt. En effet, l'idée est de factoriser $ A \in\R^{m\times n} $ sous la forme 
\begin{equation}
A=QR,
\end{equation} où $ Q\in\R^{m\times m} $ est une matrice orthogonale et $ R\in\R^{m\times n} $ est une matrice triangulaire supérieure. Supposons que la matrice A soit de rang $ r $, cela signifie qu'il existe $ r $ entrées diagonales non nulles dans $ R $  et on a la décomposition 
\begin{equation*}
A=QR=\begin{pmatrix}
\tilde{Q}&\hat{Q}
\end{pmatrix}\begin{pmatrix}
\tilde{R}\\0
\end{pmatrix}=\tilde{Q}\tilde{R},
\end{equation*}
où $ \tilde{Q} $ est une matrice $ m\times r $ à colonnes orthogonales et $ \tilde{R} $ est une matrice triangulaire supérieure $ r\times r $.

La manière classique d'obtenir une décomposition $ QR $ est l'orthogonalisation de Gram-Schmidt. Mais l'utilisation de cette orthogonalisation n'est pas aussi stable pour la décomposition $ QR $ que l'utilisation des réflecteurs de Householder\footnote{Étant donné un vecteur $ \v \in\R^{n},$ la matrice de Householder est une matrice symétrique et orthogonale définie par $ H=I-\pf{2\v\v'}{\v'\v} $. De plus, une matrice de Householder transforme un vecteur $ \x $  en le réfléchissant à travers l'hyperplan qui est perpendiculaire au vecteur $ \v $. } \cite[p.13]{BalbineAIMS}.  La dernière approche  retient donc une attention particulière de notre part. Pour ce faire, le procédé consiste à multiplier $ A $ par une séquence de matrices orthogonales simples $ Q_{k} $ et à obtenir une matrice triangulaire supérieure $ R=Q_{n}\cdots Q_{2}Q_{1}A $. A la k-ième étape, $ Q_{k} $ introduit des zéros sous la diagonale dans la k-ième colonne et conserve les zéros des lignes précédentes. Cela signifie que si
\begin{equation*}
A=\begin{pmatrix}
a_{11}&a_{12}&\cdots&a_{1n}\\
a_{21}&a_{22}&\cdots&a_{2n}\\
\vdots&\vdots&\ddots&\vdots\\
a_{m1}&a_{m2}&\cdots&a_{mn}\\
\end{pmatrix},\text{ alors } Q_{1}A=\begin{pmatrix}
a_{11}&a_{12}&\cdots&a_{1n}\\
0&a_{22}&\cdots&a_{2n}\\
\vdots&\vdots&\ddots&\vdots\\
0&a_{m2}&\cdots&a_{mn}\\
\end{pmatrix}
\end{equation*}
A la k-ième étape, on a 
\begin{equation*}
Q_{k}\cdots Q_{1}A=\begin{pmatrix}
a_{11}&a_{12}&\cdots&\cdots&\cdots&a_{1n}\\
0&a_{21}&\cdots&\cdots&\cdots&a_{2n}\\
0&0&\ddots&\cdots&\cdots&\vdots\\
0&0&a_{jk}&a_{jk+1}&\cdots&\vdots\\
\vdots&\vdots&\vdots&\vdots&\ddots&\vdots\\
0&0&0&a_{mk+1}&\cdots&a_{mn}\\
\end{pmatrix}.
\end{equation*}
Construisons $ Q^{T} $ en fixant $ Q_{k}=\begin{pmatrix}
I&0\\0&F
\end{pmatrix} $, où $ I\in\R^{k-1\times k-1} $ est la matrice identité qui
permet de préserver les zéros déjà introduits dans les $ k - 1 $ premières colonnes et $ F\in\R^{m-k+1\times m-k+1} $ est le réflecteur de Householder qui introduit des zéros sous la diagonale dans la k-ième colonne. Pour $ \x\in\R^{m-k+1}, F=I-2\f{\v\v^{T}}{\v^{T}\v} $, où $ \v=\parallel\x\parallel\mbf{e}_{1}-\x $ est le vecteur normal et $ \mbf{e}_{1} $ est le vecteur unitaire canonique. Il vient $ Q^{T} = Q_{n}\cdots Q_{2}Q_{1} $ et cela implique $ Q = Q_{1}Q_{2}\cdots Q_{n} $. Cette méthode est plus précise que la précédente mais elle échoue aussi lorsque A est de rang déficient.
\subsubsection*{(b) Décomposition en valeurs singulières}
Un des grands enjeux de l'algèbre linéaire est de "réduire" les matrices d'endomorphismes,
par exemple de les diagonaliser (c'est-à-dire de trouver une base où la matrice de l'endomorphisme
est diagonale) lorsque cela est possible. Cela permet de simplifier considérablement
certains calculs, comme par exemple les puissances n-ièmes de ces matrices, la résolution des systèmes différentiels linéaires, etc. On rappelle qu'une matrice carrée symétrique $ A\in\R^{n\times n}$ est diagonalisable s'il existe  une matrice carrée inversible P telle que
\begin{equation}\label{diag}
\Delta=P^{-1}AP \text{ soit une matrice diagonale}.
\end{equation}
Les colonnes de P sont des vecteurs propres de A qui forment une base orthonormée
de $ \R^{n} $. Les éléments diagonaux de $ \Delta $ sont
les valeurs propres de A. L’idée de la décomposition en valeurs singulière (SVD pour Singular Values Decomposition) est similaire à la décomposition en
valeurs propres, mais fonctionne pour n’importe quelle matrice A de taille $ m\times n $.

En effet, il a été prouvé que pour toute matrice $ A\in\K^{m\times n} $, il existe une décomposition
\begin{equation}
A=U\Sigma V^{*}
\end{equation}
 où U est une matrice unitaire $ m\times m $, $ \Sigma $ est une matrice $ m\times n $ telle que $$ \Sigma=\begin{pmatrix}
 \Sigma_{r}&0\\ 0&0
 \end{pmatrix} $$ et
V est une matrice unitaire $ n \times n $. La matrice diagonale $ \Sigma_{r}=diag(\sigma_{1},\dots,\sigma_{r}) $ où les $ \sigma_{i} $ sont appelés valeurs singulières de A pour tout $ i = 1,...,r $ et le rang de la matrice A est $ r $.

Les valeurs singulières de A sont uniques mais les vecteurs singuliers ne sont déterminés de manière unique que s'il n'y a pas de valeurs singulières multiples. Pour les valeurs singulières multiples, ils peuvent être pris comme orthonormaux et sont dans l'espace
des lignes de A.

 Supposons que les vecteurs singuliers $ \v_{j}, j = 1,...,r $ aient été choisis avec des valeurs singulières $ \sigma_{j} $, alors les vecteurs singuliers $ \u_{j}, j = 1,...,r $ sont déterminés de manière unique à l'aide de la relation suivante
 \begin{equation}\label{uj}
 A\v_{j}=\sigma_{j}\u_{j},\; j=1,\dots,r.
 \end{equation}
De même, si les vecteurs singuliers  $ \u_{j}, j = 1,...,r $ sont choisis, les vecteurs singuliers  $ \v_{j}, j = 1,...,r $ sont déterminés de manière unique à partir de
\begin{equation}
A^{T}\u_{j}=\sigma_{j}\v_{j},\; j=1,\dots,r.
\end{equation}
A partir de l'observation ci-dessus, on peut conclure que la SVD fournit des informations complètes concernant les quatre sous-espaces fondamentaux suivants associés à A:
\begin{equation*}
\begin{matrix}
\mathcal{N}(A)=\spam\{\v_{r+1},\dots,\v_{n}\},&\mathcal{R}(A)=\spam\{\u_{1},\dots,\u_{r}\},\\
\mathcal{N}\left(A^{T}\right)=\spam\{\v_{1},\dots,\v_{r}\},&\mathcal{R}\left(A^{T}\right)=\spam\{\u_{r+1},\dots,\u_{m}\}.\\
\end{matrix}
\end{equation*} 
 L'exemple ci-dessous constitue une illustration de cette théorie.
\begin{ex}
\emph{\\}
Considérons $ A=\begin{pmatrix}
0&3\\-2&0\\0&0
\end{pmatrix}\in\R^{3\times 2} $. A est réelle, du coup $ A^{*}=A^{T}=\begin{pmatrix}
0&-2&0\\
3&0&0\\
\end{pmatrix}.$
$$A^{*}A=A^{T}A=\begin{pmatrix}
0&-2&0\\ 3&0&0
\end{pmatrix}\begin{pmatrix}
0&3\\-2&0\\0&0
\end{pmatrix}=\begin{pmatrix}
4&0\\ 0&9
\end{pmatrix}. $$
Les valeurs propres de $ A^{*}A $ sont les $ \lambda_{j} $ racines de l'équation $ \det(A^{*}A-\lambda I_{2})=0 $, c'est-à-dire
\begin{equation*}
\begin{vmatrix}
4-\lambda&0\\
0&9-\lambda
\end{vmatrix}=0\Leftrightarrow (4-\lambda)(9-\lambda)=0\Leftrightarrow \lambda=4\text{ ou } \lambda=9.
\end{equation*}
Ainsi, $ \lambda_{1}=9 $ et $ \lambda_{2}=4 $ $ (\lambda_{1}\geq \lambda_{2}) $. Les valeurs singulières de A sont $ \sigma_{1}=\sqrt{\lambda_{1}}=3 $ et $ \sigma_{2}=\sqrt{\lambda_{2}}=2 $. Puisqu'on a deux valeurs singulières non nulles, alors $r= rang(A) =2$.\\
 On trouve $ \Sigma_{2}=diag(\sigma_{1},\sigma_{2})=\begin{pmatrix}
3&0\\0&2
\end{pmatrix} $ et $ \Sigma\in\K^{3\times 2}=\begin{pmatrix}
3&0\\0&2\\0&0
\end{pmatrix}. $ \\
Soit $ \x=\begin{pmatrix}
x\\y
\end{pmatrix} $ le vecteur propre associé à la valeur propre $ \lambda_{j_{,1\leq j\leq 2}} $ de $ A^{*}A $. Il vérifie
\begin{equation}
\label{eqsvd}
(A^{*}A-\lambda_{j}I_{2})\x=\mbf{0}\text{ ou }\begin{pmatrix}
4-\lambda_{j}&0\\0&9-\lambda_{j}
\end{pmatrix}\begin{pmatrix}
x\\y
\end{pmatrix}=\begin{pmatrix}
0\\0
\end{pmatrix}\Leftrightarrow\begin{cases}
(4-\lambda_{j})x+0y=0\\ 0x+(9-\lambda_{j})y=0.
\end{cases}
\end{equation} 
Pour $ j=1 $, $ \lambda_{1}=9 $, la relation \eqref{eqsvd} est  vérifiée par le couple $ (0,y)_{y\in\R} $, $ \v_{1}=\begin{pmatrix}
0\\1
\end{pmatrix} $.\\
Pour $ j=2 $, $ \lambda_{2}=4 $, la relation \eqref{eqsvd} est  vérifiée par le couple $ (x,0)_{x\in\R} $, $ \v_{2}=\begin{pmatrix}
-1\\0
\end{pmatrix} $.\\
Du coup, $ V\in\R^{2\times 2}=\begin{pmatrix}
\v_{1}&\v_{2}
\end{pmatrix}=\begin{pmatrix}
0&-1\\
1&0
\end{pmatrix} $ et $ V^{*}=V^{T}=\begin{pmatrix}
0&1\\-1&0
\end{pmatrix}.$\\
De la relation \eqref{uj}, on a $ \u_{j}=\pf{1}{\sigma_{j}}A\v_{j} $, $ 1\leq j\leq 2=r=rang(A) $. Il vient:\\
$\text{Pour }j=1,\,\u_{1}=\pf{1}{\sigma_{1}}A\v_{1}=\pf{1}{3}\begin{pmatrix}
0&3\\-2&0\\0&0
\end{pmatrix}\begin{pmatrix}
0\\1
\end{pmatrix}=\pf{1}{3}\begin{pmatrix}
3\\0\\0
\end{pmatrix}=\begin{pmatrix}
1\\0\\0
\end{pmatrix}, $\\
$\text{Pour }j=2,\,\u_{2}=\pf{1}{\sigma_{2}}A\v_{2}=\pf{1}{2}\begin{pmatrix}
0&3\\-2&0\\0&0
\end{pmatrix}\begin{pmatrix}
-1\\0
\end{pmatrix}=\pf{1}{2}\begin{pmatrix}
0\\2\\0
\end{pmatrix}=\begin{pmatrix}
0\\1\\0
\end{pmatrix}, $\\
Le vecteur-colonne $\u_{3}\text{ est tel que }\langle\u_{1},\u_{3}\rangle=\langle\u_{2},\u_{3}\rangle=0 \text{ et }\Arrowvert\u_{3}\Arrowvert=1;\,\u_{3}=\begin{pmatrix}
0\\0\\1
\end{pmatrix} $.\\
 Ainsi, $ U\in\R^{3\times 3}=\begin{pmatrix}
\u_{1}&\u_{2}&\u_{3}
\end{pmatrix}=\begin{pmatrix}
1&0&0\\0&1&0\\0&0&1
\end{pmatrix}$, $\Sigma=\begin{pmatrix}
3&0\\0&2\\0&0
\end{pmatrix}$ et $ V^{*}=\begin{pmatrix}
0&1\\-1&0
\end{pmatrix}.$\\
Par suite, $ A=\begin{pmatrix}
	1&0&0\\0&1&0\\0&0&1
	\end{pmatrix}\begin{pmatrix}
	3&0\\0&2\\0&0
	\end{pmatrix}\begin{pmatrix}
	0&1\\-1&0
	\end{pmatrix} $.

\end{ex}
\subsubsection*{Approximation de faible rang}
Étant donnée une matrice A, on demande une matrice $ R\in\mathcal{R}_{s} $ de faible rang (c'est-à-dire, $ s < rang(A) $) telle que $ \parallel
A-R
\parallel $ soit minimisée. La réponse est donnée par Erhard Schmidt (1907)(parfois, ce résultat est attribué à Eckart-Young, qui a réinventé l'énoncé plus tard en 1936). Dans son article, il étudie la décomposition en valeurs singulières infinies pour les opérateurs. Le cas fini suivant en est une application particulière.\newpage
\begin{theo}
\emph{\\}
\begin{enumerate}
\item[(a)] Soit $ A,R\in\K^{m\times n} $ avec $ r=rang(R) $. Les valeurs singulières de $ A $ et $ A-R $ sont notées $ \sigma_{i}(A) $ et $ \sigma_{i}(A-R) $, respectivement. Alors
\begin{equation}\label{2.25}
\sigma_{i}(A-R)\geq \sigma_{r+i}(A) \text{ pour tout } 1\leq i\leq\min\{m,n\}.
\end{equation}
\item[(b)]
Soit $ s\in\{0,1,\cdots ,\min\{m,n\}\} $. On utilise la décomposition en valeurs singulières $ A=U\Sigma V^{T} $ pour définir
\begin{equation}\label{2.26a}
R=U\Sigma_{s}V^{T}\text{ avec }(\Sigma_{s})_{ij}=\begin{cases}
\sigma_{i}, i=j\leq s,\\
0 \text{ sinon},
\end{cases}
\end{equation}
c'est-à-dire que $ \Sigma_{s} $ résulte de $ \Sigma $ en remplaçant toutes les valeurs singulières $ \sigma_{i}=\Sigma_{ii} $ pour $ i> s $ par zéro.

 L'erreur d'approximation est alors
\begin{equation}\label{2.26b}
\begin{Vmatrix}
A-R
\end{Vmatrix}_{2}=\sigma_{s+1} \text{ et }\begin{Vmatrix}
A-R
\end{Vmatrix}_{F}=\sqrt{\sum_{i=s+1}^{\min\{m,n\}}\sigma_{i}^{2}}.
\end{equation}
L'inégalité \eqref{2.25} devient $ \sigma_{i}(A-R)= \sigma_{s+i}(A) $.
\end{enumerate}
\end{theo}
\begin{proof}
\emph{\\} Voir \cite[p.35]{hackbusch2012tensor}
\end{proof}
\begin{rem}[Meilleure approximation de rang-{\normalfont k}]
\emph{\\}Pour $ A\in\K^{m\times n} $ construire $ R $ comme dans \eqref{2.26a}. Alors $ R $ est la solution des deux problèmes de minimisation suivants:
\begin{equation}
\underset{rang(R)\leq r}{\min}\parallel A-R\parallel_{2}\text{ et }\underset{rang(R)\leq r}{\min}\parallel A-R\parallel_{F}. 
\end{equation}
Les valeurs des minima sont données dans \eqref{2.26b}. L'élément minimisant $ R $ est unique si et seulement si $ \sigma_{r} > \sigma_{r+1} $.
\end{rem}
%Le contenu du paragraphe suivant se restreint exclusivement au cas de la \textbf{dimension finie}:
%
%Considérons l'application $ \epsilon_{V}:V\rightarrow V^{**} $ définie par $ \epsilon_{V}(\v):\psi\rightarrow\psi(\v) $ pour tout $ \psi\in V^{*} $. L'application $ \v\rightarrow \epsilon_{V}(\v)(\psi) $ est une forme linéaire sur V.  Le résultat à présenter ci-dessous nous sera beaucoup utile lors de l'introduction des tenseurs comme formes multilinéaires. Sa démonstration est à retrouver dans \cite[p.25]{Michel2013}. 
% 
%\begin{prop}
%\emph{\\}
%Si V est de dimension finie alors l'application $ \epsilon_{V} $ est un isomorphisme linéaire entre V et son bidual $ V^{**} $.
%\end{prop}

\subsection{Espace vectoriel quotient}
 Soient V un espace vectoriel sur $ \K $ et W un sous-espace vectoriel de V. Considérons sur V la relation
d'équivalence
$$\v\Re \w\Rightarrow \v-\w\in W. $$
On note V/W l'ensemble quotient. Soit $ \overline{\v} $ la classe d'équivalence du vecteur $ \v $. Par définition
$$\overline{\v} = \{\w,\, \w=\v+\x,\x\in W\}.$$ Il est connu que si
$\acute{\v}$ $ \in $ $\overline{\v} $ et $\acute{\w}$ $ \in $ $\overline{\w} $, alors $ \overline{\v+\w} =\overline{\v'+\w'}$ et pour tout $\alpha$ $\in$$ \K $, $ \overline{\alpha \v}=\overline{\alpha \v'}. $
Ceci permet définir les opérations suivantes dans $ V/W$:

\begin{equation}
\overline{\v}+\overline{\w}=\overline{\v+\w}\text{ et }\alpha\overline{\v}=\overline{\alpha \v}.
\end{equation}

Ces deux opérations munissent $ V/W $ d'une structure d'un $ \K $-espace vectoriel appelée \textit{espace vectoriel quotient} \cite[p.50]{hackbusch2012tensor}.

L'application $ \pi :V\longrightarrow V/W $ définie par $\pi(\v) = \overline{\v}$ est linéaire et surjective. Elle est appelée la \textit{projection canonique}. 

Le théorème suivant est d'une importance capitale car, il va intervenir plus d'une fois dans les démonstrations des résultats essentiels relatifs au produit tensoriel.

\begin{theo}[Théorème de factorisation]
	\label{theofact}
	\emph{\\}
	Soit $ f : V\rightarrow W$ une application linéaire surjective. Il existe un unique isomorphisme 
	$$h : V/\ker(f)\longrightarrow W\text{ telle que } h\circ\pi=f \text{ \cite[p.12]{Michel2013}}.$$
\end{theo}

\begin{proof}
	\emph{\\}
Considérons le diagramme suivant:
\begin{center}
	\begin{tikzpicture}
	\node (A) at (0,2) {$V$};
	\node (B) at (2,2) {$W $};
	\node (C) at (2,0) {$V/\ker(f). $};
	\draw[->,very thick](A)--(B) node[midway,above,sloped,color=black]{$ f $};
	\draw[->,very thick](A)--(C) node[midway,left,color=black]{$ \pi $};
	\draw[->,very thick](C)--(B) node[midway,right,color=black]{$h$};
	\end{tikzpicture}
\end{center}
$ \forall \v\in V, \pi(\v)=\overline{\v},\, h\circ \pi(\v)=h[\pi(\v)]=h(\overline{\v})=f(\v),$  avec $ f $ et $ \pi $ des applications linéaires surjectives.\\
D'une part, montrons l'\textbf{existence} de $ h $:
\begin{list}{$ \ast $}{}
\item Montrons que $ h:V/\ker(f)\longrightarrow W,h(\overline{\v})=f(\v) $ est une application(bien définie).

En effet, soient $\overline{\v},\overline{\w}\in V/\ker(f) $ tels que $ \overline{\v}=\overline{\w} $. Alors, on a:
\begin{eqnarray}
&&\v\equiv \w [\mo\ker(f)]\nonumber\\
&\Leftrightarrow&\v-\w\in \ker(f)\text{ (opération dans $ V/\ker(f) $)}\nonumber\\
&\Leftrightarrow&f(\v-\w)=0 \text{ (par déf. de $ \ker(f) $)}\nonumber\\
&\Leftrightarrow& f(\v)-f(\w)=0\text{ (car f est linéaire)}\nonumber\\
&\Leftrightarrow & f(\v)=f(\w) \text{ (opération dans un $ \K $-ev.)} \nonumber\\
& \Leftrightarrow& h(\overline{\v})=h(\overline{\w}). \text{ (par déf. de $ h $)}\nonumber
\end{eqnarray}
$ h $ est donc une application bien définie (sa définition ne dépend pas du représentant (classe) choisi).

\item
h est un homomorphisme car $ \pi $ et $ f $ le sont ($ h\circ \pi=f $).

\item Montrons que h est un isomorphisme:


 \begin{tabular}{lll}
D'un côté, $ \ker(h) $&=&$ \{\overline{\v}\in V/\ker(f)/h(\overline{\v})=0\} $\\
	&=&$ \{\overline{\v}\in V/\ker(f)/f(\v)=0\} $\\
	&=&$ \{\overline{\v}\in V/\ker(f)/\v\in\ker(f)\} $\\
	&=&$ \{\overline{\mbf{0}}\} $; h est donc injectif.\\
\end{tabular}

D'un autre côté, h est surjectif car $ \pi $ et $ f $ le sont; ($ h\circ \pi=f $). D'où l'existence de l'isomorphisme h.\\
\end{list}
D'autre part, montrons l'\textbf{unicité} de $ h $:\\
Supposons qu'il existe un autre isomorphisme $ h':V/\ker (f)\longrightarrow W $ tel que $ h'\circ \pi=f $, on va montrer que $ h'=h $. En effet, soit $\overline{\v}\in V/\ker(f)$, son image par $ h' $ est $ h'(\overline{\v})=f(\v)=h(\v) $; $ h $ est donc unique.\\
D'où l'existence d'un unique isomorphisme $ h : V/\ker(f)\longrightarrow W\text{ telle que } h\circ\pi=f. $
\end{proof}
%\subsection{Convention d'Einstein}
%
%Soit E un $ \K $-espace vectoriel de dimension finie n. Nous écrirons dorénavant les vecteurs de base sous la
%forme $ e_{1}, e_{2},\dots,e_{n} $, les indices étant placés en bas (en indice !). Si v est un vecteur de E, sa décomposition
%dans la base $ \{e_{i}\}_{1\leq i\leq n} $ s'écrira
%$$v=x^{1}e_{1}+x^{2}e_{2}+\dots+x^{n}e_{n}=\sum_{i=1}^{n}x^{i}e_{i}.$$
%On remarque que dans cette notation les composantes de v s'écrivent $ (x^{1},\dots, x^{n}) $ les indices étant en haut
%(en exposant).
%
%Remarquons que la plupart des formules écrites ci-dessus, telles que les combinaisons linéaires, l'écriture d'un vecteur dans une base, fait intervenir des sommes par rapport à un
%indice répété deux fois. Pour alléger les notations, Einstein proposa de supprimer dans ces conditions le
%signe $\sum$ et d'adopter la convention suivante:
%
%\begin{rem}[Convention d'Einstein]
%	\emph{\\}
%	Toutes les fois que dans un monôme figure deux fois le même indice, une fois
%	en indice supérieur, une fois en indice inférieur, on doit, sauf avis contraire, sommer tous les monômes
%	obtenus en donnant à cet indice, toutes les valeurs possibles.  \cite[p.20]{Michel2013}
%\end{rem}
%Par exemple, l'écriture du vecteur v dans la base $ \{e_{i}\}_{1\leq i\leq n} $ s'écrit:
%\begin{equation}
%	v=x^{i}e_{i}.
%\end{equation}
%Toutefois, dans ce chapitre et dans la suite, afin de ne pas trop perturber nos habitudes, nous conserverons
%le signe $\sum$ dans les formules, mais nous respecterons la convention sur les indices et exposants.
%

\subsection{Applications  multilinéaires}
\begin{defi}[Forme bilinéaire]
\emph{\\}
Soit V un $ \K $-espace vectoriel. Une application $ \Phi :V\times V\rightarrow\K $
est appelée forme bilinéaire si elle vérifie pour tous  $\v_{1}, \v_{2},\w_{1},\w_{2}, \v,\w \in V$ et $a_{1}, a_{2}\in \K $:
\begin{center}
\begin{tabular}{cccc}
	$ (1) $&$ \Phi(a_{1}\v_{1}+a_{2}\v_{2},\w) $&=&$ a_{1}\Phi(\v_{1},\w)+a_{2}\Phi(\v_{2},\w), $\\
	$ (2) $&$ \Phi(\v,a_{1}\w_{1}+a_{2}\w_{2}) $&=&$ a_{1}\Phi(\v,\w_{1})+a_{2}\Phi(\v,\w_{2}).$\\
\end{tabular}
\end{center}
\end{defi}
Ceci est équivalent à dire que $\Phi$ est linéaire en chacun de ses arguments et à valeurs dans $ \K $. 
On note $ \L(V,V;\K) $ l'ensemble des formes bilinéaires sur V à valeurs dans $ \K $.
\begin{defi}[Application multilinéaire]
	\emph{\\}
	 Soient $V^{(1)}, \ldots, V^{(d)}$ et W des $ \K $-espaces vectoriels. Une application $$ \Phi :V^{(1)}\times \dots\times V^{(d)}\longrightarrow W $$ est multilinéaire si elle est linéaire sur chaque argument; c'est à-dire si pour tout\\  $\v^{(k)},\x^{(k)}\in V^{(k)}, \alpha,\beta\in \mathbb{K},$ $ k=1,\dots,d $,
	\begin{eqnarray}
		\Phi\left(\v^{(1)},\dots,\alpha \v^{(k)}+\beta \x^{(k)},\dots,\v^{(d)}\right)&=&\alpha \Phi\left(\v^{(1)},\dots,\v^{(k)},\dots,\v^{(d)}\right)+\nonumber\\
		&&\beta\Phi\left(\v^{(1)},\dots, \x^{(k)},\dots,\v^{(d)}\right).
	\end{eqnarray}
	
\end{defi}

On note par $ \L\left(V^{(1)},V^{(2)},\dots,V^{(d)}; W\right)$ l'ensemble de telles applications multilinéaires. Tout élément $ \Phi\in \L\left(V^{(1)},V^{(2)},\dots,V^{(d)}; W\right) $ ayant $ d $ arguments est aussi appelée application \textit{d-linéaire}. Pour $ d = 2 $, on parle
d'application bilinéaire et trilinéaire pour $ d = 3 $.
\begin{propp}\label{propdim}
\emph{\\}
L'ensemble  $ \L\left(V^{(1)},V^{(2)},\dots,V^{(d)}; W\right)$ est un $ \K $-espace vectoriel. Si chacun des $ V^{(k)}$ est de
dimension finie $ n^{(k)} $ et si W est de dimension $ p $, alors  $ \L\left(V^{(1)},V^{(2)},\dots,V^{(d)}; W\right)$ est de dimension finie et on a
\begin{equation}\label{dimmulti}
\dim\left(\L\left(V^{(1)},V^{(2)},\dots,V^{(d)}; W\right)\right)=p \cdot n^{(1)}n^{(2)}\cdots n^{(d)}.
\end{equation}
\end{propp}

%\begin{defi}[Application multilinéaire alternée]
%\emph{\\}
%Soient E et F des $ \K $ espaces vectoriels.\\
%Une application d-linéaire $ \Phi :E^{d}\longrightarrow F $
%est dite alternée, si $\Psi(v_{1},\cdots,v_{d})=0$ dès que deux vecteurs parmi les $ v_{1},\cdots,v_{d} $ sont égaux. 
%%\cite[p. 76]{Michel2013}
%\end{defi}
\section{Produit tensoriel des espaces vectoriels}
\label{sec:1.2tensor}
Cette section joue un rôle capital dans ce chapitre. En effet, c'est ici où nous allons construire un espace vectoriel quotient particulier appelé \textit{espace tensoriel}.
\subsection{Produit tensoriel de deux espaces vectoriels}
\subsubsection{Définition formelle}\label{section 3.2.1}
Soient V et W deux $ \K $-espaces vectoriels. Notons $ \K^{V\times W} $ le $ \K $-espace vectoriel dont une base est donnée par tous les couples $ (\v,\w) $ où $ \v\in V $ et $ \w\in W $. C'est-à-dire,
\begin{equation}
\K^{V\times W}=\left\{\sum_{i=1}^{m}\lambda_{i}(\v_{i},\w_{i}), \, \v_{i}\in V,\,\w_{i}\in W,\,\lambda_{i}\in \K \text{ et }m\in \N\right\}.
\end{equation}
 Soit N\footnote{D'après \cite[p.51]{hackbusch2012tensor}, une autre approche équivalente consiste à poser que\begin{equation*}
 	N=\spam\begin{Bmatrix}
 	\displaystyle\sum_{i=1}^{m}\displaystyle\sum_{j=1}^{n}\alpha_{i}\beta_{j}(v_{i},w_{j})-\left(\displaystyle\sum_{i=1}^{m}\alpha_{i}v_{i},\displaystyle\sum_{j=1}^{n}\beta_{j}w_{j}\right)\\
 	\\
 	\forall\, m,n\in \N^{*},\,\alpha_{i},\beta_{j}\in \K,\, v_{i}\in V,\,w_{j}\in W
 	\end{Bmatrix}.
 	\end{equation*}} le sous-espace de $ \K^{V\times W} $ engendré  par les
éléments de la forme
\begin{equation}
\begin{cases}
 (\lambda \v+\mu\v',\w)-\lambda(\v,\w)-\mu(\v',\w)\\
(\v,\lambda \w+\mu\w')-\lambda(\v,\w)-\mu(\v,\w'),\, \forall\lambda,\mu\in\K,\,\v,\v'\in V, \w,\w'\in W.
\end{cases}
\end{equation}

\begin{defi}
\emph{\\}
On appelle espace tensoriel algébrique des espaces vectoriels V et W, l’espace vectoriel quotient noté et défini par
\begin{equation}\label{3.10}
V\otimes_{a} W=\K^{V\times W}/N.
\end{equation}
\end{defi}

Considérons $ \pi: \K^{V\times W}\rightarrow V\otimes_{a} W$ la projection canonique qui, à un vecteur $ \sum_{i}\lambda_{i}(\v_{i},\w_{i}) $ de $ \K^{V\times W} $ fait correspondre sa classe d'équivalence dans l'espace vectoriel $ V\otimes_{a} W $. L'application $ \pi $ est linéaire;
$$\pi\left(\sum_{i}\lambda_{i}(\v_{i},\w_{i})\right)=\sum_{i}\lambda_{i} \pi(\v_{i},\w_{i}) $$ et l'espace vectoriel quotient $V\otimes_{a} W $ est engendré par les vecteurs $ \v\otimes \w $ avec $ (\v,\w)\in V\times W $. On notera $ \v\otimes \w $ la classe d'équivalence du couple $ (\v,\w) $;
\begin{equation}
\v\otimes \w=\pi(\v,\w)=\{(\v,\w)+\u,\u\in N\}.
\end{equation}
Ainsi, tout vecteur de $V\otimes_{a} W $ s'écrit comme une somme finie $ \underset{i\in I}{\sum}\lambda_{i}(\v_{i}\otimes \w_{i}) $ avec $ \v_{i}\in V $, $ \w_{i}\in W $, $ \lambda_{i}\in\K $ et $ I $ est un ensemble fini d'indices. En particulier,
\begin{equation}
V\otimes_{a} W=\{\v\otimes \w : \v\in V,\, \w\in W\}.
\end{equation}

\begin{defi}[Espace tensoriel, Tenseur]\label{defi1.20}
	\emph{\\}
	\begin{enumerate}
		\item[$ (a) $]
		$ V\otimes_{a} W $  est à nouveau un espace vectoriel, qui est maintenant appelé \textquotedblleft espace tensoriel\textquotedblright.
		\item[$ (b) $]
		Les éléments de $ V\otimes_{a} W $ sont appelés \textquotedblleft tenseurs\textquotedblright, et en particulier $ \mathbf{A}\in V\otimes_{a} W $ est un tenseur algébrique\footnote{Il existe de même des tenseurs  dits \textit{topologiques}, mais, ce cas ne sera pas traité dans ce travail.}.
		\item[$ (c) $]
		Tout produit $ \v\otimes \w \,(\v\in V, \w\in W)$ est appelé \textquotedblleft tenseur élémentaire\textquotedblright.
	\end{enumerate}
\end{defi}
La remarque ci-dessous présente quelques conventions générales à considérer dans tout le reste:
\begin{rem}
	\emph{\\}
	\begin{list}{$\star$}{}
		\item
		Pour les espaces vectoriels V et W de dimensions finies, on omet le suffixe $ a $ et on écrit simplement $ V\otimes W. $ 
		\item
Le suffixe de  $ V\otimes_{a} W $ (a pour algébrique) sera déplacé vers la gauche lorsque des indices apparaissent sur le coté droit comme dans $  \agktensor V^{(k)}$.
	\end{list}
\end{rem}

\subsubsection{Propriétés caractéristiques}
\begin{propp}\label{lem3.11}
	\emph{\\}
	Les propriétés algébriques caractéristiques de l'espace tensoriel $V\otimes_{a} W  $ se traduisent par la bilinéarité:
	\begin{eqnarray}\label{3.13}
	&(a)&(\lambda \v)\otimes \w=\v\otimes(\lambda \w)=\lambda\cdot(\v\otimes \w),\: \forall\lambda\in \K,\, \v\in V,\w\in W,\nonumber\\
	&(b)&(\v'+\v'')\otimes \w= (\v'\otimes \w)+(\v''\otimes \w),\:\forall \v', \v''\in V,\, \w\in W,\nonumber\\
	&(c)&\v\otimes(\w'+\w'')=(\v\otimes \w')+(\v\otimes \w''),\:\forall \v\in V, \w',\w''\in W,\\
	&(d)&\mbf{0}\otimes \w=\v\otimes \mbf{0}=\mbf{0},\:\forall \v\in V, \w\in W.\nonumber
	\end{eqnarray}
	%De même, $ v\otimes 1=v, \quad\forall v\in V,1\in \K. $	
\end{propp}

La \defref{defitensor} à présenter ci-dessous est une caractérisation de l'espace tensoriel issu de deux espaces vectoriels et, s'explicitera davantage par les résultats la succédant.

\begin{defi}[Produit et espace tensoriels]\label{defitensor}
	\emph{\\}
	Soient  trois $ \K $-espaces vectoriels V, W et $ \V $. Une application $ \otimes :V\times W \rightarrow  $$ \mathcal{V} $ est un produit tensoriel et $ \mathcal{V} $ un espace tensoriel, c'est-à-dire qu'il est isomorphe à $ V\otimes_{a} W$, si les propriétés suivantes sont vérifiées:
	\begin{itemize}
		\item[$ (1) $]
		Propriété $ \spam $: $ \mathcal{V} =\spam\{\v\otimes \w :\v\in V,\, \w\in W \};$
		\item[$ (2) $]
		$ \otimes $ est bilinéaire (voir \eqref{3.13});
		\item[$ (3) $]
		L'indépendance linéaire des vecteurs $ \{\v_{i}\}_{1\leq i\leq n}\subset V $ et $ \{\w_{j}\}_{1\leq j\leq m}\subset W $ implique celle des vecteurs $ \{\v_{i}\otimes \w_{j}\} $ dans $ \mathcal{V} $ \cite[p. 54]{hackbusch2012tensor}.
	\end{itemize}
\end{defi}
La bilinéarité de $ \otimes $ au sens usuel se traduit par la \propref{proptensor} donnée ci-dessous.
\begin{prop}\label{proptensor}
	\emph{\\}
	L’application  canonique $t:V\times W\rightarrow V\otimes_{a} W$ définie par $t (\v,\w) \longrightarrow \v\otimes \w $ est bilinéaire. 
	%et les produits $ x\otimes y $ engendrent l’espace vectoriel $ E\otimes F $.
\end{prop}

\begin{proof}
	\emph{\\}
Montrons que l'application $ t: V\times W \longrightarrow V\otimes_{a} W$ définie par $ t(\v,\w)=\v\otimes \w $ est bilinéaire.
\begin{list}{$ \ast $}{}
\item D'une part, $ \forall \v,\v'\in V $, $ \w\in W $, $ a,b\in\K $, on a:
\begin{eqnarray}
t(a\v+b\v',\w)&=&(a\v+b\v')\otimes \w \quad\text{(par définition de t)}\nonumber\\
&=&(a\v\otimes \w)+(b\v'\otimes \w)\quad\text{(en vertu de \eqref{3.13}(b))}\nonumber\\
&=&a(\v\otimes \w)+b(\v'\otimes \w)\quad\text{(en vertu de \eqref{3.13}(a))}\nonumber\\
&=&at(\v,\w)+bt(\v',\w).\quad\text{(par définition de t)}\label{(1.22)}
\end{eqnarray}
\item D'autre part, $ \forall \v\in V $, $ \w,\w'\in W $, $ a,b\in\K $, on a:
\begin{eqnarray}
t(\v,a\w+b\w')&=&\v\otimes(a\w+b\w')\quad\text{(par définition de t)}\nonumber\\
&=&(\v\otimes a\w)+(\v\otimes b\w')\quad\text{(en vertu de \eqref{3.13}(c))}\nonumber\\
&=&a(\v\otimes \w)+b(\v\otimes \w')\quad\text{(en vertu de \eqref{3.13}(a))}\nonumber\\
&=&at(\v,\w)+bt(\v,\w').\quad\text{(par définition de t)}\label{(1.23)}
\end{eqnarray}
\end{list}
De \eqref{(1.22)} et \eqref{(1.23)}, on tire que t est bilinéaire. D'où la proposition.
\end{proof}
\subsubsection{Propriété universelle du produit tensoriel}
Les résultats à présenter dans cette sous section montrent un caractère unique du produit tensoriel permettant de transposer les propriétés de bilinéarité dans la catégorie des espaces vectoriels et des applications linéaires.
\begin{prop}[Propriété fondamentale du produit tensoriel]\label{prop1.3}
\emph{\\}
Pour tout espace vectoriel M sur $ \K $ et toute application bilinéaire $\phi:V\times W\longrightarrow M  $, il existe une unique application linéaire $ f:V\otimes_{a} W \longrightarrow M$ telle que $ \phi(\v,\w)=f(\v\otimes \w) $, $ \forall $ $ \v\in V $ et $ \w\in W $.
\end{prop}
\begin{proof}
	\emph{\\}
	Nous traduisons cette propriété en disant que le diagramme suivant 
	\begin{center}
	\begin{tikzpicture}
	\node (A) at (0,2) {$ V\times W $};
	\node (B) at (2,0) {$ M $};
	\node (C) at (2,2) {$ V\otimes_{a} W $};
	\draw[->,very thick](A)--(B) node[midway,below,color=black]{$ \phi $};
	\draw[->,very thick](A)--(C) node[midway,above,color=black]{$ t $};
	\draw[->,very thick](C)--(B) node[midway,right,color=black]{$f$};
	\end{tikzpicture}
\end{center}est commutatif.

$ \mapsto $ D'une part, montrons l'existence de f:

Considérons l'application bilinéaire $ g:\K^{V\times W} \longrightarrow M$ définie par $$ g\left(\sum a_{i}(\v_{i},\w_{i})\right)=\sum a_{i}\phi(\v_{i},\w_{i}). $$
 $ \ker(g)=\left\{\sum a_{i}(\v_{i},\w_{i})\in\K^{V\times W}/g\left(\sum a_{i}(\v_{i},\w_{i})\right)=0\right\} $. On a $ N\subset \ker(g) $;

En effet, comme $ \phi $ est bilinéaire, $ \forall \v,\v'\in V $, $ \w\in W $ et $ a,b\in\K $, on a:
\begin{eqnarray}
g((a\v+b\v',\w)-a(\v,\w)-b(\v',\w))&=&g(a\v+b\v',\w)-ag(\v,\w)-bg(\v',\w)\nonumber\\
&=&\phi(a\v+b\v',\w)-a\phi(\v,\w)-b\phi(\v',\w) \nonumber\\
&=&a\phi(\v,\w)+b\phi(\v',\w)-a\phi(\v,\w)-b\phi(\v',\w)\nonumber\\
&=&0.\nonumber
\end{eqnarray}
De même,
\begin{eqnarray}
g((\v,a\w+b\w')-a(\v,\w)-b(\v,\w'))&=&g(\v,a\w+b\w')-ag(\v,\w)-bg(\v,\w')\nonumber\\
&=&\phi(\v,a\w+b\w')-a\phi(\v,\w)-b\phi(\v,\w') \nonumber\\
&=&a\phi(\v,\w)+b\phi(\v,\w')-a\phi(\v,\w)-b\phi(\v,\w')\nonumber\\
&=&0.\nonumber
\end{eqnarray}
Donc tous les générateurs de N sont dans $ \ker(g) $ et donc N est un sous-espace vectoriel de $ \ker(g) $. Le \theoref{theofact} implique qu'il existe une application linéaire f tel que le diagramme suivant soit commutatif:	\begin{center}
	\begin{tikzpicture}
	\node (A) at (0,2) {$ \K^{V\times W }$};
	\node (B) at (2,2) {$ V\otimes_{a}W $};
	\node (C) at (2,0) {$ M $};
	\draw[->,very thick](A)--(B) node[midway,above,sloped,color=black]{$ \pi $};
	\draw[->,very thick](A)--(C) node[midway,left,color=black]{$ g $};
	\draw[->,very thick](B)--(C) node[midway,right,color=black]{$f$};
	\end{tikzpicture}
\end{center}
L'application linéaire $ f $ vérifie $ f(\v\otimes \w) =g(\v,\w)=\phi(\v,\w)$. L'existence est donc démontrée.
  
$ \mapsto $ Montrons l'unicité de f:

L'unicité de f résulte directement de la formule $ f(\v\otimes \w)=\phi(\v,\w)$ et du fait que les vecteurs $ \v\otimes \w $ engendrent $ V\otimes_{a} W.$
\end{proof}

\begin{theo}[Propriété universelle du produit tensoriel]\label{theo1.3}\emph{\\}
Soit G un $ \K $-espace vectoriel et $ t': V\times W\rightarrow G  $ une application bilinéaire surjective. Supposons que pour toute application bilinéaire $ \phi :V\times W\rightarrow M $ on ait la factorisation 
	\begin{center}
	\begin{tikzpicture}
	\node (A) at (0,2) {$ V\times W $};
	\node (B) at (2,2) {G};
	\node (C) at (2,0) {$ M $};
	\draw[->,very thick](A)--(B) node[midway,above,sloped,color=black]{$ t' $};
	\draw[->,very thick](A)--(C) node[midway,left,color=black]{$ \phi $};
	\draw[->,very thick](B)--(C) node[midway,right,color=black]{$f'$};
	\end{tikzpicture}
\end{center}
avec $ f' $ linéaire. Alors G est isomorphe au produit tensoriel $ V\otimes_{a} W $ \cite[p.102]{Michel2013}.
\end{theo}

\begin{proof}
	\emph{\\}
	Prenons pour M, dans la \propref{prop1.3}, l'espace $ V\otimes_{a} W $ et pour application bilinéaire $ \phi $, l'application $ t $. On a le diagramme suivant:
	\begin{center}
		\begin{tikzpicture}
		\node (A) at (0,2) {$ V\times W $};
		\node (B) at (2,2) {G};
		\node (C) at (2,0) {$ V\otimes_{a} W$};
		\draw[->,very thick](A)--(B) node[midway,above,sloped,color=black]{$ t' $};
		\draw[->,very thick](A)--(C) node[midway,below,left,color=black]{$ t $};
		\draw[->,very thick](B)--(C) node[midway,below,right,color=black]{$f'$};
		\end{tikzpicture}
	\end{center}
	et donc $ f'(t'(\v,\w))=\v\otimes \w $. De même, en prenant pour M l'espace vectoriel G, on récupère le diagramme commutatif:
		\begin{center}
		\begin{tikzpicture}
		\node (A) at (0,2) {$ V\times W $};
		\node (B) at (2,2) {$ V\otimes_{a} W $};
		\node (C) at (2,0) {$ G $};
		\draw[->,very thick](A)--(B) node[midway,above,sloped,color=black]{$ t $};
		\draw[->,very thick](A)--(C) node[midway,left,color=black]{$ t' $};
		\draw[->,very thick](B)--(C) node[midway,right,color=black]{$f''$};
		\end{tikzpicture}
	\end{center}
	et $ f''(\v\otimes \w)=t'(\v,\w) $. D'où $ f'[f''(\v\otimes \w)]=f'[t'(\v,\w)]=\v\otimes \w $ et $ f''[f'(t'(\v,\w))]=f''(\v\otimes \w)=t'(\v,\w) $. Comme $ t' $ est surjective, on en déduit que $ f''\circ f'=id_{G} $.
\end{proof}
Le \theoref{theo1.3} admet comme conséquence: la détermination du produit tensoriel de deux espaces vectoriels peut donc se faire soit en utilisant la définition, soit en utilisant la propriété universelle.


\begin{prop}\label{rem:1}
\emph{\\}
Étant donnés trois $ \K$-espaces vectoriels V, W et M, il existe un isomorphisme linéaire $$\Psi:\L(V\otimes_{a} W, M) \rightarrow \L(V,W;M)$$ entre l'espace des applications linéaires de $ V\otimes_{a} W $ à valeurs dans M et l'espace des applications bilinéaires de $ V\times W $ dans M. %\cite[p.103]{Michel2013}
\end{prop}
\begin{proof}
\emph{\\}
Cet isomorphisme résulte en effet du diagramme commutatif 		\begin{center}
	\begin{tikzpicture}
	\node (A) at (0,2) {$ V\times W $};
	\node (B) at (2,2) {$ V\otimes_{a} W $};
	\node (C) at (2,0) {$ M. $};
	\draw[->,very thick](A)--(B) node[midway,above,sloped,color=black]{$ t $};
	\draw[->,very thick](A)--(C) node[midway,left,color=black]{$ \phi $};
	\draw[->,very thick](B)--(C) node[midway,right,color=black]{$f$};
	\end{tikzpicture}
\end{center}
Considérons une application linéaire $ f\in\L(V\otimes_{a} W, M) .$ Posons $ \Psi(f)(\v,\w)=f(t(\v,\w)) $ pour tout $ (\v,\w)\in V\times W $. L'application $ \Psi(f) $ est bilinéaire et le diagramme précédent montre que $ \Psi $ est surjective. Supposons $ \Psi(f)=0 $. Alors $ f(\v\otimes \w)=0 $ pour tout $ (\v,\w)\in V\otimes_{a} W $. Comme les vecteurs $ \v\otimes \w $ engendrent $ V\otimes_{a} W $, on en déduit que $ f $ est l'application nulle. Ainsi $ \Psi $ est un isomorphisme.
\end{proof}

\subsubsection{Écriture  spécifique d'un vecteur de $ V\otimes_{a} W $ }
\begin{prop}
\emph{\\}
Soient $ \v $ et $ \w $ deux vecteurs non nuls de $ V $ et $ W $ respectivement. Alors $ \v\otimes\w\neq \ov $ \cite[p.104]{Michel2013}.
\end{prop}

\begin{proof}
\emph{\\}
En effet, comme $ \v $ et $ \w $ sont non nuls, il existe des formes linéaires non nulles $ f\in V^{*}$ et $ g\in W^{*} $ telles que $ f(\v)\neq 0 $ et $ g(\w)\neq 0 $.

Soit $ \psi:V\times W\rightarrow\K $ la forme bilinéaire définie par $ \psi(\v,\w)=f(\v)g(\w) $. D'après le \theoref{theofact}, il existe une unique application linéaire $ h:V\at W\rightarrow\K $ telle que $ h(\v\otimes\w)=\psi(\v,\w)=f(\v)g(\w) $. Comme $ \psi(\v,\w)\neq 0 $, on en déduit $ h(\v\otimes\w)\neq0 $ et le vecteur $ \v\otimes\w $ est non nul.
\end{proof}



\begin{cor}\label{cor:6}
\emph{\\}
Soient les vecteurs $ \v\in V $ et $ \w\in W. $ Alors l'équation \begin{equation}
\v\otimes \w=\ov
\end{equation}implique $ \v=\ov $ ou $ \w=\ov $ \cite[p.104]{Michel2013}.
\end{cor}
\begin{prop}\label{prop1.5}
\emph{\\}
Tout vecteur $ \z\neq \ov $ de $ V\otimes_{a} W $ s'écrit sous la forme
\begin{equation}\label{0.1}
\z=\sum_{i=1}^{R}\v_{i}\otimes\w_{i}
\end{equation} où les vecteurs $ \{\v_{1},\cdots,\v_{R}\} $ sont linéairement indépendants dans V et les vecteurs $ \{\w_{1},\cdots,\w_{R}\} $ sont linéairement indépendants dans W \cite[p.54]{hackbusch2012tensor}.
\end{prop}
\begin{proof}
\emph{\\}
Choisissons une écriture de $ \z $, $ \z=\sum_{i=1}^{R}\v_{i}\otimes \w_{i} $, où $ R $ est minimal. Si $ R=1 , \z=\v\otimes \w$. Comme $ \z\neq \ov $, d'après le  \corref{cor:6}, on a $ \v\neq \ov $ et $ \w\neq \ov $ et donc ces vecteurs sont libres. Supposons que $ R\geq 2 $. Si la famille $ \{\v_{1},\cdots,\v_{R}\} $ est liée, un de ces vecteurs peut s'écrire comme combinaison linéaire des autres que nous pouvons écrire sans perdre de généralité: $ \v_{R}=\sum_{i=1}^{R-1}\alpha_{i}\v_{i} $. Ainsi,

\begin{eqnarray}
\z&=&\sum_{i=1}^{R-1}\v_{i}\otimes \w_{i}+(\v_{R}\otimes \w_{R})\nonumber\\
&=&\sum_{i=1}^{R-1} \v_{i}\otimes \w_{i}+\left(\sum_{i=1}^{R-1}\alpha_{i}\v_{i}\otimes \w_{R}\right)\nonumber\\
&=&\sum_{i=1}^{R-1}\v_{i}\otimes\underset{\w'_{i}}{\underbrace{(\w_{i}+\alpha_{i}\w_{R})}} \text{ (en vertu de \eqref{3.13}(a))}\nonumber\\
&=&\sum_{i=1}^{R-1}\v_{i}\otimes \w'_{i}.\nonumber
\end{eqnarray}
 Et cette écriture contredit la minimalité de $ R $. Ainsi les vecteurs $ \{\v_{1},\cdots,\v_{R}\} $ sont linéairement indépendants dans V. Supposons à présent que la famille de vecteurs de W, $ \{\w_{1},\cdots,\w_{R}\} $ soit liée; un de ces vecteurs peut s'écrire comme combinaison linéaire des autres, supposons que $ \w_{R}=\sum_{i=1}^{R-1}\alpha_{i}\w_{i} $. Ainsi,
 
 \begin{eqnarray}
 \z&=&\sum_{i=1}^{R-1} \v_{i}\otimes \w_{i}+(\v_{R}\otimes \w_{R})\nonumber\\
 &=&\sum_{i=1}^{R-1} \v_{i}\otimes \w_{i}+\left(\v_{R}\otimes \sum_{i=1}^{R-1}\alpha_{i}\w_{i}\right)\nonumber\\
 &=&\sum_{i=1}^{R-1}\underset{\v'_{i}}{\underbrace{(\v_{i}+\alpha_{i}\v_{R})}\otimes \w_{i}} \text{ (en vertu de \eqref{3.13}(a))}\nonumber\\
 &=&\sum_{i=1}^{R-1}\v'_{i}\otimes \w_{i}.\nonumber
 \end{eqnarray} Ce raisonnement  conduit à une contradiction sur la minimalité de $ R $. Ainsi la famille $ \{\w_{1},\cdots,\w_{R}\} $ est libre dans W. D'où la proposition.
\end{proof}
Ce résultat admet comme conséquence: Tout vecteur de la forme $ \z=\sum_{i=1}^{R}\v_{i}\otimes \w_{i} $, où les vecteurs $ \{\v_{1},\cdots, \v_{R}\} $ sont linéairement indépendants ainsi que les vecteurs 
$ \{\w_{1},\cdots, \w_{R}\} $, est non nul \cite[p.105]{Michel2013}.

%L'entier naturel minimal $ r $ apparaissant dans l'équation \eqref{0.1} sera appelé plutard rang du tenseur $ \z $. 

\begin{prop}
\emph{\\}
Soient $ \a_{1},\cdots,\a_{R} $ des vecteurs quelconques de V. Alors
pour tous $ \b_{1},\cdots,\b_{R} $ indépendants dans W, la relation
$$ \sum_{i=1}^{R}\a_{i}\otimes \b_{i}=\ov$$
entraîne $ \a_{1}=\cdots=\a_{R} =\ov$ \cite[p.11]{Awanegeo}.
\end{prop}

\begin{proof}
\emph{\\}
Supposons qu'il existe $ i(i=1,\cdots,R) $ tel que $ \a_{i} $ ne soit pas nul. Considérons la forme linéaire $ a^{i} $ sur V telle que $ a^{i}(\a_{i})=1 $ et pour $ j(j=1,\cdots,R) $, on considère la forme linéaire $ b^{j} $ sur W définie par
$$b^{j}(\b_{k})=\delta_{k}^{j}=\begin{cases}
1\text{ si } j=k,\\
0\text{ sinon.}
\end{cases} $$
Soit $ \phi:V\times W \rightarrow \K,$ la forme bilinéaire sur $ V\times W $ définie par: $\phi(\v,\w)=a^{i}(\v)b^{i}(\w) $  pour tous $ (\v,\w)\in V\times W $.  D'après la  \propref{prop1.3}, il existe une forme linéaire unique $ f $ sur $ V\otimes_{a} W $ telle que 
$f(\v\otimes\w)=\phi(\v,\w)=a^{i}(\v)b^{i}(\w). $ En particulier, on a:
$$0=f\left(\sum_{j=1}^{R}\a_{j}\otimes \b_{j}\right)=\sum_{j=1}^{R}f(\a_{j}\otimes \b_{j})=\sum_{j=1}^{R}a^{i}(\a_{j})b^{i}(\b_{j}) =1,$$ ce qui est absurde, par conséquent $ \a_{1}=\cdots=\a_{R}=\ov. $
\end{proof}
Le résultat suivant se limite aux $ \K $-espaces vectoriels V et W  de dimension finie. 
\begin{prop}\label{basis}
	\emph{\\}
	Si V et W sont des espaces de dimension finie, il en est de même de $ V\otimes W $ et l'on a
	\begin{enumerate}
	\item[(a)]$ \dim (V\otimes W)=\dim(V)\cdot\dim(W). $
	\item[(b)] Si $ \{\v_{i}\}_{i=1,\cdots,n} $ est une base de V et $ \{\w_{j}\}_{j=1,\cdots,m} $ une base de W, alors $ \{\v_{i}\otimes \w_{j}\}$ est une base de $ V\otimes W$ \cite[p. 53]{hackbusch2012tensor}.
	\end{enumerate}
\end{prop}
	
\begin{proof}
	\emph{\\}
Soit M un $ \K $-espace vectoriel de dimension finie. D'après la  \remref{rem:1}, l'espace vectoriel $ \L(V,W;M) $ des applications bilinéaires de $ V\times W $ dans  M  et l'espace vectoriel $ \L(V\otimes W,M) $ des applications linéaires de $ V\otimes W $ dans M sont isomorphes. Comme la dimension de $ \L(V,W;M) $ est égale à $ \dim V.\dim W.\dim M $, on en déduit que $ \dim \L(V\otimes W,M)=\dim V.\dim W.\dim M  $. Du coup,
\begin{equation}
\dim V\otimes W=\dim V. \dim W.
\end{equation}
Considérons une base $ \{\v_{i}\}_{i=1,\cdots,n} $ de V et $ \{\w_{j}\}_{j=1,\cdots,m} $ une base de W. Par définition du produit tensoriel, les vecteurs $ \v_{i}\otimes \w_{j} $ engendrent $ V\otimes W $, comme il y en a autant que la dimension de $ V\otimes W $, ils forment une base de cette espace.
\end{proof}
Étant donnés deux $ \K $-espaces vectoriels V et W, on peut construire les espaces produit tensoriel $ V\otimes_{a}W $ et $ W\otimes_{a} V $. Ils ne sont évidemment pas égaux. Toutefois, on a le résultat suivant:
\begin{theo}
\emph{\\}
Il existe un isomorphisme et un seul de $ V\otimes_{a} W $ sur $ W\otimes_{a} V $
appliquant $ \v\otimes \w $ sur $ \w\otimes \v $ \cite[p.13]{Awanegeo}.
\end{theo} 
\begin{proof}
\emph{\\}
Soit $ h:V\times W\rightarrow W\otimes_{a} V $ l'application bilinéaire définie par: $ h(\v,\w)=\w\otimes \v, $ pour tous $ \v\in V $ et $ \w\in W $. En vertu de la \propref{prop1.3}, il existe une application linéaire unique $ \tilde{h} $ de $ V\otimes_{a} W $ dans $ W\otimes_{a} V $ telle que $\tilde{h}(\v\otimes \w)=\w\otimes \v$; en vertu 

De même, il existe une application linéaire unique $ \tilde{g} $ de $ W\otimes_{a} V $ dans $ V\otimes_{a} W $ telle que   $\tilde{g}(\w\otimes \v)=\v\otimes \w$,
 pour tous $ \v\in V $ et $ \w\in W. $ Et donc,
 $$\tilde{h}\circ \tilde{g}(\w\otimes \v)=\tilde{h}[\tilde{g}(\w\otimes \v)]=\tilde{h}(\v\otimes \w) =\w\otimes \v=id_{W\otimes_{a} V}$$ et 
  $$\tilde{g}\circ \tilde{h}(\v\otimes \w)=\tilde{g}[\tilde{h}(\v\otimes \w)]=\tilde{g}(\w\otimes \v) =\v\otimes \w=id_{V\otimes_{a} W}$$
  Il en résulte qu'il existe une application linéaire unique $ \tilde{h} $ de $ V\otimes_{a} W $ sur $ W\otimes_{a} V $ telle que
  
   $\tilde{h}(\v\otimes \w)=\w\otimes \v \text{ pour tous }  \v\in V  \text{ et } \w\in W. $
\end{proof}
\subsection{Produit tensoriel de plusieurs espaces vectoriels}
La sous section suivante va présenter maintenant le produit tensoriel de plusieurs espaces vectoriels; qui est effectivement la généralisation du cas de deux espaces vectoriels. En dépit de tout ce qui a été dit ci-haut, on va montrer que le produit tensoriel est associatif; raison pour laquelle certains résultats ci-haut présentés ne seront plus détaillés.
\subsubsection{Définition formelle}
Nous supposons dans ce paragraphe que chaque $ V^{(k)}, k = 1,\cdots,d $, est un $ \K $-espace vectoriel. Considérons $ U = \K^{V^{(1)}\times \cdots\times V^{(d)}} $ l'espace vectoriel sur $ \K $ dont une base est donnée par tous les d-uplets $ (\v^{(1)}, \v^{(2)},\cdots, \v^{(d)}) $
où $ \v^{(k)} \in V^{(k)} $ et N le sous espace de U engendré par les vecteurs de la forme
$$\begin{cases}
(\v^{(1)},\cdots,\v^{(k-1)},\v^{(k)}+\w^{(k)},\v^{(k+1)},\cdots,\v^{(d)})-(\v^{(1)},\cdots,\v^{(k-1)},\v^{(k)},\v^{(k+1)},\cdots,\v^{(d)})\\
-(\v^{(1)},\cdots,\v^{(k-1)},\w^{(k)},\v^{(k+1)},\cdots,\v^{(d)}),\\
(\v^{(1)},\cdots,\v^{(k-1)},a\v^{(k)},\v^{(k+1)},\cdots,\v^{(d)})-a(\v^{(1)},\cdots,\v^{(k-1)},\v^{(k)},\v^{(k+1)},\cdots,\v^{(d)})
\end{cases} $$
où $ a\in\K $ et $ \v^{(k)},\w^{(k)}\in V^{(k)} $ ceci pour $ k=1,\cdots,d. $

\begin{defi}
	\emph{\\}
L'espace tensoriel algébrique $ V^{(1)}\otimes_{a}\cdots\otimes_{a}V^{(d)} $ des $ \K $-espaces vectoriels $ V^{(1)},\cdots,V^{(d)} $ est le $ \K $-espace vectoriel $$\V=\agktensor V^{(k)}= U/N. $$
\end{defi}
On note $ \ktensor \v^{(k)} $ la classe d'équivalence, dans $ U/N $ de $ (\v^{(1)},\cdots,\v^{(d)}) $. C'est-à-dire si $$\pi :U=\K^{V^{(1)}\times \cdots\times V^{(d)}}\longrightarrow U/N$$ est la projection linéaire canonique, alors
$$\ktensor \v^{(k)}=\pi(\v^{(1)},\cdots,\v^{(d)}). $$
Comme la famille des $ (\v^{(1)},\cdots,\v^{(d)}) $ est une base de V, les vecteurs $ \ktensor \v^{(k)} $ engendrent l'espace tensoriel $ \V=\agktensor V^{(k)} $.
 
La définition suivante caractérise l'espace tensoriel issu de plus de deux espaces vectoriels et s'explicitera davantage par les résultats la succédant.
\begin{defi}[Produit et espace tensoriels généralisés]\label{poposition3.21}
	\emph{\\}
Considérons $ d+1 $ $ \K $-espaces vectoriels $ V^{(1)} $, $ V^{(2)} $, $ \dots $, $ V^{(d)} $ des dimensions respectives  $ n^{(1)} $, $ n^{(2)} $, $ \dots $, $ n^{(d)} $ et $ \V $. De même, soient
	$$B^{(k)}=\left(\mbf{e}_{j_{k}}^{(k)}\right),\, 1\leq j_{k}\leq n^{(k)} $$ les bases respectives de ces $ d $ espaces vectoriels $ \left(V^{(k)}\right) $, $ k=1:d $. L'application $$ \otimes : V^{(1)}\times V^{(2)}\times \cdots\times V^{(d)}\longrightarrow \V $$ est un produit tensoriel et $ \V $ un espace tensoriel algébrique si les propriétés suivantes sont vérifiées (\cite[p.125]{Nyenyezi2018}):
	\begin{enumerate}
		\item[$ (1) $]
		Propriété $ \spam $: $\V=\spam\left\{\ktensor \v^{(k)},\v^{(k)}\in V^{(k)}\right\}$,
		\item[$ (2) $]
		$ \otimes $ est multilinéaire: c'est-à-dire $ \forall \lambda\in \K,\, \v^{(k)},\w^{(k)}\in V^{(k)} $ et $ k\in \{1,\dots,d\} $:
		\begin{eqnarray}
		&\v^{(1)}\otimes \v^{(2)}\otimes\cdots\otimes\left(\lambda \v^{(k)}+\w^{(k)}\right)\otimes \cdots\otimes \v^{(d)}=&\nonumber\\
		&\lambda (\v^{(1)}\otimes \v^{(2)}\otimes\cdots\otimes \v^{(k)}\otimes \cdots
		\otimes \v^{(d)})+\left(\v^{(1)}\otimes \v^{(2)}\otimes\cdots\otimes \w^{(k)}\otimes \cdots\otimes \v^{(d)}\right),&\nonumber
		\end{eqnarray}
		\item[$ (3) $]
		L'indépendance linéaire des vecteurs $ \left\{\v_{j_{k}}^{(k)}\right\}\subset V^{(k)}$  implique celle des vecteurs $ \left\{\ktensor \v_{j_{k}}^{(k)}\right\}\subset \V $, où $ j_{k}\in I_{k}=\{1,\dots,n^{(k)}\} $ et $ n^{(k)} $ est la dimension de l'espace vectoriel $ V^{(k)} $.
		
		Alors, l'espace vectoriel $ \V $ muni de cette application (ce produit tensoriel) est appelé espace tensoriel algébrique ou simplement espace tensoriel et  est souvent noté \\$ \V=_{a}\gktensor V^{(k)} $ (a, pour algébrique) et sa dimension est donnée par
		\begin{equation}\label{3.17}
		\dim(\V)=\prod_{k=1}^{d}\dim \left(V^{(k)}\right).
		\end{equation}
	\end{enumerate}
	
\end{defi}


\subsubsection{Propriétés caractéristiques du produit tensoriel}
Notons $ t: V^{(1)}\times \cdots\times V^{(d)}\rightarrow \V $ l'application linéaire composée de l'injection linéaire de $ V^{(1)}\times \cdots\times V^{(d)} $ dans U et de la projection canonique $ \pi:U\longrightarrow U/N $. Cette application vérifie
$$t(\v^{(1)},\cdots,\v^{(d)})=\v^{(1)}\otimes\cdots \otimes\v^{(d)} $$ pour tout $ (\v^{(1)},\cdots,\v^{(d)})\in V^{(1)}\times \cdots\times V^{(d)} $. 
\begin{prop}\label{prop:1.9}
\emph{\\}
L'application $t: V^{(1)}\times \cdots\times V^{(d)}\longrightarrow \V $ définie par $ t(\v^{(1)},\cdots,\v^{(d)})\longrightarrow \ktensor \v^{(k)}$ est d-linéaire.
\end{prop}
\begin{proof}
\emph{\\}
On a, pour tout $ \v^{(k)},\w^{(k)} \in V^{(k)}$ et $ a,b\in \K $:
\begin{eqnarray}
&&t(\v^{(1)},\cdots,\v^{(k-1)},a\v^{(k)}+b\w^{(k)},\v^{(k+1)},\cdots,\v^{(d)})\nonumber\\
&=&\pi(\v^{(1)},\cdots,\v^{(k-1)},a\v^{(k)}+b\w^{(k)},
\v^{(k+1)},\cdots,\v^{(d)}).\nonumber
\end{eqnarray} D'après la définition de N:
\begin{eqnarray}
&&\pi(\v^{(1)},\cdots,\v^{(k-1)},a\v^{(k)}+b\w^{(k)},\v^{(k+1)},\cdots,\v^{(d)})\nonumber\\ &=&\pi(a(\v^{(1)},\cdots,\v^{(k-1)},\v^{(k)},\v^{(k+1)},\cdots,\v^{(d)})+b(\v^{(1)},\cdots,\v^{(k-1)},\w^{(k)},\v^{(k+1)},\cdots,\v^{(d)})\nonumber\\
&=&a\pi(\v^{(1)},\cdots,\v^{(k-1)},\v^{(k)},\v^{(k+1)},\cdots,\v^{(d)})+b\pi(\v^{(1)},\cdots,\v^{(k-1)},\w^{(k)},\v^{(k+1)},\cdots,\v^{(d)})\nonumber\\
&=&at(\v^{(1)},\cdots,\v^{(k-1)},\v^{(k)},\v^{(k+1)},\cdots,\v^{(d)})+bt(\v^{(1)},\cdots,\v^{(k-1)},\w^{(k)},\v^{(k+1)},\cdots,\v^{(d)}).\nonumber
\end{eqnarray} Ainsi,
\begin{eqnarray}
&&t(\v^{(1)},\cdots,\v^{(k-1)},a\v^{(k)}+b\w^{(k)},\v^{(k+1)},\cdots,\v^{(d)})\nonumber\\
&=&at(\v^{(1)},\cdots,\v^{(k-1)},\v^{(k)},\v^{(k+1)},\cdots,\v^{(d)})+ bt(\v^{(1)},\cdots,\v^{(k-1)},\w^{(k)},\v^{(k+1)},\cdots,\v^{(d)}).\nonumber
\end{eqnarray} Par suite, $ t $ est $ d $-linéaire.
\end{proof}
\begin{theo}
\emph{\\}
Pour tout espace vectoriel M sur $ \K $ et toute application multilinéaire $ \phi:V^{(1)}\times\cdots\times V^{(d)}\longrightarrow M $, il existe une unique application linéaire $ \psi:\V\longrightarrow M $ telle que $ \phi=\psi\circ t$ \cite[p. 125]{Michel2013}.
\end{theo}
Ceci se traduit en disant que le diagramme suivant est commutatif
	\begin{center}
	\begin{tikzpicture}
	\node (A) at (0,2) {$V^{(1)}\times\cdots\times V^{(d)} $};
	\node (B) at (3,2) {$ \V$};
	\node (C) at (3,0) {$ M $};
	\draw[->,very thick](A)--(B) node[midway,above,sloped,color=black]{$ t $};
	\draw[->,very thick](A)--(C) node[midway,left,color=black]{$ \phi $};
	\draw[->,very thick](B)--(C) node[midway,right,color=black]{$\psi$};
	\end{tikzpicture}
\end{center}

\begin{proof}
\emph{\\}
Considérons l'application linéaire $ g: U\longrightarrow M $ définie par $ g(\u_{1},\cdots,\u_{d})=\phi(\u_{1},\cdots,\u_{d}) $. Cette application linéaire est bien définie car on connait sa valeur sur tous les vecteurs de base de U. Comme $ \phi $ est multilinéaire, on a que $ N\subset \ker(g) $. Ainsi $ g $ induit une application linéaire $ h $ sur l'espace quotient $ h:U/N\longrightarrow M $ telle que $ h(\u_{1}\otimes \cdots\otimes \u_{d})=g(\u_{1},\cdots,\u_{d}).$
\end{proof}
\subsubsection{Propriété universelle du produit tensoriel}
\begin{theo}
\emph{\\}
Soit U un $ \K $-espace vectoriel et $ t':V^{(1)}\times\cdots\times V^{(d)}\longrightarrow U $ une application $ d $-linéaire surjective. Supposons que pour toute application $ d $-linéaire $ \phi:V^{(1)}\times\cdots\times V^{(d)}\longrightarrow M $ on ait la factorisation 
	\begin{center}
	\begin{tikzpicture}
	\node (A) at (0,2) {$V^{(1)}\times\cdots\times V^{(d)} $};
	\node (B) at (3,2) {$ U$};
	\node (C) at (3,0) {$ M $};
	\draw[->,very thick](A)--(B) node[midway,above,sloped,color=black]{$ t' $};
	\draw[->,very thick](A)--(C) node[midway,left,color=black]{$ \phi $};
	\draw[->,very thick](B)--(C) node[midway,right,color=black]{$f'$};
	\end{tikzpicture}
\end{center} avec $ f' $ linéaire. Alors U est isomorphe à l'espace tensoriel $\V$ \cite[p.126]{Michel2013}.
\end{theo}
\begin{proof}
\emph{\\}
Si $ M=\V$ et si $ \phi $ est l'application $ d $-linéaire $ t $, alors on a le diagramme commutatif suivant:
	\begin{center}
	\begin{tikzpicture}
	\node (A) at (0,2) {$V^{(1)}\times\cdots\times V^{(d)}$};
	\node (B) at (3,2) {$ U$};
	\node (C) at (3,0) {$ \V $};
	\draw[->,very thick](A)--(B) node[midway,above,sloped,color=black]{$ t' $};
	\draw[->,very thick](A)--(C) node[midway,left,color=black]{$ t $};
	\draw[->,very thick](B)--(C) node[midway,right,color=black]{$f'$};
	\end{tikzpicture}
\end{center}
et donc $ f'(t'(\v{(1)},\cdots,\v^{(k)}))=\ktensor\v^{(k)} $. De même, en prenant pour M l'espace vectoriel U, on récupère le diagramme commutatif suivant:
	\begin{center}
	\begin{tikzpicture}
	\node (A) at (0,2) {$V^{(1)}\times\cdots\times V^{(d)} $};
	\node (B) at (3,2) {$\V $};
	\node (C) at (3,0) {$ U $};
	\draw[->,very thick](A)--(B) node[midway,above,sloped,color=black]{$ t $};
	\draw[->,very thick](A)--(C) node[midway,left,color=black]{$ t' $};
	\draw[->,very thick](B)--(C) node[midway,right,color=black]{$f''$};
	\end{tikzpicture}
\end{center}
et $ f''\left(\ktensor \v^{(k)}\right)=t'(\v^{(1)},\cdots,\v^{(d)}) $. D'où $ f'\left(f''\left(\ktensor \v^{(k)}\right)\right)=\ktensor \v^{(k)} $ et\\ $ f''(f'(t'(\v^{(1)},\cdots,\v^{(d)})))=\ktensor \v^{(k)}.$\\ Ainsi, $ f'\left(f''\left(\ktensor \v^{(k)}\right)\right)=\ktensor \v^{(k)}=t'(\v^{(1)},\cdots,\v^{(d)}) $. Comme $ t' $ est surjective, on en déduit que $ f''\circ f'=Id_{U}.$
\end{proof}
Le résultat de cette proposition peut aussi être utilisé comme une définition équivalente de l'espace tensoriel $\V$.

Dans ce paragraphe, il s'agit d'étudier le produit tensoriel d'espaces vectoriels sur $ \K $ de dimension finie.
\begin{theo}
	\emph{\\}
Si chacun des espaces vectoriels $ V^{(k)} $ est de dimension finie, alors $\gktensor V^{(k)} $ est aussi de dimension finie et l'on a:
\begin{equation*}
\dim \left(\gktensor V^{(k)}\right)=\dim(V^{(1)})\cdots \dim(V^{(d)})  \text{ \cite[p.53]{hackbusch2012tensor}}.
\end{equation*}

\end{theo} 
\begin{proof}
	\emph{\\}
Si l'un des espaces $ V^{(k)} $ est réduit à $ \{0\} $, alors, le produit tensoriel $ \gktensor V^{(k)} $ est nul. Le théorème est dans ce cas démontré. Supposons donc que chacun des espaces $ V^{(k)} $ soit non nul. D'après l'identification $ \L\left(V^{(1)},\cdots,V^{(d)};M\right)=\L\left(\gktensor V^{(k)};M\right)  $ qui est vraie pour tout espace vectoriel M, on déduit en particulier $ \L\left(V^{(1)},\cdots,V^{(d)};\K\right)=\L\left(\gktensor V^{(k)};\K\right)  $. 

Soit $ \L\left(V^{(1)},\cdots,V^{(d)};\K\right)=\left(\gktensor V^{(k)}\right)^{*}.$ De même, on sait, d'après la relation \eqref{dimmulti} de la  \proppref{propdim},  que $ \dim\left(\L(V^{(1)},\cdots,V^{(d)};\K)\right)=\dim\left(V^{(1)}\right)\cdots \dim \left(V^{(d)}\right)\cdot \dim(\K). $

 Comme $ \dim\left(\gktensor V^{(k)}\right)^{*}=\dim \left(\gktensor V^{k}\right), $ on a finalement 
 \begin{equation*}
 \dim\left(\gktensor V^{k}\right)=\dim\left(V^{(1)}\right)\cdots \dim \left(V^{(d)}\right).
 \end{equation*}

 \end{proof}
\begin{prop}
\emph{\\}
Soit $ \{\v_{1}^{(k)},\v_{2}^{(k)},\v_{n^{(k)}}^{(k)}\} $ une base de $ V^{(k)} $ pour $ k=1,\cdots,d $. Alors la famille $$\left\{\v_{j_{1}}^{(1)}\otimes \v_{j_{2}}^{(2)}\otimes\cdots\otimes \v_{j_{d}}^{(d)},\: j_{k}=1,\cdots,n^{(k)}\right\} $$ est une base de $ \V $ \cite[p.128]{Michel2013}.
\end{prop}
\begin{proof}
\emph{\\}
D'après la multilinéarité de l'application $ t $ définie dans la  \propref{prop:1.9}, la famille $ \left\{\v_{j_{1}}^{(1)}\otimes \v_{j_{2}}^{(2)}\otimes\cdots\otimes \v_{j_{d}}^{(d)}\right\} $ engendre $ \V $. Comme la cardinalité de cette  famille est égale à la dimension du produit tensoriel, le résultat s'en déduit.
\end{proof}

\begin{prop}
	\emph{\\}
		Le produit tensoriel est associatif 
		\begin{equation}
		U\otimes_{a}(V\otimes_{a} W)=(U\otimes_{a} V)\otimes_{a} W \text{   \cite[p.56]{hackbusch2012tensor}}.
		\end{equation}
\end{prop}
\begin{proof}
	\emph{\\}
	Soit $ \u_{i}(i\in I) $, $ \v_{j}(j\in J) $, $ \w_{k}(k\in K) $ des bases de U, V et W avec I, J et K des ensembles finis. Comme vu dans la \propref{basis}, $ V\otimes_{a} W $ a la base $ \v_{j}\otimes \w_{k}\quad((j,k)\in J\times K) $, tandis que $ U\otimes_{a} (V \otimes_{a} W) $ a la base $ \u_{i}\otimes( \v_{j}\otimes \w_{k}) $ avec $ (i,(j,k))\in I\times (J\times K) $.
	
	De même, $ (U\otimes_{a} V)\otimes_{a} W $ a la base $ (\u_{i}\otimes \v_{j})\otimes \w_{k} $ avec $ ((i,j),k)\in (I\times J)\times K $.
	Par la bijection évidente entre $ I\times (J\times K)  $ et $ (I\times J)\times K  $, on a l'isomorphisme \\
	$ U\otimes_{a} (V \otimes_{a} W)\cong  (U\otimes_{a} V) \otimes_{a} W$.
\end{proof}
 Avec le résultat ci-haut présenté, on a la remarque suivante:
 \begin{rem}
 \emph{\\}
 Dans la pratique, on confond $ (\u \otimes \v)\otimes \w $ et $ \u\otimes  (\v\otimes \w) $
 qu’on écrit $ \u \otimes \v\otimes \w $, et on désigne par $ (U \otimes_{a} V)\otimes_{a} W $ l’espace engendré par
 $ \u \otimes \v\otimes \w $, où $ \u\in U $, $ \v\in V $ et $ \w\in W .$\\
 Et, on définit le produit tensoriel $ \agktensor V^{(k)}  $
 des espaces vectoriels $ V^{(1)},\cdots,V^{(d)} $ sur $ \K $ par:
 $$\agktensor V^{(k)}=V^{(1)}\otimes_{a}(V^{(2)}\otimes_{a} \cdots\otimes_{a} V^{(d)}), $$
 et on confond aussi $ \ktensor \v^{(k)} $ avec $ \v^{(1)}\otimes (\v^{(2)}\otimes\cdots\otimes \v^{(k)}) $ {\rmfamily \cite[p.16]{Awanegeo}}.
 \end{rem}
%\subsubsection{Produit tensoriel d'applications multilinéaires}
%Le point \ref{sectensorapp} a permis de définir le produit tensoriel de deux applications linéaires. Ici, il s'agit simplement de généraliser en considérant le produit tensoriel d'un nombre quelconque d'applications linéaires ainsi que le produit tensoriel d'applications bilinéaires ou multilinéaires.
%
%En effet, considérons les applications linéaires $ f_{i}: E_{i}\rightarrow F_{i} $ pour $ i=1,\dots,n $ où $ E_{i} $ et $ F_{i} $ sont des $ \K $-espaces vectoriels. L'application $ \psi:E_{1}\times\cdots\times E_{n}\rightarrow F_{1}\otimes\cdots\otimes F_{n} $ donnée par $ \psi(v_{1},\dots,v_{n})=\varphi_{1}(v_{1})\otimes\cdots\otimes \varphi_{n}(v_{n}) $ est $ n $-linéaire. Par factorisation, on en déduit qu'il existe une application linéaire notée $ f_{1}\otimes\cdots\otimes f_{n} $ et définie par $$ f_{1}\otimes\cdots\otimes f_{n}(v_{1}\otimes\cdots\otimes v_{n})=f_{1}(v_{1})\cdots f_{n}(v_{n}) $$
%Cette application linéaire $ f_{1}\otimes\cdots\otimes f_{n} $ est dans l'espace $ \L(E_{1}\otimes\cdots\otimes E_{n};F_{1}\otimes\cdots\otimes F_{n}) $ et non, comme la notation pourrait le suggérer, dans le produit tensoriel $ \L(E_{1},F_{1})\otimes\cdots\otimes \L(E_{n},F_{n})$. Mais comme pour le cas $ n=2 $, il existe une application linéaire injective:
%$$i:\L(E_{1},F_{1})\otimes\cdots\otimes \L(E_{n},F_{n})\rightarrow\L(E_{1}\otimes\cdots\otimes E_{n};F_{1}\otimes\cdots\otimes F_{n}). $$ Mais, si tous les espaces $ E_{i} $ et $ F_{i} $ sont de dimension finie, alors les deux espaces $ \L(E_{1},F_{1})\otimes\cdots\otimes \L(E_{n},F_{n}) $ et $\L(E_{1}\otimes\cdots\otimes E_{n};F_{1}\otimes\cdots\otimes F_{n})  $ sont des dimensions finies (cfr \remref{remtensorapp}), l'injection i est un isomorphisme. Dans ce cas, la confusion disparaît.
%
%En considérant deux applications bilinéaires $ \varphi_{1}:E_{1}\times E_{2}\rightarrow G_{1} $ et $ \varphi_{2}:F_{1}\times F_{2}\rightarrow G_{2} $ où $ E_{1},E_{2},F_{1},F_{2}, G_{1} $ et $ G_{2} $ sont des $ \K $-espaces vectoriels, on a le résultat suivant:
%\begin{defi}
%	\emph{\\}
%Il existe une application bilinéaire notée et définie par$$\varphi_{1}\otimes \varphi_{2}:(E_{1}\otimes F_{1})\times(E_{2}\otimes F_{2})\rightarrow (G_{1}\otimes G_{2})  $$ vérifiant $ (\varphi_{1}\otimes \varphi_{2})(v_{1}\otimes w_{1},v_{2}\otimes w_{2})=\varphi_{1}(v_{1},v_{2})\otimes \varphi_{2}(w_{1},w_{2}) $ pour tous $ v_{i}\in E_{i} $, $ w_{i}\in F_{i} $, $ i=1,2 $. Cette application est appelée le produit tensoriel de $ \varphi_{1} $ par $ \varphi_{2} $.
%\end{defi}

%\subsection{Quantiques}
%Le terme \textquotedblleft quantique \textquotedblright\, introduit par  ARTHUR Cayley en 1854 est utilisé pour les polynômes homogènes à variables multiples, c'est-à-dire, les polynômes dont les variables $ x_{i}(i\in B) $ sont une somme finie de termes $ a_{\nu}\mathbf{x}^{\nu}=a_{\nu}\prod_{i\in B}x_{i}^{\nu_{i}} $ avec des multi-indices $ \nu $ de longueur $ |\nu|=\sum_{i\in B}\nu^{i}=d\in\N $ et $ a_{\nu}\in\K.$ Ces quantités ont la propriété 
%\begin{equation}
%p(\lambda \mathbf{x})=\lambda^{d}p(\mathbf{x}).
%\end{equation}
%\begin{prop}\label{prop3.69}
%\emph{\\}
%Soit V un espace vectoriel avec la base (algébrique) $ \{b_{i}; i\in B\} $. Alors l'espace tensoriel algébrique symétrique $ \G_{d}(V) $ est isomorphe à l'ensemble des quantiques à variables $ \{x_{i} : i\in B\} $. \cite[p. 83]{hackbusch2012tensor}
%\end{prop}
%Cette déclaration vaut également pour $ \dim(V)=\# B=\infty. $ Le produit infini $ \mathbf{x}^{\nu}= \prod_{i\in B}x_{i}^{\nu_{i}}$ a du sens, car au plus $ d $ exposants $ \nu_{i} $ sont différents de zéro.
%\begin{proof}
%\emph{\\}
% Pour la preuve de la proposition \ref{prop3.69}, considérons un tenseur général $ \vey\in \tensor V. $ En  utilisant la base $ \{b_{i}:i\in B\} $, nous pouvons écrire $ \vey=\sum_{i\in B^{d}}\mathbf{a}_{i}\tensor b_{ij} $ (presque tous les $ \mathbf{a}_{i} $ disparaissent). Par conséquent, tout tenseur symétrique peut s'écrire comme $ P_{\G}\vey=\sum_{i\in B^{d}} P_{\G} \left(\mathbf{a}_{i}\tensor b_{ij}\right).$ L'isomorphisme dans les quantiques de degré d est donné par
% \begin{equation}
% \Phi :P_{\G}\left(\mathbf{a}_{i}\tensor b_{ij}\right)\longrightarrow \mathbf{a}_{i}\prod_{j=1}^{d} x_{ij}.
% \end{equation}
%Notons que 
%$$\prod_{j=1}^{d}x_{ij}=\prod_{i\in B} x_{i}^{\nu_{i}} \text{ avec } \nu_{i}=\#\{j\in\{1,\cdots,d\} :i_{j}=i\}\quad \forall i\in B. $$
%La symétrie de $ P_{\G}\vey $ correspond au fait que tous les facteurs $ x_{i} $ commutent.
%\end{proof}
%
%Ci-dessus, nous avons utilisé l'image de $ P_{\G} $ pour définir $ \G_{d}(V) $. Dans le cas de $ d=2, $ par exemple : $ \vey=P_{\G}(a\otimes b)=a\otimes b+b\otimes a $ représente un tenseur symétrique. Au lieu de cela, on peut utiliser $ \vey=\frac{1}{2}\otimes^{2}(a+b)- \frac{1}{2}\otimes^{2}(a-b).$
%
%Dans la  dernière représentation, chaque terme lui-même est symétrique. En général, $ \G_{d}(V) $ peut être généré par les produits tensoriels d-fois de $ \otimes_{v}^{d} $ :
%$$\G(V) =span\left\{\otimes^{d}v :v\in V\right\}.$$
%Dans le langage des quantiques, cela signifie que le polynôme s'écrit comme somme de d\up{e} puissances $ \left(\sum_{i=1}^{n}a_{i}x_{i}\right)^{d} $ des formes linéaires. La décomposition d'un polynôme homogène dans cette forme spéciale est adressée par Brachat-Comon-Mourrain-Tsigaridas.
%\section{Espaces tensoriels symétriques et  antisymétriques}
%\subsection{Définitions}
%Dans tout ce qui suit, tous les espaces vectoriels $ V_{j} $ coïncident et sont notés par V: $ V=V_{1}=V_{2}=\cdots=V_{d}. $
%Le produit tensoriel d-fois est noté par $ \V=\bigotimes^{d}V $, où $ d\geq 2 $ est requis. Nous nous référons ici à l'espace tensoriel algébrique, c'est-à-dire $ \V=\,\atensor V $.
%
%A titre de rappel, une bijection $ \pi : D\longrightarrow D $ de l'ensemble $ D=\{1,\cdots,d\} $ est appelée \textit{permutation}. Soit $ P=\{\pi:D\longrightarrow D \text{ bijectif }\} $ l'ensemble de toutes les permutations. Son cardinal $ \# P =d! $ augmente rapidement avec l'augmentation de d.
%$ (P,\circ) $ est un groupe, où $ \circ $ est défini par la composition : $\tau \circ \pi(j)=\tau[\pi(j)].  $ L'inverse de $ \pi $ est noté par $ \pi^{-1} $.
%
%Une permutation $ \pi\in P $ donne lieu à une application $ \V\longrightarrow\V $ notée à nouveau par $ \pi $ :
%\begin{equation}
%\pi: \tensor v^{(j)}\longrightarrow\tensor v^{(\pi^{-1}(j))}. 
%\end{equation}
%\begin{defi}
%	\emph{\\}
%	\begin{itemize}
%		\item[(a)]
%		$ \vey\in\V=\otimes^{d}V $ est symétrique, si $ \pi(\vey)=\vey $, $ \forall \pi\in P. $
%		\item[(b)]
%		L'espace tensoriel symétrique est défini par 
%		\begin{equation}
%		\G=\G(V)=\G_{d}(V)=\bigotimes_{sym}^{d}V=\{\vey\in \V :\vey \text{ est symétrique}\}.
%		\end{equation}
%		\item[(c)]
%		Un tenseur $ \vey\in \V=\otimes^{d}V $ est antisymétrique si $ \pi(\vey)=signe(\pi)\vey $, $ \forall \pi\in P .$
%		\item[(d)]
%		L'espace tensoriel antisymétrique est défini par 
%		\begin{equation}
%		\U=\U(V)=\U_{d}(V)=\bigotimes_{anti}^{d}V=\{\vey\in\V :\vey \text{ est antisymétrique}\}.
%		\end{equation}
%	\end{itemize}
%\end{defi}
%Une définition équivalente d'un tenseur symétrique (antisymétrique) $ \vey $ est $ \vey=\pi(\vey) \left(\vey=-\pi(\vey)\right)$ pour tous les échanges des couples $$ \pi:(1,\cdots,i,\cdots,j,\cdots,d)\longmapsto (1,\cdots,j,\cdots,i,\cdots,d),$$ puisque toutes les permutations de P sont des produits de permutations par paire. En d'autres termes, un tenseur est dit \textit{symétrique} si ses éléments restent constants sous toute permutation des indices \cite[p.459]{Hong2008}.
%
%Pour $ d=2 $ et $ V=\K^{n} $, les tenseurs de $ \G $ et $ \U $ correspondent aux matrices symétriques $ (M_{ij}=M_{ji}) $ et matrices antisymétriques $ (M_{ij}=-M_{ji}) $, respectivement. \cite[pp: 80-81]{hackbusch2012tensor}
%
%\begin{rem}\label{prop3.36}
%	\emph{\\}
%	$ \G $ et $\U  $ sont des sous-espaces de $ \V=\bigotimes^{d}V.$
%\end{rem}
%
%
%\subsection{L'algèbre tensorielle $ \mathcal{A}(V) $}
%%Soit $ V $ un $ \K $-espace vectoriel algébrique. Posons pour simplifier l'écriture dès que $ d>1 $, $ V^{\otimes^{d}}=V\otimes \cdots\otimes V $ le produit de $ d $ exemplaires de $ V $.
% Par convention, on pose $ V^{\otimes^{0}}=\K$ et $ V^{\otimes^{1}}=V$. On peut considérer la somme directe externe notée $ \A(V) $ de tous ces espaces vectoriels $$\A(V)=\sum_{d\in\N} V^{\otimes^{d}}.$$
%Une structure d'algèbre est donnée par un couple $ (V,\mu) $ où V est un espace vectoriel et $ \mu  $ une application bilinéaire $ \mu:V\times \V\rightarrow V $ appelée la multiplication de V. En écrivant tout simplement $ \mu(v,w)=v\cdot w $, alors la bilinéarité de $ \mu $ se traduit par:
%$$\begin{cases}
%v\cdot (w_{1}+w_{2})=v\cdot w_{1}+v\cdot w_{2},\\
% (v_{1}+v_{2})\cdot w=v_{1}\cdot w+v_{2}\cdot w,\\
% (av)\cdot w=v\cdot(aw)=a(v\cdot w),
%\end{cases}\forall v,v_{1},v_{2},w,w_{1},w_{2}\in V\text{ et }a\in \K. $$ Ainsi, $ \mu(v,w)=v\cdot w $ définie une loi de composition interne; une multiplication, qui est distributive à gauche et à droite par rapport à l'addition.
%
%\begin{defi}[Algèbre tensorielle]
%	\emph{\\}
%Il existe sur l'espace vectoriel $ \A(V) $ une unique structure d'algèbre définie par l'application bilinéaire $ \mu $ $$\mu_{\A(V)}:\A(V)\times \A(V)\rightarrow \A(V) $$ telle que $ \mu_{\A(V)}(v_{1}\otimes\cdots\otimes v_{d},v'_{1}\otimes\cdots\otimes v'_{l})=v_{1}\otimes\cdots\otimes v_{d}\otimes v'_{1}\otimes\cdots\otimes v'_{l} $ pour tous $ v_{1}\otimes\cdots\otimes v_{d}\in V^{\otimes^{d}} $ et $ v'_{1}\otimes\cdots\otimes v'_{l}\in V^{\otimes^{l}} $ et tout couple $ (d,l) $. Munie de cette structure, $ \A(V) $ est appelé l'algèbre tensorielle de $ V $. \cite[p.138]{Michel2013} 
%\end{defi}
%\newpage
\section*{Conclusion partielle}

Dans ce chapitre, nous avons présenté d'une part un contenu minimal jugé nécessaire sur les espaces vectoriels non seulement permettant d'asseoir la théorie des espaces tensoriels par la suite, mais aussi pouvant intervenir dans le calcul tensoriel.
D'autre part, il a été question de présenter les espaces tensoriels grâce à la notion de produit tensoriel défini au minimum entre deux espaces vectoriels. Par exemple, partant de deux espaces vectoriels V et W tels que $ (\v,\w)\in V\times W $, l'espace tensoriel  résultant noté $ V\at W $ est engendré par les éléments de la forme $ \v\otimes \w $ (\textit{tenseurs}). Le produit tensoriel $ \otimes  $ est bilinéaire ou multilinéaire selon qu'il est défini à partir de deux ou plusieurs espaces vectoriels. En outre, l'indépendance linéaire des vecteurs de V et de ceux de W implique directement celle des éléments de $ V\at W $. Si de plus  V et W sont des dimensions finies, il en est de même de $ V\otimes W $  et sa dimension est le produit des dimensions de V et W. Par ailleurs, le produit tensoriel de plusieurs espaces vectoriels est associatif, ce qui permet de transposer aisément certains résultats en dimension supérieure.

Cependant, aucune technique de calcul de $ \v\otimes \w$ n'a été présentée; ce chapitre se limitait exclusivement à la construction et à la présentation des propriétés de tels éléments. C'est ainsi que le chapitre \ref{chap2} va non seulement montrer comment représenter les tenseurs mais aussi comment arriver à réaliser certaines opérations dans un espace tensoriel.


\chapter{Représentations et traitements des tenseurs numériques}\label{chap2}

\section*{Introduction}
 Dans ce chapitre, il sera question de présenter d'une part la représentation des tenseurs numériques, et d'autre part, les techniques liées au calcul tensoriel. Dans sa suite, la section \ref{sec:2.1visual} va présenter une représentation appropriée des tenseurs numériques. La section \ref{sec:2.2traitements} va exposer quelques traitements numériques applicables aux tenseurs. La section \ref{sec:2.3rang} présentera le rang tensoriel qui malheureusement n'est pas défini de façon unique contrairement au cas des matrices. Pour finir, la section \ref{sec:2.4formats} parlera des décompositions tensorielles qui sont en effet des généralisations des décompositions matricielles; on va en présenter cinq mais, une attention particulière sera portée  sur quatre d'entre elles qui sont considérées comme les plus couramment utilisées en calcul tensoriel, il s'agit de la décomposition polyadique canonique, la décomposition de Tucker, la décomposition hiérarchique de Tucker et la décomposition en train de tenseurs.
\section{Représentation visuelle des tenseurs}\label{sec:2.1visual}
A partir de cette section, l'espace tensoriel algébrique  $ \V=\agktensor V^{(k)}$ sera noté tout simplement par $ \V $. Par ailleurs, le mot \textit{tenseur} sera considéré au sens de la définition suivante:
\begin{defi}[Tenseur numérique]\label{tensornum}
	\emph{\\}
	Un tenseur $ \vey\in\V$ est considéré comme un tableau multidimensionnel appelé aussi hypermatrice dont les entrées numériques sont 
	\begin{equation*}
	v_{j_{1}j_{2}\cdots j_{d}}=\vey\left(j_{1},j_{2},\cdots,j_{d}\right),
	\end{equation*}
	où chaque indice $ j_{k} $ varie jusqu'à un nombre $ n^{(k)} $ qui est la dimension de son espace vectoriel associé $ V^{(k)}$ \cite[p.124]{Nyenyezi2018}.
\end{defi} 
Cette notion de tenseurs ne doit pas être confondue avec les tenseurs utilisés en physique et en ingénierie (tels que les tenseurs de contrainte), qui sont généralement appelés champs tensoriels en mathématiques \cite[p.455]{Hong2008}.\\

Alors que les vecteurs ont des entrées $ v_{i} $ avec un indice et que les matrices ont des entrées $ a_{ij} $ avec deux indices, les tenseurs portent $ d $ indices. Le nombre naturel $ d $ définit l'ordre du tenseur et est choisi en raison  de son interprétation comme dimension spatiale.
%\cite[p. 3]{hackbusch2012tensor}.
Les indices $$ j\in\{1,\cdots,d\} $$ correspondent à la \textquotedblleft j-ième direction\textquotedblright, la \textquotedblleft j-ième position\textquotedblright, la \textquotedblleft j-ième dimension\textquotedblright, le \textquotedblleft j-ième axe\textquotedblright, le \textquotedblleft j-ième site\textquotedblright ou le \textquotedblleft j-ième mode\textquotedblright. Les noms \textquotedblleft direction\textquotedblright\,et \textquotedblleft dimension\textquotedblright\, proviennent des fonctions $ f(x_{1},\cdots,x_{d}) $, où la variable $ x_{j} $ correspond à la j-ième direction spatiale \cite[p.3]{hackbusch2012tensor}. De ce fait, un scalaire est considéré comme tenseur d'ordre $ d=0 $, un vecteur comme un tenseur d’ordre $ d = 1 $, une matrice
comme un tenseur d’ordre $ d = 2 $ et on parle simplement de tenseur pour un tableau multidimensionnel d’ordre $ d > 2 $.\\
Les tenseurs numériques ont une  \textit{représentation visuelle appropriée}. Et généralement, cette dernière est réalisée de deux manières en utilisant:
\begin{enumerate}
\item Un point avec des lignes qui représentent  les différentes directions du tenseur, ou
\item Des \textit{boîtes} et des \textit{tranches}.
\end{enumerate}
La \figref{fig:trivialtensors} illustre une représentation des tenseurs numériques d'ordre inférieur.
\begin{figure}[!h]
	\encad{ \centering\begin{subfigure}[]{0.3\textwidth}
			\centering
			\begin{tikzpicture}[scale=1.2]
			\node at (-2,0)[scale= 1.2] {$\cdb{\bullet}\equiv$};
			\cuboid{-1.5}{0}{0}{0.2}{0.2}{0.2}{fill=violet!80};
			\end{tikzpicture}
			\caption{Scalaire a ($ d=0 $)}
			\label{fig:tens_dot(a)}
		\end{subfigure}\qquad
		\begin{subfigure}[]{0.3\textwidth}
			\centering
			\begin{tikzpicture}[scale=1.2]
			\draw (0,-1) -- (0,1)  node [midway] {$/$} node [scale=1.2] (0,1){\cdb{$\bullet$}};
			\node at (0.5,0) {$\equiv$};
			\cuboid{1}{-1}{0}{0.2}{1.5}{0.2}{fill=violet!80};
			\end{tikzpicture}
			\caption{Vecteur $ \mbf{v} $ ($ d=1 $)}
			\label{fig:tens_vec(b)}
		\end{subfigure}
		\begin{subfigure}[]{0.3\textwidth}
			\centering
			\begin{tikzpicture}[scale=1.2]
			\draw (3,-1) -- (3,1)  node [near start] {$/$} node[near end] {$/$} node [midway,scale=1.2,color=red!80]{\cdb{$\bullet$}};
			\node at (3.5,0) {$\equiv$};
			\cuboid{4}{-1}{0}{1.5}{1.5}{0}{fill=violet!70};
			\end{tikzpicture}
			\caption{Matrice $A \;(d=2)$}
			\label{tens_mat(c)}
		\end{subfigure}}
	\caption{Représentation visuelle des tenseurs pour $ d\in\{0,1,2\} $ \cite[p.133]{Nyenyezi2018}.}
	\label{fig:trivialtensors}
	\end{figure}

Les tenseurs d'ordre élevé ($ d>2 $) sont en effet des généralisations multidimensionnelles des matrices, la \figref{matrixblock}  en illustre un échantillon via une représentation visuelle issue des blocs matriciels. Du coup, les tenseurs d'ordre $ d>3 $  peuvent être représentés comme un tas de volumes (\figref{fig:2.1}).
\begin{figure}[!h]
	\encad{\centering\begin{subfigure}[]{0.5\textwidth}
			\centering
			\includegraphics[width=0.6\linewidth]{"My figures/FIG22"}
			\caption{Tenseur d'ordre 3}
			\label{matrixblock1}
		\end{subfigure}
		\begin{subfigure}[]{0.4\textwidth}
			\centering
			\includegraphics[width=0.6\linewidth]{"My figures/FIG23"}
			\caption{Tenseur d'ordre 4}
			\label{matrixblock2}
	\end{subfigure}}
	\caption{Illustration des tenseurs d'ordre $d\in\{3,4\}  $ via des blocs matriciels \cite[p.2]{cichocki2014era}.}
	\label{matrixblock}
\end{figure}

 La philosophie de ce modèle (\figref{matrixblock}) sert de base à la représentation visuelle (tant que possible) d'un tenseur d'ordre élevé en général. A présent, revenons à la représentation  des tenseurs d'ordre supérieurs basée sur le modèle approprié. Un tenseur d'ordre 3 est visualisé dans \ref{fig:2.1}(\subref{tens_d3(d)}) comme un empilement de matrices, un tenseur d'ordre 4 est visualisé dans \ref{fig:2.1}(\subref{tens_d4(f)}) comme une ligne des tenseurs d'ordre 3, un tenseur d'ordre 5 est visualisé d'une part, dans \ref{fig:2.1}(\subref{tens_d5(g)}) comme une pile des tenseurs d'ordre 4, et d'autre part dans \ref{fig:2.1}(\subref{tens_d5(i)}) comme un bloc des tenseurs d'ordre 3 qui à leur tour sont des superpositions des matrices. En dernier lieu, un tenseur d'ordre 6 est présenté dans \ref{fig:2.1}(\subref{tens_d6(k)}) comme une pile des tenseurs d'ordre 5.\\


\begin{figure}[!h]
	\encad{
		\begin{subfigure}[]{0.25\textwidth}
			\centering
			\begin{tikzpicture}[scale=0.4]
			\draw (-6,0) -- (-2,0)  node [near start] {$/$} node[near end] {$/$} node [midway,scale=1.2]{\cdb{$\bullet$}};
			\draw (-4,-2) -- (-4,0)   node [near start] {$/$};
			\node at (-1.5,0) {$\equiv$};
			\cuboid{0}{0}{0}{3}{3}{0}{fill=violet!80};
			\cuboid{0}{0}{0.5}{3}{3}{0}{fill=violet!80};
			\cuboid{0}{0}{1.0}{3}{3}{0}{fill=violet!80};
			\cuboid{0}{0}{1.5}{3}{3}{0}{fill=violet!80};
			\end{tikzpicture}
			\caption{Tenseur $\mathbf{A}:\: d=3$}
			\label{tens_d3(d)}
		\end{subfigure}
		\begin{subfigure}[]{0.30\textwidth}
			\centering
			\begin{tikzpicture}[scale=0.95]
			\draw (-1,0) -- (1,0)  node [near start] {$/$} node[near end] {$/$} node [midway,scale=1.5]{\cdb{$\bullet$}};
			\draw (0,-1) -- (0,1)   node [near start] {$/$} node[near end] {$/$} ;
			\end{tikzpicture}
			\caption{Tenseur $\mathbf{A}:d=4$}
			\label{tens_d4(e)}
		\end{subfigure}
		\begin{subfigure}[]{0.35\textwidth}
			\centering
			\begin{tikzpicture}[scale=0.4]
			\cuboid{0.0}{0.0}{0.0}{2}{2}{2}{fill=violet!80};
			\cuboid{2.5}{0.0}{0.0}{2}{2}{2}{fill=violet!80};
			\cuboid{5.0}{0.0}{0.0}{2}{2}{2}{fill=violet!80};
			\cuboid{7.5}{0.0}{0.0}{2}{2}{2}{fill=violet!80};
			\end{tikzpicture}
			\caption{Tenseur $\mathbf{A}:\: d=4$}
			\label{tens_d4(f)}
		\end{subfigure}
		
		\vspace{1cm}
		
		\begin{subfigure}[]{0.3\textwidth}
			\centering
			\begin{tikzpicture}[scale=0.25]
			\cuboid{0.0}{0.0}{0.0}{2}{2}{2}{fill=violet!80};
			\cuboid{2.5}{0.0}{0.0}{2}{2}{2}{fill=violet!80};
			\cuboid{5.0}{0.0}{0.0}{2}{2}{2}{fill=violet!80};
			\cuboid{7.5}{0.0}{0.0}{2}{2}{2}{fill=violet!80};
			\cuboid{0.0}{2.5}{0.0}{2}{2}{2}{fill=violet!80};
			\cuboid{2.5}{2.5}{0.0}{2}{2}{2}{fill=violet!80};
			\cuboid{5.0}{2.5}{0.0}{2}{2}{2}{fill=violet!80};
			\cuboid{7.5}{2.5}{0.0}{2}{2}{2}{fill=violet!80};
			\cuboid{0.0}{5.0}{0.0}{2}{2}{2}{fill=violet!80};
			\cuboid{2.5}{5.0}{0.0}{2}{2}{2}{fill=violet!80};
			\cuboid{5.0}{5.0}{0.0}{2}{2}{2}{fill=violet!80};
			\cuboid{7.5}{5.0}{0.0}{2}{2}{2}{fill=violet!80};
			\cuboid{0.0}{7.5}{0.0}{2}{2}{2}{fill=violet!80};
			\cuboid{2.5}{7.5}{0.0}{2}{2}{2}{fill=violet!80};
			\cuboid{5.0}{7.5}{0.0}{2}{2}{2}{fill=violet!80};
			\cuboid{7.5}{7.5}{0.0}{2}{2}{2}{fill=violet!80};
			\end{tikzpicture}
			\caption{Tenseur \;$\mathbf{A}\; (d=5).$}
			\label{tens_d5(g)}
		\end{subfigure}
		\begin{subfigure}[]{0.25\textwidth}
			\centering
			\begin{tikzpicture}[scale=0.75]
			\draw (-1,0) -- (1,0)  node [near start] {$/$} node[near end] {$/$} node [midway,scale=1.5]{\cdb{$\bullet$}};
			\draw (0,-1) -- (0,1)   node [near start] {$/$} node[near end] {$/$} ;
			\draw (0,0) -- (1,1)  node [midway,scale=1] {$|$};
			\end{tikzpicture}
			\caption{Tenseur $\mathbf{A}:\: d=5$}
			\label{tens_d5(h)}
		\end{subfigure}
		\begin{subfigure}[]{0.35\textwidth}
			\centering
			\begin{tikzpicture}[scale=0.25]
			\cuboid{0.0}{0.0}{0}{2}{2}{0}{fill=violet!80};
			\cuboid{2.5}{0.0}{0}{2}{2}{0}{fill=violet!80};
			\cuboid{5.0}{0.0}{0}{2}{2}{0}{fill=violet!80};
			\cuboid{7.5}{0.0}{0}{2}{2}{0}{fill=violet!80};
			\cuboid{0.0}{2.5}{0}{2}{2}{0}{fill=violet!80};
			\cuboid{2.5}{2.5}{0}{2}{2}{0}{fill=violet!80};
			\cuboid{5.0}{2.5}{0}{2}{2}{0}{fill=violet!80};
			\cuboid{7.5}{2.5}{0}{2}{2}{0}{fill=violet!80};
			\cuboid{0.0}{5.0}{0}{2}{2}{0}{fill=violet!80};
			\cuboid{2.5}{5.0}{0}{2}{2}{0}{fill=violet!80};
			\cuboid{5.0}{5.0}{0}{2}{2}{0}{fill=violet!80};
			\cuboid{7.5}{5.0}{0}{2}{2}{0}{fill=violet!80};
			\cuboid{0.0}{7.5}{0}{2}{2}{0}{fill=violet!80};
			\cuboid{2.5}{7.5}{0}{2}{2}{0}{fill=violet!80};
			\cuboid{5.0}{7.5}{0}{2}{2}{0}{fill=violet!80};
			\cuboid{7.5}{7.5}{0}{2}{2}{0}{fill=violet!80};
			
			\cuboid{0.0}{0.0}{0.5}{2}{2}{0}{fill=violet!80};
			\cuboid{2.5}{0.0}{0.5}{2}{2}{0}{fill=violet!80};
			\cuboid{5.0}{0.0}{0.5}{2}{2}{0}{fill=violet!80};
			\cuboid{7.5}{0.0}{0.5}{2}{2}{0}{fill=violet!80};
			\cuboid{0.0}{2.5}{0.5}{2}{2}{0}{fill=violet!80};
			\cuboid{2.5}{2.5}{0.5}{2}{2}{0}{fill=violet!80};
			\cuboid{5.0}{2.5}{0.5}{2}{2}{0}{fill=violet!80};
			\cuboid{7.5}{2.5}{0.5}{2}{2}{0}{fill=violet!80};
			\cuboid{0.0}{5.0}{0.5}{2}{2}{0}{fill=violet!80};
			\cuboid{2.5}{5.0}{0.5}{2}{2}{0}{fill=violet!80};
			\cuboid{5.0}{5.0}{0.5}{2}{2}{0}{fill=violet!80};
			\cuboid{7.5}{5.0}{0.5}{2}{2}{0}{fill=violet!80};
			\cuboid{0.0}{7.5}{0.5}{2}{2}{0}{fill=violet!80};
			\cuboid{2.5}{7.5}{0.5}{2}{2}{0}{fill=violet!80};
			\cuboid{5.0}{7.5}{0.5}{2}{2}{0}{fill=violet!80};
			\cuboid{7.5}{7.5}{0.5}{2}{2}{0}{fill=violet!80};
			
			\cuboid{0.0}{0.0}{1.0}{2}{2}{0}{fill=violet!80};
			\cuboid{2.5}{0.0}{1.0}{2}{2}{0}{fill=violet!80};
			\cuboid{5.0}{0.0}{1.0}{2}{2}{0}{fill=violet!80};
			\cuboid{7.5}{0.0}{1.0}{2}{2}{0}{fill=violet!80};
			\cuboid{0.0}{2.5}{1.0}{2}{2}{0}{fill=violet!80};
			\cuboid{2.5}{2.5}{1.0}{2}{2}{0}{fill=violet!80};
			\cuboid{5.0}{2.5}{1.0}{2}{2}{0}{fill=violet!80};
			\cuboid{7.5}{2.5}{1.0}{2}{2}{0}{fill=violet!80};
			\cuboid{0.0}{5.0}{1.0}{2}{2}{0}{fill=violet!80};
			\cuboid{2.5}{5.0}{1.0}{2}{2}{0}{fill=violet!80};
			\cuboid{5.0}{5.0}{1.0}{2}{2}{0}{fill=violet!80};
			\cuboid{7.5}{5.0}{1.0}{2}{2}{0}{fill=violet!80};
			\cuboid{0.0}{7.5}{1.0}{2}{2}{0}{fill=violet!80};
			\cuboid{2.5}{7.5}{1.0}{2}{2}{0}{fill=violet!80};
			\cuboid{5.0}{7.5}{1.0}{2}{2}{0}{fill=violet!80};
			\cuboid{7.5}{7.5}{1.0}{2}{2}{0}{fill=violet!80};
			\cuboid{0.0}{0.0}{1.5}{2}{2}{0}{fill=violet!80};
			\cuboid{2.5}{0.0}{1.5}{2}{2}{0}{fill=violet!80};
			\cuboid{5.0}{0.0}{1.5}{2}{2}{0}{fill=violet!80};
			\cuboid{7.5}{0.0}{1.5}{2}{2}{0}{fill=violet!80};
			\cuboid{0.0}{2.5}{1.5}{2}{2}{0}{fill=violet!80};
			\cuboid{2.5}{2.5}{1.5}{2}{2}{0}{fill=violet!80};
			\cuboid{5.0}{2.5}{1.5}{2}{2}{0}{fill=violet!80};
			\cuboid{7.5}{2.5}{1.5}{2}{2}{0}{fill=violet!80};
			\cuboid{0.0}{5.0}{1.5}{2}{2}{0}{fill=violet!80};
			\cuboid{2.5}{5.0}{1.5}{2}{2}{0}{fill=violet!80};
			\cuboid{5.0}{5.0}{1.5}{2}{2}{0}{fill=violet!80};
			\cuboid{7.5}{5.0}{1.5}{2}{2}{0}{fill=violet!80};
			\cuboid{0.0}{7.5}{1.5}{2}{2}{0}{fill=violet!80};
			\cuboid{2.5}{7.5}{1.5}{2}{2}{0}{fill=violet!80};
			\cuboid{5.0}{7.5}{1.5}{2}{2}{0}{fill=violet!80};
			\cuboid{7.5}{7.5}{1.5}{2}{2}{0}{fill=violet!80};
			\end{tikzpicture}
			\caption{Tenseur $\mathbf{A}:\: d=5$}
			\label{tens_d5(i)}
		\end{subfigure}
		
		\begin{subfigure}[]{0.3\textwidth}
			\centering
			\begin{tikzpicture}[scale=0.75]
			\draw (-1,0) -- (1,0)  node [near start] {$/$} node[near end] {$/$} node [midway,scale=1.5]{\cdb{$\bullet$}};
			\draw (0,-1) -- (0,1)   node [near start] {$/$} node[near end] {$/$} ;
			\draw (0,0) -- (1,1)  node [midway,scale=1] {$|$};
			\draw (0,0) -- (-1,1)  node [midway,scale=1] {$|$};
			\end{tikzpicture}
			\caption{Tenseur $\mathbf{A}:\: d=6$}
			\label{tens_d6(j)}
		\end{subfigure}
		\begin{subfigure}[]{0.7\textwidth}
			\centering
			\includegraphics[width=0.7\linewidth]{"My figures/FIG41"}
			\caption{Tenseur $\mathbf{A}:\: d=6$ \cite[p.4]{cichocki2014era}}
			\label{tens_d6(k)}
	\end{subfigure}}
	
	\caption{Représentation visuelle des tenseurs pour $ d\in\{3,4,5,6\} $ (\cite[p.133]{Nyenyezi2018}).}
	\label{fig:2.1}
\end{figure}
Les sous-tenseurs à valeur vectorielle sont appelés des \textit{fibres}; définies en fixant tous les indices sauf un (\figref{fibres_modes}), et les sous-tenseurs à valeur matricielle sont appelés des \textit{tranches}, obtenues en fixant tous les indices sauf deux (\figref{Tensor_modes}).\\
On utilise également la notation deux-points pour signifier la sélection de l’ensemble des
indices d’une dimension. Par exemple un fibre d'un tenseur $ \vey $ d'ordre 3  peut être écrit $$\mathbf{x}=\vey\left(:,j_{2},:\right) ,\; j_{2}=1,\cdots,n^{(2)}.$$ Alternativement, la k-ième tranche frontale d'un tenseur tridimensionnel, $ \mbf{A}_{::k} $, peut être dénotée de manière plus compacte par $ \mbf{A}_{k} $.

\begin{rem}[Mode d'un tenseur]
	\emph{\\}
	Un vecteur peut être noté soit en "mode colonne" ou en
	"mode ligne". Pour une matrice, les colonnes constituent ce qu’on appelle mode colonne ou première mode et les lignes constituent le mode ligne ou deuxième mode (l'inverse est aussi vrai). Le terme mode permet de généraliser ces concepts de lignes, colonnes, ou encore d’axes, aux tableaux à $ d $ dimensions. De ce fait, un tenseur d’ordre $ d $ a
	$ d $ axes principaux différents qu’on appelle généralement modes de ce tenseur.
\end{rem}

\begin{figure}[!h]
	\encad{\begin{subfigure}[]{0.3\textwidth}
			\centering
			\includegraphics[width=0.5\linewidth]{"My figures/FIG27"}
			\caption{Fibres colonnes (Mode 1)}
			\label{fibre_d1}
		\end{subfigure}
		\begin{subfigure}[]{0.3\textwidth}
			\centering
			\includegraphics[width=0.5\linewidth]{"My figures/FIG28"}
			\caption{Fibres lignes (Mode 2)}
			\label{fibre_d2}
		\end{subfigure}
		\begin{subfigure}[]{0.35\textwidth}
			\centering
			\includegraphics[width=0.5\linewidth]{"My figures/FIG29"}
			\caption{Fibres tubes (Mode 3)}
			\label{fibre_d3}
	\end{subfigure}}
	\caption{Fibres d’un tenseur tridimensionnel \cite[p.2]{cichocki2014era}.}
	\label{fibres_modes}
\end{figure}

\begin{figure}[!h]
	\encad{	\centering\begin{subfigure}[]{0.3\textwidth}
		
			\begin{tikzpicture}[scale = 0.9]
			\node at (1.5,3,4.5) {Mode $1$};
			\tikzstyle{bottomfill} = [thick, fill=lightgray, fill opacity=.1]
			\tikzstyle{camerafill} = [thick, fill=violet!80]
			\tikzstyle{lightfill} = [thick, fill=violet!80]
			\tikzstyle{lightfill2} = [thick, fill=gray]
			\node at (1,-1.5,3) {$3$};
			\node at (0,-1.5,3) {$2$};
			\node at (-1,-1.5,3) {$1$};
			\sliceM{0}{0}{3}{2}{2}{lightfill2}
			\sliceMm{0}{0}{3}{2}{2}{lightfill}
			\sliceMe{0}{0}{3}{2}{2}{lightfill2}
			\sliceMm{1}{0}{3}{2}{2}{lightfill}
			\sliceMm{2}{0}{3}{2}{2}{lightfill}
			\node at (-1.1,0,1){{\tiny$\mathbf{X}_{1,:,:}$}};
			\end{tikzpicture}
			\caption{Tranches verticales}
			\label{mode_d1}
		\end{subfigure}
		\begin{subfigure}[]{0.3\textwidth}
			\begin{tikzpicture}[scale = 0.8]
			\node at (2,3.5,0) {Mode $2$};
			\tikzstyle{bottomfill} = [thick, fill=lightgray, fill opacity=.1]
			\tikzstyle{camerafill} = [thick, fill=violet!80]
			\tikzstyle{lightfill} = [thick, fill=violet!80]
			\tikzstyle{lightfill2} = [thick, fill=gray]
			\node at (0,2.5,0.5) {$1$};
			\node at (0,1.5,0.5) {$2$};
			\node at (0,0.5,0.5) {$3$};
			\sliceM{1}{1}{0.5}{2}{2}{lightfill2}
			\sliceMm{1}{1}{0.5}{2}{2}{lightfill2}
			\sliceMe{1}{1}{0.5}{2}{2}{lightfill}
			\sliceMe{1}{2}{0.5}{2}{2}{lightfill}
			\sliceMe{1}{3}{0.5}{2}{2}{lightfill}
			\node at (1.5,2.8,1){{\tiny$\mathbf{X}_{:,1,:}$}};
			\end{tikzpicture}
			\caption{Tranches horizontales}
			\label{mode_d2}
		\end{subfigure}
		\begin{subfigure}[]{0.35\textwidth}
			\begin{tikzpicture}[scale = 0.8]
			\node at (1.5,4.0,1.0) {Mode $3$};
			\tikzstyle{bottomfill} = [thick, fill=lightgray, fill opacity=.1]
			\tikzstyle{camerafill} = [thick, fill=violet!80]
			\tikzstyle{lightfill} = [thick, fill=violet!80]
			\tikzstyle{lightfill2} = [thick, fill=gray]
			\node at (3,0.5,2.5) {$1$};
			\node at (3,0.5,1.5) {$2$};
			\node at (3,0.5,0.5) {$3$};
			\sliceM{0}{1}{0}{2}{2}{lightfill}
			\sliceMm{0}{1}{0}{2}{2}{lightfill2}
			\sliceMe{0}{1}{0}{2}{2}{lightfill2}
			\sliceM{0}{1}{1}{2}{2}{lightfill}
			\sliceM{0}{1}{2}{2}{2}{lightfill}
			\node at (0.5,1.5,1){{\tiny$\mathbf{X}_{:,:,1}$}};
			\end{tikzpicture}
			\caption{Tranches frontales}
			\label{mode_d3}
	\end{subfigure}}
	\caption{Tranches d’un tenseur tridimensionnel (\cite[p.143]{Nyenyezi2018} et \cite[p.2]{cichocki2014era}). }
	\label{Tensor_modes}
\end{figure}


\section{Quelques traitements numériques des tenseurs}
\label{sec:2.2traitements}
Dans cette section, il sera question de présenter quelques opérations tensorielles. Et la plupart d'entre elles se ramèneront aux notions matricielles précédemment présentées.
\subsection{Produit de Kronecker des matrices}
Le produit tensoriel et plus particulièrement celui de Kronecker est parmi les plus utiles en calcul numérique tensoriel.
\subsubsection{Produit de Kronecker de deux  vecteurs} Étant donnés deux vecteurs $ \mbf{u}\in \R^{n^{(1)}} $ et $ \mbf{v}\in\R^{n^{(2)}} $, deux utilisations différentes mais équivalentes du produit de Kronecker sont à distinguer: Le premier aboutit à un long vecteur dont la longueur est le produit des longueurs des entrées:
\begin{equation}\label{(2.1)}
\text{Pour } \mbf{u}=\begin{pmatrix}
u_{1}\\
u_{2}\\
\vdots\\
u_{n^{(1)}}
\end{pmatrix},\;\mbf{v}=\begin{pmatrix}
v_{1}\\
v_{2}\\
\vdots\\
v_{n^{(2)}}
\end{pmatrix},\; \mbf{u}\otimes \mbf{v}=\begin{pmatrix}
u_{1}\mbf{v}\\
u_{2}\mbf{v}\\
\vdots\\
u_{n^{(1)}}\mbf{v}
\end{pmatrix}\in \R^{n^{(1)}\cdot n^{(2)}}.
\end{equation}

La deuxième utilisation du produit tensoriel est appelée \textit{produit extérieur} (noté souvent par le symbole  "$ \circ $") (voir \eqref{produitexterieur}). Il en résulte une matrice de rang 1 de taille $ n^{(1)}\times n^{(2)} $:
\begin{equation}
\mbf{u}\otimes \mbf{v}=\mbf{u}\mbf{v}^{T}=\begin{pmatrix}
u_{1}\mbf{v}^{T}\\
u_{2}\mbf{v}^{T}\\
\vdots\\
u_{n^{(1)}}\mbf{v}^{T}
\end{pmatrix}\in \R^{n^{(1)}\cdot n^{(2)}}.
\end{equation}
Pour la plupart des cas, s'il n'y a pas d'indication spécifique, la notation $ \otimes $ fera référence à ce dernier produit.
\begin{ex}
	\emph{\\}
	Considérons les vecteurs $ \mbf{u}=\begin{pmatrix}
	1\\
	2\\
	3
	\end{pmatrix} $ et $ \mbf{v}=\begin{pmatrix}
	3\\
	-1\\
	4
	\end{pmatrix} $. Leur produit tensoriel est
	\begin{equation*}
	\mbf{u}\otimes \mbf{v}=\begin{pmatrix}
	3&-1&4\\
	6&-2&8\\
	9&-3&12
	\end{pmatrix}.
	\end{equation*}
\end{ex}
Il est à observer que le produit de deux tenseurs d'ordre 1 (vecteurs) donne un tenseur d'ordre 2 (matrice); les ordres étant additionnés.

La notation $ \ktensor \mbf{v}^{(k)}=\mbf{v}^{(1)}\otimes \mbf{v}^{(2)}\otimes \cdots\otimes \mbf{v}^{(d)} $ indique le produit de Kronecker de $ d $ vecteurs $ \mbf{v}^{(1)}\in V^{(1)} $, $ \mbf{v}^{(2)}\in V^{(2)} $, $\dots$, $ \mbf{v}^{(d)}\in V^{(d)} $.
%Finalement, 

%\section{Quelques produits dans les espaces tensoriels}
\subsubsection{Produit de Kroneceker des matrices}
Soit $ d $ paires d'espaces vectoriels $ V^{(k)} $ et $ W^{(k)}\,;$ $ 1\leq k\leq d $ et les espaces tensoriels correspondant
$\mathcal{V}\text{ et } \W=\gktensor W^{(k)} $ donnés par les applications linéaires $ A^{(k)}:V^{(k)}\longrightarrow W^{(k)}.$
Le produit tensoriel des $ A^{(k)} $, produit dit de Kronecker, est l'application linéaire
\begin{equation}
\A=\gktensor A^{(k)}:\mathcal{V}\longrightarrow \W
\end{equation} défini par 
\begin{equation}\label{(1.4b)}
\ktensor \mbf{v}^{(k)}\in \V\longrightarrow \A\left(\ktensor \mbf{v}^{(k)}\right)=\ktensor\left(A^{(k)}\left( \mbf{v}^{(k)}\right)\right)\in \W,\, \forall \mbf{v}^{(k)}\in V^{(k)}.
\end{equation}
Puisque $\V$ est engendré  par des tenseurs élémentaires, l'équation \eqref{(1.4b)} définit $ \A $ de manière unique sur $ \V. $ Dans le cas où $ V^{(k)}=\R^{I_{k}} $ et $ W^{(k)}=\R^{J_{k}} $, les applications $ A^{(k)} $ sont des matrices de $\R^{I_{k}\times J_{k}}  $. Le produit de Kronecker $ \ktensor A^{(k)} $ appartient à l'espace des matrices $ \R^{\mathbf{I}\times \mathbf{J}} $ avec $ \mathbf{I}=I_{1}\times\cdots\times I_{d} $ et $ \mathbf{J}=J_{1}\times\cdots\times J_{d}.$

Pour $ d=2 $, soit $ I_{1}=\{1,\cdots,n^{(1)}\} $, $ I_{2}=\{1,\cdots,n^{(2)}\} $, $ J_{1}=\{1,\cdots,m^{(1)}\} $ et $ J_{2}=\{1,\cdots,m^{(2)}\} $ des ensembles ordonnés d'indices et utilisant l'ordre lexicographique des couples $ (i,j) $ dans $ \mathbf{I}=I_{1}\times I_{2} $ et $ \mathbf{J}=J_{1}\times J_{2} $. Alors la matrice $ A\otimes B\in \R^{\mathbf{I}\times \mathbf{J}} $ a la forme du bloc
\begin{equation}
\label{2.001}
A\otimes B=\begin{bmatrix}
a_{11}B&a_{12}B&\cdots&a_{1n^{(2)}}B\\
a_{21}B&a_{22}B&\cdots&a_{2n^{(2)}}B\\
\vdots&\vdots&\ddots&\vdots\\
a_{n^{(1)}1}B&a_{n^{(1)}2}B&\cdots&a_{n^{(1)}n^{(2)}}B
\end{bmatrix}.
\end{equation}
Ce produit n’est pas commutatif, cependant il est associatif et distributif par rapport à l’addition. Voici  deux propriétés relatives à ce produit (tirées de \cite[p. 26]{bessaoudi2019reconnaissance}):

\begin{propp}\label{propp:2.1}
	\emph{\\}
	Considérons les matrices $ A^{(k)}\in\R^{I_{k}\times I_{k}},\, k\in\{1,\cdots,d\} $, et  $ B^{(k)}\in\R^{J_{k}\times P_{k}},\, k\in\{1,\cdots,d\} $. Quel que soit $ d\in \N^{*} $, le produit de Kronecker $ \otimes $ vérifie la propriété suivante:
	\begin{equation}
	\left(A^{(1)}\otimes\cdots\otimes A^{(d)}\right)\left(B^{(1)}\otimes\cdots\otimes B^{(d)}\right)=\left(A^{(1)}B^{(1)}\right)\otimes\cdots\otimes\left(A^{(d)}B^{(d)}\right).
	\end{equation}
\end{propp}

\begin{propp}\label{propp:2.2}
	\emph{\\}
Quelles que soient les matrices $ A^{(k)}\in\R^{I_{k}\times I_{k}},\,k\in\{1,\cdots,d\} $, la transposée de leur produit de Kronecker vérifie la relation suivante:
	\begin{equation}
	\left(A^{(1)}\otimes A^{(2)}\otimes\cdots\otimes A^{(d)}\right)^{T}=A^{(1)^T}\otimes A^{(2)^T}\otimes\cdots\otimes A^{(d)^T}.
	\end{equation}
\end{propp}

\begin{prop}
	\label{prop:2.1}
	\emph{\\}
	\begin{enumerate}
		\item[(1)] Si A et B sont des matrices inversibles, alors $ A\otimes B $ est inversible et \begin{equation}\label{inverse}
		(A\otimes B)^{-1}=A^{-1}\otimes B^{-1}.
		\end{equation}
		\item[(2)]
		Si A et B sont des matrices orthogonales, alors $ A\otimes B $ est aussi une matrice orthogonale. 
	\end{enumerate}
\end{prop}
\begin{proof}
	\emph{\\}
	\begin{enumerate}
		\item[(1)]Supposons que les matrices A et B sont des matrices carrées d'ordres respectifs $ n $ et $ p $ et inversibles. D'après la \proppref{propp:2.1};
		$$(A\otimes B)\cdot(A^{-1}\otimes B^{-1})=AA^{-1}\otimes BB^{-1}=I_{n}\otimes I_{p}=I_{np}.$$ De même, $$(A^{-1}\otimes B^{-1})\cdot(A\otimes B)=A^{-1}A\otimes B^{-1}B=I_{n}\otimes I_{p}=I_{np}.$$ D'où $$(A\otimes B)^{-1}=A^{-1}\otimes B^{-1}.$$
		\item[(2)]Les matrices A et B étant orthogonales, elles vérifient $ A^{T}=A^{-1} $ et $ B^{T}=B^{-1} $. D'après la \proppref{propp:2.2} et la \propref{prop:2.1}; relation \eqref{inverse}, on a respectivement $$(A\otimes B)^{T}=A^{T}\otimes B^{T}\text{ et } (A\otimes B)^{-1}=A^{-1}\otimes B^{-1}.$$ On en déduit $$(A\otimes B)^{T}=A^{T}\otimes B^{T}=A^{-1}\otimes B^{-1}=(A\otimes B)^{-1}. $$ Par suite, $ A\otimes B $ est aussi orthogonale.
	\end{enumerate}
	
\end{proof}

\begin{defi}[Valeurs propres]
	\emph{\\}
	Étant données  A et B  deux matrices carrées complexes d'ordre $ n $ et $ p $ respectivement et des valeurs propres $ \{\lambda_{1},\dots,\lambda_{n}\} $ et $ \{\mu_{1},\dots,\mu_{p}\} $, on définit:
	\begin{enumerate}
		\item[$ (1) $] Les valeurs propres de $ A\otimes B $ sont les produits $ \{\lambda_{i}\mu_{j}\} $, $ i=1,\dots,n $; $ j=1,\dots,p $ des valeurs propres de A et de B.
		\item[$ (2) $] Les valeurs propres de $ A\otimes I_{p} +I_{n}\otimes B$ sont les sommes $ \{\lambda_{i}+\mu_{j}\} $.
	\end{enumerate}
\end{defi}
\begin{prop}
	\emph{\\}
	Pour toutes matrices carrées A et B réelles ou complexes d'ordres respectifs $ n $ et $ p $, on a\begin{equation}
	\det(A\otimes B)=(\det A)^{p}(\det B)^{n}\text{ \cite[p.121]{Michel2013}}.
	\end{equation}
\end{prop}
\begin{proof}
	\emph{\\}
	Considérons, dans tous les cas A et B comme des matrices complexes. Soient $ \{\lambda_{1},\dots,\lambda_{n}\} $ les valeurs propres de A et $ \{\mu_{1},\dots,\mu_{p}\} $ celles de B. On sait que $$\det A=\lambda_{1}\cdots\lambda_{n}\text{ et }\det B=\mu_{1}\cdots\mu_{p}.$$
	Les valeurs propres de $ A\otimes B $ étant les produits $ \{\lambda_{i}\mu_{j}\} $, $ i=1,\dots,n $ et $ j=1,\dots,p $, on a alors\begin{equation*}
	\det(A\otimes B)=\prod_{i=1}^{n}\prod_{j=1}^{p}\lambda_{i}\mu_{j}=\left(\prod_{i=1}^{n}\lambda_{i}\right)^{p}\left(\prod_{j=1}^{p}\mu_{j}\right)^{n}=(\det A)^{p}\cdot (\det B)^{n}.
	\end{equation*}
\end{proof}

%\begin{defi}[Tenseur de rang-1]
%\emph{\\} Un tenseur d-dimensionnel 
%$ \mathbf{A} \in \R^{I_{1}\times I_{2}\times\cdots\times I_{d}} $ est de rang-1 s'il peut s'écrire comme produit extérieur de $ d $ vecteurs, c'est-à-dire,
%\begin{equation}
%\mbf{A}=\mbf{a}^{(1)}\circ \mbf{a}^{(2)}\circ\cdots \circ\mbf{a}^{(d)}.
%\end{equation}
%\end{defi}
%Ceci signifie que chaque élément du tenseur est le produit des éléments vectoriels correspondants:
%\begin{equation}
%a_{j_{1}j_{2}\cdots j_{d}}=a_{j_{1}}^{(1)}a_{j_{2}}^{(2)}\cdots a_{j_{d}}^{(d)},\, \forall 1\leq j_{k}\leq I_{k}.
%\end{equation}
%La \figref{fig:rankone} illustre $ \mbf{A}=\mbf{a}\circ\mbf{b}\circ\mbf{c} $; un tenseur tridimensionnel de rang-1.\\
%La définition suivante est l'analogue d'une matrice symétrique.

La définition suivante sera utile dans la suite.
\begin{defi}[Symétrie et tenseurs]\label{tensorsymdef}
	\emph{\\}
 Un tenseur est dit cubique si tous les modes ont la même taille, c'est-à-dire que $ \mbf{A}\in\R^{I\times I\times\cdots\times I} $. Un tenseur cubique est dit supersymétrique (Cependant, ce terme est contesté par Comon et al., qui préfèrent plutôt juste "symétrique") si ses éléments restent constants sous toute permutation des indices. 
\end{defi}
Par exemple, un tenseur tridimensionnel $ \mbf{A}\in\R^{I\times I\times I} $ est supersymétrique si 
\begin{equation}
a_{ijk}=a_{ikj}=a_{jik}=a_{jki}=a_{kij}=a_{kji},\;\forall i,j,k=1,\ldots,I.
\end{equation}
Les tenseurs peuvent également être (partiellement) symétriques dans deux ou plusieurs modes. Par exemple, un tenseur  tridimensionnel $ \mbf{A}\in\R^{I\times I\times R} $  est symétrique dans les modes un et deux si toutes ses tranches frontales sont symétriques, c'est-à-dire, $$\mbf{A}_{k}=\mbf{A}_{k}^{T},\;\forall k=1,\ldots,R.$$
\subsection{Vectorisation des tenseurs}
Les tenseurs numériques peuvent être facilement remodelés dans un vecteur en ordonnant les entrées en longues colonnes suivant l'ordre dit lexicographique.

Étant donné un tenseur $ \mathbf{A} \in \R^{I_{1}\times I_{2}\times\cdots\times I_{d}} $ où chaque entrée est déterminée par un d-uplet $\left( j_{1},j_{2},\cdots,j_{d}\right) $, un nombre naturel $ N=\prod_{k=1}^{d}\# I_{k} $ et un ensemble d'indices $ J=\{1,2,\cdots,N\} $. On peut obtenir un long vecteur dénoté $ \mathbf{a} =vec(\mbf{A})$ à partir de ce tenseur, où chaque entrée est déterminée par un indice $ j\in J $, en définissant un isomorphisme \cite[p.158]{hackbusch2012tensor} $$ \phi :\left(j_{1},j_{2},\cdots,j_{d}\right)\in I_{1}\times I_{2}\times\cdots\times I_{d}\longrightarrow J $$ tel que 
\begin{equation}\label{5.6}
j=\phi\left(j_{1},j_{2},\cdots,j_{d}\right) =j_{1}+\sum_{k=2}^{d}\left(j_{k}-1\right)\prod_{l=1}^{k-1}n^{(l)}.
\end{equation}
En pratique, les indices en J peuvent être obtenus comme 
\begin{equation*}
J=\begin{bmatrix}
I_{1}\\
I_{2}\\
\vdots\\
I_{d}
\end{bmatrix}\in \R^{n^{(1)}\cdot n^{(2)}\cdots n^{(d)}}.
\end{equation*}

La manipulation des tenseurs nécessite souvent leur reformatage (\textit{remise en forme}); un cas particulier de remise en forme des tenseurs en matrices est appelé \textit{matricialisation} comme le précise le point suivant.
\subsection{Matricialisation des tenseurs}
La \textit{matricialisation} appelée aussi \textit{dépliage} ou \textit{aplatissement} est une opération de remodelage d'un tenseur en une matrice. Un tenseur $ \mathbf{A} \in \R^{I_{1}\times I_{2}\times\cdots\times I_{d}} $ peut être transformé en matrice de différentes manières.

L'idée est de réduire l'ensemble de dimension $ D=\{1,2,\cdots,d\} $ en deux sous-ensembles disjoints et complémentaires $ t $ et $ t^{c} $ tels que $ \emptyset\subsetneq t\subsetneq D $ et $ t^{c}=D\textbackslash t .$ Alors, on obtient les ensembles d'indices $ I_{t}=\times_{k\in t}I_{k} $ (qui constituera les indices des lignes) et $ I_{t^{c}}=\times_{k\in t^{c}}I_{k} $ (qui constituera les indices des colonnes). Alors la t-matricialisation de $ \mathbf{A} $ est donnée par 
\begin{equation}\label{matricisation1}
M_{t}(\mathbf{A})=A^{(t)}\in \R^{I_{t}\times I_{t^{c}}},
\end{equation}où $ M_{t} $ est considérée comme application de t-matricialisation. Les entrées de la matrice $ A^{(t)} $ sont $ A^{(t)}(k,l) $; les indices k et l sont donnés respectivement par
\begin{eqnarray}
k&=&j_{1}+\sum_{r=2}^{k}\left(j_{r}-1\right)\prod_{l=1}^{r-1}n^{(l)}\\
&&\text{ et }\nonumber\\
l&=&j_{k+1}+\sum_{r=k+2}^{d}\left(j_{r}-1\right)\prod_{l=1}^{r-1}n^{(l)}.
\end{eqnarray}
Le t-rang est le rang de la matrice $ A^{(t)} $.

Pour t qui est un singleton, on forme les matrices dites mode $ k $ ou matrices de dépliage de mode $ k $ données par
\begin{equation}\label{(2.15)}
	A^{(k)}\in \R^{I_{k}\times (I_{1}\times I_{2}\times \cdots\times I_{k-1}\times I_{k+1}\times\cdots\times I_{d})},\: k=1:d.
\end{equation}
On observe qu'il suffit de remodeler la matrice en fixant l'indice correspondant à l'ensemble des indices $ I_{k} $ en mode-ligne en faisant varier tous les autres indices en mode-colonne. La matrice $ A^{(k)} $ est équivalente au tenseur entier mais c'est une matrice en mode $ k $. L'entrée $ a_{j_{1}j_{2}\cdots j_{d}} $ devient $ a_{il} $ où $ i=j_{k} $ et 
\begin{equation}
l=j_{1}+\sum_{r=2}^{k-1}\left(j_{r}-1\right)\prod_{l=1}^{r-1}n^{(l)}+\sum_{r=k+1}^{d}\left(j_{r}-1\right)\prod_{l=1}^{r-1}n^{(l)}.
\end{equation}

\begin{ex}
\emph{\\}
Étant donné un tenseur  du troisième ordre $ \mbf{A}\in\R^{3\times 3\times 3} $ avec $ n^{(1)}=n^{(2)}=n^{(3)}=3 $, sa  taille est $ n=[3,3,3] $ et son nombre d'entrées est $ N=27 $. La \figref{fig:Tenseurd'ordre3} illustre le classement des indices considéré ici pour un tenseur à trois dimensions. La figure montre que pour les différents modes, les indices désignent des pensées différentes mais l'ensemble du tenseur est décrit par n'importe quel mode.
\newpage
\begin{figure}[htbp]
\encad{\centering\begin{tikzpicture}
	\cuboid{0}{0}{0}{2.1}{2.1}{4}{fill=cyan!55};
	\draw [->,thick](-0.5,2.4,4) --++(0.9,0,0) node[below, midway, scale=0.5] {mode $1$};
	\draw [->,thick](-0.5,2.4,4) --++(0,-0.9,0) node[below,midway, rotate=-90, scale=0.5] {mode $2$};
	\draw [->,thick](-0.5,2.4,4) --++(0,0,-0.9) node[left,above, rotate=45, scale=0.5] {mode $3$};
	\node (A) at (0.2,0.9,3.5) {111};
	\node (B) at (0.2,0.5,3.5) {211};
	\node (C) at (0.2,0.1,3.5) {311};
	\node (D) at (0.9,0.9,3.5) {112};
	\node (E) at (0.9,0.5,3.5) {212};
	\node (F) at (0.9,0.1,3.5) {312};
	\node (G) at (1.6,0.9,3.5) {113};
	\node (H) at (1.6,0.5,3.5) {213};
	\node (I) at (1.6,0.1,3.5) {313};
	
	\node[color=magenta] (AA) at (0.6,1.7,2.5) {121};
	\node[color=magenta] (BB) at (0.6,1.3,2.5) {221};
	\node[color=magenta] (CC) at (0.6,0.9,2.5) {321};
	\node[color=magenta] (DD) at (1.3,1.7,2.5) {122};
	\node[color=magenta] (EE) at (1.3,1.3,2.5) {222};
	\node[color=magenta] (FF) at (1.3,0.9,2.5) {322};
	\node[color=magenta] (GG) at (2.0,1.7,2.5) {123};
	\node[color=magenta] (HH) at (2.0,1.3,2.5) {223};
	\node[color=magenta] (II) at (2.0,0.9,2.5) {323};
	
	\node (AAA) at (0.9,2.5,1.5) {131};
	\node (BBB) at (0.9,2.1,1.5) {231};
	\node (CCC) at (0.9,1.7,1.5) {331};
	\node (DDD) at (1.6,2.5,1.5) {132};
	\node (EEE) at (1.6,2.1,1.5) {232};
	\node (FFF) at (1.6,1.7,1.5) {332};
	\node (GGG) at (2.3,2.5,1.5) {133};
	\node (HHH) at (2.3,2.1,1.5) {233};
	\node (III) at (2.3,1.7,1.5) {333};
	
	\end{tikzpicture}
	\begin{tikzpicture}
	\node[circle,scale=0.5,fill= violet!80] (E) at (0,1){Mode $1$};
	\node[circle,scale=0.5,fill= gray!30] (A) at (0,0.5){$i,j,k$};
	\node (B) at (3,1.4){\cre{$i\rightarrow$}indice ligne};
	\node (C) at (3,0.75){ \cre{$j\rightarrow$} indice colonne};
	\node (D) at (3,0){\cre{$k\rightarrow$} indice tranche};
	\draw [->](A) to (1.8,1.3);
	\draw [->](A) to (C) ;
	\draw [->](A) to (D) ; 
	
	\node[circle,scale=0.5,fill= violet!80] (EE) at (0,-1){Mode $2$};
	\node[circle,scale=0.5,fill= gray!30] (AA) at (0,-1.5){$i,j,k$};
	\node (BB) at (3,-1){\cre{$i\rightarrow$} indice tranche};
	\node (CC) at (3,-1.5){ \cre{$j\rightarrow$} indice colonne};
	\node (DD) at (3,-2){\cre{$k\rightarrow$} indice ligne};
	\draw [->](AA) to (BB);
	\draw [->](AA) to (CC) ;
	\draw [->](AA) to (DD) ; 
	
	\node[circle,scale=0.5,fill= violet!80] (EEE) at (0,-3){Mode $3$};
	\node[circle,scale=0.5,fill= gray!30] (AAA) at (0,-3.5){$i,j,k$};
	\node (BBB) at (3,-3){\cre{$i\rightarrow$} indice ligne};
	\node (CCC) at (3,-3.5){ \cre{$j\rightarrow$} indice tranche};
	\node (DDD) at (3,-4){\cre{$k\rightarrow$} indice colonne};
	\draw [->](AAA) to (BBB);
	\draw [->](AAA) to (CCC) ;
	\draw [->](AAA) to (DDD) ; 
	\end{tikzpicture}\emph{\\}
	\vspace{0.2cm}}
	\caption{ %\caption{Indices de la matricisation du tenseur $\mathbf{A}\in\mathbb{R}^{3\times{3}\times{3}}\; en\; mode\; 2$.}
		%\label{tabMode2}
		Indices dans un tenseur d'ordre $3$
		 \cite[p.136]{Nyenyezi2018}.}
	\label{fig:Tenseurd'ordre3}
\end{figure}
Lorsqu'il est vectorisé, ses entrées seront positionnées de $ 1 $ à $ 27 $ (on peut utiliser la commande \texttt{vec} de {\ttfamily Matlab}). La matricialisation
de $ \mathbf{A} $ dans les modes respectifs $ 1 $, $ 2 $, $ 3 $, donne les $ 3 $ matrices d'indices suivantes:
$$A^{(1)}=
\begin{bmatrix}
\begin{array}{*{11}{c}}
\mbf{1}11&\mbf{1}12&\mbf{1}13&&\mbf{1}21&\mbf{1}22&\mbf{1}23&&\mbf{1}31&\mbf{1}32&\mbf{1}33\\
\mbf{2}11&\mbf{2}12&\mbf{2}13&&\mbf{2}21&\mbf{2}22&\mbf{2}23&&\mbf{2}31&\mbf{2}32&\mbf{2}33\\
\mbf{3}11&\mbf{3}12&\mbf{3}13&&\mbf{3}21&\mbf{3}22&\mbf{3}23&&\mbf{3}31&\mbf{3}32&\mbf{3}33\\
\end{array}
\end{bmatrix}$$
$$A^{(2)}=\begin{bmatrix}
\begin{array}{*{11}{c}}
1\mbf{1}1&1\mbf{1}2&1\mbf{1}3&&2\mbf{1}1&2\mbf{1}2&2\mbf{1}3&&3\mbf{1}1&3\mbf{1}2&3\mbf{1}3\\
1\mbf{2}1&1\mbf{2}2&1\mbf{2}3&&2\mbf{2}1&2\mbf{2}2&2\mbf{2}3&&3\mbf{2}1&3\mbf{2}2&3\mbf{2}3\\
1\mbf{3}1&1\mbf{3}2&1\mbf{3}3&&2\mbf{3}1&2\mbf{3}2&2\mbf{3}3&&3\mbf{3}1&3\mbf{3}2&3\mbf{3}3\\
\end{array}
\end{bmatrix}$$

$$A^{(3)}=\begin{bmatrix}
\begin{array}{*{11}{c}}
11\mbf{1}&12\mbf{1}&13\mbf{1}&&21\mbf{1}&22\mbf{1}&23\mbf{1}&&31\mbf{1}&32\mbf{1}&33\mbf{1}\\
11\mbf{2}&12\mbf{2}&13\mbf{2}&&21\mbf{2}&22\mbf{2}&23\mbf{2}&&31\mbf{2}&32\mbf{2}&33\mbf{2}\\
11\mbf{3}&12\mbf{3}&13\mbf{3}&&21\mbf{3}&22\mbf{3}&23\mbf{3}&&31\mbf{3}&32\mbf{3}&33\mbf{3}\\
\end{array}
\end{bmatrix}.$$

On observe que,  pour chaque $k=1,2,3$, le numéro de ligne de la matrice $ A^{(k)} $ correspond au k\up{ième} indice du tenseur $\mbf{A}.$

La \figref{fig:matricialisation} visualise un tenseur tridimensionnel de taille ($ 3\times 3\times 3 $) et ses dépliages en mode $ 1 $, mode $ 2 $ et mode $ 3 $ respectivement. Le nombre $ r_{k} $, $  k = 1,2,3 $ indique le rang de la matrice de dépliage dans le mode $ k $.
\begin{figure}[!h]
	\centering
	\encad{
		\begin{subfigure}[]{0.2\textwidth}
			\begin{tikzpicture}[scale=0.4]
			\node at (0,6.5,0) {$\mbf{A} \in \R^{3\times 3 \times 3}$};
			\cuboid{0}{2}{0}{1}{1}{1}{fill=violet!80};
			\cuboid{1}{2}{0}{1}{1}{1}{fill=violet!80};
			\cuboid{2}{2}{0}{1}{1}{1}{fill=violet!80};
			\cuboid{0}{3}{0}{1}{1}{1}{fill=violet!80};
			\cuboid{1}{3}{0}{1}{1}{1}{fill=violet!80};
			\cuboid{2}{3}{0}{1}{1}{1}{fill=violet!80};
			\cuboid{0}{4}{0}{1}{1}{1}{fill=violet!80};
			\cuboid{1}{4}{0}{1}{1}{1}{fill=violet!80};
			\cuboid{2}{4}{0}{1}{1}{1}{fill=violet!80};
			
			\cuboid{0}{2}{1}{1}{1}{1}{fill=violet!80};
			\cuboid{1}{2}{1}{1}{1}{1}{fill=violet!80};
			\cuboid{2}{2}{1}{1}{1}{1}{fill=violet!80};
			\cuboid{0}{3}{1}{1}{1}{1}{fill=violet!80};
			\cuboid{1}{3}{1}{1}{1}{1}{fill=violet!80};
			\cuboid{2}{3}{1}{1}{1}{1}{fill=violet!80};
			\cuboid{0}{4}{1}{1}{1}{1}{fill=violet!80};
			\cuboid{1}{4}{1}{1}{1}{1}{fill=violet!80};
			\cuboid{2}{4}{1}{1}{1}{1}{fill=violet!80};
			
			\cuboid{0}{2}{2}{1}{1}{1}{fill=violet!80};
			\cuboid{1}{2}{2}{1}{1}{1}{fill=violet!80};
			\cuboid{2}{2}{2}{1}{1}{1}{fill=violet!80};
			\cuboid{0}{3}{2}{1}{1}{1}{fill=violet!80};
			\cuboid{1}{3}{2}{1}{1}{1}{fill=violet!80};
			\cuboid{2}{3}{2}{1}{1}{1}{fill=violet!80};
			\cuboid{0}{4}{2}{1}{1}{1}{fill=violet!80};
			\cuboid{1}{4}{2}{1}{1}{1}{fill=violet!80};
			\cuboid{2}{4}{2}{1}{1}{1}{fill=violet!80};
			\end{tikzpicture}
			\caption{}
			\label{sub:aa}
		\end{subfigure}
		\begin{subfigure}[]{0.25\textwidth}
			\begin{tikzpicture}[scale=0.2]
			\node at (2,8.5,0) {$\mbf{A}^{(1)} \in \R^{3\times 9}$};
			\cuboid{0}{1}{0}{3}{3}{1}{fill=violet!80};
			\cuboid{3}{1}{0}{3}{3}{1}{fill=violet!80};
			\cuboid{6}{1}{0}{3}{3}{1}{fill=violet!80};
			\node at (2,-1.1,0) {\small{$r_1 = rang(\mbf{A}^{(1)})$}};
			\end{tikzpicture}
			\caption{}
			\label{sub:bb}
		\end{subfigure}
		\begin{subfigure}[]{0.2\textwidth}
			\begin{tikzpicture}[scale=0.2]
			\node at (8,8.5,0) {$\mbf{A}^{(2)} \in \R^{9\times 3}$};
			\cuboid{0}{0}{0}{3}{3}{1}{fill=violet!80};
			\cuboid{0}{3}{0}{3}{3}{1}{fill=violet!80};
			\cuboid{0}{6}{0}{3}{3}{1}{fill=violet!80};
			\node at (4,-1.1,1) {\small{$r_2 = rang(\mbf{A}^{(2)})$}};
			\end{tikzpicture}
			\caption{}
			\label{sub:cc}
		\end{subfigure}
		\begin{subfigure}[]{0.2\textwidth}
			\begin{tikzpicture}[scale=0.2]
			\node at (5,9.0,0) {$\mbf{A}^{(3)} \in \R^{3\times 9}$};
			\cuboid{0}{4}{0}{3}{1}{3}{fill=violet!80};
			\cuboid{0}{4}{3}{3}{1}{3}{fill=violet!80};
			\cuboid{0}{4}{6}{3}{1}{3}{fill=violet!80};
			\node at (2,0,1) {\small{$r_3 = rang(\mbf{A}^{(3)})$}};
			\end{tikzpicture}
			\caption{}
			\label{sub:dd}
		\end{subfigure}
		\emph{\\}
		\vspace{0.2cm}
	}
	\caption{Tenseur d'ordre $d=3$ (\ref{sub:aa}), taille $(3 \times 3 \times 3)$ et sa matricialisation en mode $1$ (\ref{sub:bb}), mode $2$ (\ref{sub:cc}), et mode $3$ (\ref{sub:dd}) respectivement \cite[p.137]{Nyenyezi2018}.}
	\label{fig:matricialisation}
\end{figure}
\newpage
Pratiquement, considérons les tranches frontales de $ \mbf{A}\in\R^{3\times 4\times2} $\begin{equation}\label{(2.11)}
\mbf{A}_{1}=\begin{bmatrix}
1&4&7&10\\ 2&5&8&11\\3&6&9&12\\
\end{bmatrix},\, \mbf{A}_{2}=\begin{bmatrix}
13&16&19&22\\
14&17&20&23\\
15&18&21&24\\
\end{bmatrix}
\end{equation}
Alors, les trois déploiements mode k sont:
\begin{eqnarray}
A^{(1)}&=&\begin{bmatrix}
1&4&7&10&13&16&19&22\\
2&5&8&11&14&17&20&23\\
3&6&9&12&15&18&21&24\\
\end{bmatrix}\nonumber\\
A^{(2)}&=&\begin{bmatrix}
1&2&3&13&14&15\\
4&5&6&16&17&18\\
7&8&9&19&20&21\\
10&11&12&22&23&24\\
\end{bmatrix}\nonumber\\
A^{(3)}&=&\begin{bmatrix}
1&2&3&4&5&\cdots&9&10&11&12\\
13&14&15&16&17&\cdots&21&22&23&24\\
\end{bmatrix}.\nonumber
\end{eqnarray}
Différents auteurs utilisent parfois un ordre différent des colonnes pour le déroulement du mode k. En général, la permutation spécifique des colonnes n'est pas importante tant qu'elle est cohérente dans les calculs connexes \cite[p.460]{Hong2008}. Quant à la vectorisation d'un tenseur; encore une fois, l'ordre des éléments n'a pas d'importance tant qu'il est cohérent. Dans l'exemple ci-dessus, la version vectorisée est \begin{equation*}
\verb|vec|(\mbf{A})=\begin{bmatrix}
1\\2\\\vdots\\24
\end{bmatrix}.
\end{equation*}
\end{ex}










\subsection{Tensorisation des vecteurs}\label{tensoriser}
La tensorisation d'un vecteur est l'opération inverse de la vectorisation d'un tenseur: un vecteur est isomorphiquement transformé en un tenseur, même si la structure du tenseur n'est pas donnée au préalable.\\
 En effet, comme la relation \eqref{5.6} est une bijection, sa réciproque peut être utilisée pour tensoriser des vecteurs. Considérons un vecteur $ \mbf{x}\in\R^{N} $ où $N = \prod_{k =1}^d n^{(k)}$ pour un certain $n^{(k)} \in \N$, la tensorisation de $ \mbf{x} $ conduit à un tenseur $ \mbf{X} $ à $ d $ dimensions de taille $[n^{(1)},\ldots,n^{(d)}]$ tel que
$$\mathbf{X}(j_{_1},j_{_2},\ldots,j_{_d}) = \mathbf{x}_j$$ 
où j est donné par la relation \eqref{5.6}. Pour plus de détails, voir \cite[pp.170-172 et pp.10-12]{hackbusch2012tensor}.

\subsection{Produit tenseur-matrice}
Pour introduire la notation générale du produit tenseur-matrice, nous allons rappeler les principes du produit matrice-matrice, noté ici par le point. Étant données des matrices $F \in \R^{I_1 \times I_2}, U \in \R^{I_1 \times J_1} $, et $V \in \R^{I_2 \times J_2}$, il est possible de calculer le produit
\begin{equation}
\label{5.11}
G = U^T\cdot F\cdot V  \in \R^{J_{1} \times J_{2}}
\end{equation}
obéissant à la compatibilité des tailles des modes. En analysant le produit \eqref{5.11} on peut noter que les lignes de F sont multipliées par U (et non $ U^{T} $) alors que les colonnes de F sont multipliées par V. De la même manière, on peut remarquer que, dans le résultat matriciel G, les lignes de U sont associées à l'espace des lignes de G tandis que les lignes de V sont associées à l'espace des colonnes de G. Ainsi, on peut éviter de transposer U mais travailler avec la multiplication dite en mode $ k $ ($ \times_{k} $) telle que
\begin{equation}\label{modekProd1}
G = F \times_{1} U\times_{2} V.
\end{equation}

La notation dans \eqref{modekProd1} signifie que la matrice F est multipliée par U en mode 1 (mode ligne) et par V en mode2 (mode colonne).\\
En général, la multiplication en mode k est donnée par la définition ci-dessous:
% \cite[p.128]{Nyenyezi2018}: 
\begin{defi}[Multiplication en mode $ k $]
\emph{\\}
Soit un tenseur  $\mathbf{A} \in \R^{I_1 \times I_2\times \ldots \times I_{d}}$ et une matrice $M \in \R^{I_k \times J_k},\; 1\leq k \leq d.$ Le produit en mode $k$ de $\mbf{A}$ et $M$ noté $\mbf{A}\times_k M$ est un tenseur $\left(I_1 \times \ldots \times I_{k-1}\times J_k \times I_{k+1} \ldots I_d\right) $ dont les entrées sont données par
\begin{equation}
\left(\mbf{A}\times_k M \right)(i_1, i_2, \ldots,i_{k-1}, j_k,i_{k+1}, \ldots, i_d)= \sum_{i_{k}=1}^{I_{k}} a_{i_1 i_2 \ldots  i_{k-1} i_k  i_{k+1} \ldots i_d}m_{i_kj_k}. 
\end{equation}
\end{defi}
Chaque fibre mode k est multipliée par la matrice M. L'idée peut également être exprimée en termes de tenseurs dépliés:
\begin{equation}
\mbf{F}=\mbf{A}\times_{k}M\Leftrightarrow F^{(k)}=MA^{(k)}.
\end{equation}
Le produit  mode $ k $ d'un tenseur avec une matrice est lié à un changement de base dans le cas où un tenseur définit un opérateur multilinéaire \cite[p.461]{Hong2008}.

 Ce produit mode $ k $ vérifie les propriétés suivantes:

\begin{propp}
\emph{\\}
Étant donné un tenseur $\mathbf{A} \in \R^{I_1 \times I_2\times \ldots \times I_{d}}$ et les matrices $F \in \R^{I_k \times J_k}, G \in \R^{I_l \times J_l}$,\begin{equation}
(\mathbf{A}\times_k F)\times_l G = (\mathbf{A}\times_l G)\times_k F = \mathbf{A}\times_k F \times_l G, \:(k\neq l).
\end{equation}
\end{propp}
\begin{propp}
\emph{\\}
Étant donné un tenseur $\mathbf{A} \in \R^{I_1 \times I_2\times \ldots \times I_{d}}$ et les matrices $F \in \R^{I_k \times J_k}$ et $ G \in \R^{J_k \times K_k}$.
\begin{equation}
(\mathbf{A}\times_k F)\times_k G = \mathbf{A}\times_k(G.F).
\end{equation}
\end{propp}
\begin{ex}
\emph{\\}
Considérons les tranches frontales d'un tenseur $ \mbf{A}\in\R^{3\times 4\times2} $ définies par \eqref{(2.11)}, c'est-à-dire\begin{equation*}
\mbf{A}_{1}=\begin{bmatrix}
1&4&7&10\\ 2&5&8&11\\3&6&9&12\\
\end{bmatrix}, \mbf{A}_{2}=\begin{bmatrix}
13&16&19&22\\
14&17&20&23\\
15&18&21&24\\
\end{bmatrix}\text{ et une matrice  }M=\begin{bmatrix}
1&3&5\\2&4&6\\
\end{bmatrix}.
\end{equation*}
Le produit $ \mbf{F}=\mbf{A}\times_{1}M\in\R^{2\times 4\times 2} $ est
\begin{equation*}
\mbf{F}_{1}=M\mbf{A}_{1}=\begin{bmatrix}
22&49&76&103\\28&64&100&136
\end{bmatrix},\; \mbf{F}_{2}=M\mbf{A}_{2}=\begin{bmatrix}
130&157&184&211\\172&208&244&280
\end{bmatrix}.
\end{equation*}
\end{ex}
Le produit mode k(vecteur) d'un tenseur $ \mbf{A}\in\RItd $ avec un vecteur $ \mbf{v}\in\R^{I_{k}} $ est dénoté par $ \mbf{A}\overline{\times }_{k}\mbf{v} $. Le résultat est d'ordre $ d-1 $, c'est-à-dire, la taille est $ I_{1}\times\cdots\times I_{k-1}\times I_{k+1}\times\cdots\times I_{d} $. Par éléments, 
\begin{equation}
(\mbf{A}\overline{\times}_{k}\mbf{v})_{j_{1}\cdots j_{k-1}j_{k+1}\cdots j_{d}}=\sum_{j_{k}=1}^{I_{k}}a_{j_{1}j_{2}\cdots j_{d}}v_{j_{k}}.
\end{equation}
L'idée est de calculer le produit interne de chaque fibre mode k avec le vecteur $ \mbf{v} $.
\begin{ex}
\emph{\\}
 Considérons $ \mbf{A} $ tel que donné dans \eqref{(2.11)} et définissons $ \mbf{v}=\begin{bmatrix}
1&2&3&4
\end{bmatrix}^{T} $. Alors
$$\mbf{A}\overline{\times }_{2}\mbf{v}=\begin{bmatrix}
70&190\\
80&200\\
90&210\\
\end{bmatrix}. $$
\end{ex}

Lorsqu'il s'agit d'une multiplication vectorielle en mode k, la priorité est importante car l'ordre des résultats intermédiaires change. En d'autres termes,
\begin{equation}
\mbf{A}\overline{\times}_{k}\mbf{v}\overline{\times}_{l}\mbf{w}=(\mbf{A}\overline{\times}_{k}\mbf{v})\overline{\times}_{l-1}\mbf{w}=(\mbf{A}\overline{\times}_{l}\mbf{w})\overline{\times}_{k}\mbf{v},\; k\leq l.
\end{equation}

\subsection{Produit Tenseur-Tenseur}

En général, étant donnés des tenseurs $\mathbf{A} \in \R^{I_1 \times I_2\times \ldots \times I_{d_1}}$ et $\mathbf{B} \in \R^{J_1 \times J_2\times \ldots \times J_{d_2}},$ leur produit donne un tenseur $ \mbf{C} $ d'ordre $ d_{1} + d_{2} $ tel que
\begin{equation}
\mathbf{C}\left(i_{_1},i_{_2},\ldots i_{d_1},j_{_1},j_{_2},\ldots,j_{_{d_2}}\right) =   \mathbf{A}\left(i_{_1},i_{_2},\ldots i_{d_1}\right)\mathbf{B}\left({j_1},j_{_2}, \ldots,j_{_{d_2}}\right) = \mathbf{A} \otimes \mathbf{B}.
\end{equation}
\begin{ex}
\emph{\\}
A partir du produit $\begin{pmatrix}
1\\
0\\
\end{pmatrix}
\otimes
\begin{pmatrix}
1\\
0\\
\end{pmatrix}
\otimes
\begin{pmatrix}
1\\
1\\
\end{pmatrix}
= 
\begin{pmatrix}
1&0\\
0&0\\
\end{pmatrix}
\otimes
\begin{pmatrix}
1\\
1\\
\end{pmatrix}
$  on obtient un tenseur tridimensionnel $\mbf{A} \in \R^{2 \times 2 \times 2}$ qui a
des tranches identiques $\begin{pmatrix}
1&0\\
0&0\\
\end{pmatrix}$ et peut être donné par son dépliage en mode $ 1 $
$$A^{(1)}=
\begin{bmatrix}
\begin{array}{*{5}{c}}
1&0&\vline&1&0\\
0&0&\vline&0&0
\end{array}
\end{bmatrix}.
$$
\end{ex}
\subsection{Produit de Khatri-Rao}
Le produit de Khatri-Rao est le produit de Kronecker "correspondant en colonnes". Considérons les matrices  $A=[\mathbf{a}_1\cdots\mathbf{a}_K]\in \mathbb{R}^{I\times{K}}$ et $B=[\mathbf{b}_1\cdots\mathbf{b}_K]\in \mathbb{R}^{J\times{K}}$ où $\mathbf{a}_k$ et $\mathbf{b}_k$ désignent respectivement les $k^{ième}$ colonnes de A et de B. Leur produit de Khatri-Rao est dénoté $ A\odot B $. Le résultat est une matrice de taille $ (IJ)\times K $ définie par  
\begin{equation}
\label{tensProdKR}
C=A\odot{B}=\begin{bmatrix}
\mbf{a}_{1}\otimes\mbf{b}_{1}&\mbf{a}_{2}\otimes\mbf{b}_{2}&\cdots&\mbf{a}_{K}\otimes\mbf{b}_{K}
\end{bmatrix}.
\end{equation}
Si $ \mbf{a} $ et $\mbf{b}  $ sont des vecteurs, alors les produits de Khatri-Rao et de Kronecker  sont identiques, c'est-à-dire,\begin{equation}
\mbf{a}\otimes\mbf{b}=\mbf{a}\odot\mbf{b}.
\end{equation}  
%dont les colonnes sont le produit de Kronecker \eqref{(2.1)} des colonnes respectives de A et B, soit
%$$\textbf{c}_i=\textbf{a}_i\otimes{\textbf{b}_i},\;i=1:r.$$
\subsection{Produit de Hadamard}
Le produit de Hadamard est le produit matriciel par éléments. Étant données les matrices A et 
B, toutes deux de taille $ I\times J $, leur produit de Hadamard est noté $ A\ast B $. Le résultat est également de taille $ I\times J $ et défini par
\begin{equation}
A\ast B=\begin{bmatrix}
a_{11}b_{11}&a_{12}b_{12}&\cdots&a_{1J}b_{1J}\\
a_{21}b_{21}&a_{22}b_{22}&\cdots&a_{2J}b_{2J}\\
\vdots&\vdots&\ddots&\vdots\\
a_{I1}b_{I1}&a_{I2}b_{I2}&\cdots&a_{IJ}b_{IJ}\\
\end{bmatrix}.
\end{equation}
En dépit des propriétés déjà évoquées ci-haut pour le produit de Kronecker, ces produits matriciels sont liés par les relations suivantes:
\begin{propp}\emph{\\}
	\begin{enumerate}
	\item[$ (1) $]$ A\odot B\odot C=(A\odot B)\odot C=A\odot(B\odot C) $,
	\item[$ (2) $]$ (A\odot B)^{T}(A\odot B)=A^{T}A\ast B^{T}B. $
	\item[$ (3) $]
	$ (A\odot B)^{\dagger}=\left((A^{T}A)\ast(B^{T}B)\right)^{\dagger}(A\odot B)^{T} $, où $ A^{\dagger} $ dénote le pseudo-inverse de Moore-Penrose de A (voir \defref{MOORE}).
	
	\item[$ (4) $]
	Pour l'utilité du produit de Kronecker, en considérant $ \mbf{A}\in\RItd $ et $ B^{(k)}\in\R^{J_{k}\times I_{k}},\; \forall k\in\{1,\dots,d\} $; on a:
	\begin{eqnarray}
	\mbf{F}&=&\mbf{A}\times_{1}B^{(1)}\times_{2}B^{(2)}\cdots\times_{d}B^{(d)}\Leftrightarrow\nonumber\\
	F^{(k)}&=&B^{(k)}A^{(k)}\left(B^{(d)}\otimes\cdots\otimes B^{(k+1)}\otimes B^{(k-1)}\otimes\cdots\otimes B^{(1)}\right)^{T}.\nonumber
	\end{eqnarray}
	\end{enumerate}
Pour plus de détails, voir \cite[p.462]{Hong2008}.
\end{propp} 

\subsection{Produit contracté}
La multiplication des tenseurs peut être généralisée simplement par le produit dit contracté. La contraction peut se faire sur un mode spécifique (produit en mode k) ou sur plusieurs modes: pour deux vecteurs $\mathbf{u}, \mathbf{v} \in \R^{n^{(1)}}$, le produit en mode 1 est simplement le produit scalaire classique
\begin{equation}
\mathbf{u}.\mathbf{v} = \sum_{i=1}^{n^{(1)}}u_iv_i.
\end{equation}
 Pour une matrice $A \in \R^{n^{(1)}\times n^{(2)}}$ et des vecteurs $\mathbf{u} \in \R^{n^{(1)}},\mathbf{v} \in \R^{n^{(2)}}$, nous pouvons utiliser les produits suivants pour exprimer les produits matrice-vecteur:   
$$
\begin{array}{lcl}
A \mathbf{v}& =&A \times_1 \mathbf{v}\in \R^{n^{(1)}}\quad \text{produit en mode $1$},\\
\mathbf{u}^T A&= &A \times_2 \mathbf{u} \in \R^{n^{(2)}}\quad \text{produiten  mode $2$ },\\
\mathbf{u}^T A \mathbf{v}& =& A \times_1\mathbf{v}\times_2 \mathbf{u}\in \R \quad \text{produit en mode $1$-mode $2$}.
\end{array}
$$

Observons que sur la dernière multiplication, la contraction se fait sur les deux modes et le résultat est un scalaire. Ainsi, étant donnés deux tenseurs $\mathbf{A} \in \R^{I_1 \times I_2\times \ldots \times I_{d_1}}$ et $\mathbf{B} \in \R^{J_1 \times J_2\times \ldots \times J_{d_2}},$ s'il existe $I_k= J_k,\; 1 \leq k \leq d_1,d_2$ alors le \textit{produit en mode k} est donné par 
\begin{equation}
\label{contractProd}
\mathbf{C} = (\mathbf{A} \times_k \mathbf{B}) = \sum_{j_{_k}=1}^{n^{(k)}} \mathbf{A}_{i_{_1},\ldots,i_{_k},\ldots i_{d_1}}\mathbf{B}_{j_{_1},\ldots ,j_{_{k-1}},j_{_k},j_{_{k+1}} \ldots,j_{_{d_2}}}\equiv \mathbf{A} \times_k B^{(k)}.
\end{equation}

Le produit \eqref{contractProd} est le \textit{produit contracté} général où les directions correspondantes sont contractées et le tenseur résultant $\mathbf{C} $ est d'ordre $ d = d_{1} + d_{2} - 2 $. Le produit multilinéaire complet contracte toutes les directions et s'écrit
$$\mathbf{C} = \langle{\mathbf{A},\mathbf{B}}\rangle = \mathbf{A} \times_1 B^{(1)}\times_2 B^{(2)}\ldots \times_k B^{(k)},$$ le résultat est un scalaire. C'est le produit intérieur tensoriel. Bien entendu, le nombre d'indices dans les directions correspondantes doit être égal.

	
\section{Rang tensoriel}\label{sec:2.3rang}
Cette section se propose de généraliser la notion de rang matriciel présentée via la  \defref{defrang}. Mais, comme on va le montrer; il est généralement difficile de déterminer le rang d'un tenseur d'ordre supérieur. 
\begin{defi}[Tenseur élémentaire]
	\emph{\\}
	On appelle tenseur élémentaire, tout tenseur de $\V$ pouvant
	s’écrire sous la forme $$\vey= \ktensor \mbf{v}^{(k)},\, \mbf{v}^{(k)}\in V^{(k)}.$$ 
	%\cite[p.28]{Nyenyezi2018}
\end{defi}
L’ensemble des tenseurs élémentaires
de $ \V $ est noté $ \cey_{1}(\V) $ et l'on a la relation 
\begin{equation*}
\V=\spam\{\cey_{1}(\V) \}\text{ \cite[p.20]{giraldi1986}}.
\end{equation*}De ce fait résulte une deuxième écriture d’un tenseur de $ \V  $:
\begin{defi}
	\emph{\\}
	Un tenseur de $ \V $ peut s’écrire sous la forme:
	\begin{equation}\label{(7.91)}
	\vey=\sum_{i=1}^{r}\alpha_{i}\ktensor \mbf{v}_{i}^{(k)},\alpha_{i}\in \R,\, \mbf{v}_{i}^{(k)}\in V^{(k)}.
	\end{equation}
	%	et si r est le plus petit entier tel que $\vey$ s’écrit sous cette forme \eqref{(7.91)}, alors r est
	%	appelé rang  de $ \vey $. \cite[p. 29]{projetJustin2020}
\end{defi}
Ici, le tenseur est considéré comme une combinaison linéaire des tenseurs élémentaires.
\begin{defi}[Rang tensoriel canonique]
	\emph{\\}	
	Soit $\mathbf{V}$ un tenseur à $ d $ dimensions. Le plus petit entier tel que le tenseur $\mathbf{V}$ peut être exprimé par \eqref{(7.91)} est appelé rang canonique du tenseur $\mathbf{V}$. En d'autres termes, c'est le plus petit nombre de tenseurs élémentaires utilisés pour représenter le tenseur.
\end{defi}
Pratiquement, il est difficile de déterminer le rang. Même avec l'arithmétique exact, le calcul du rang est, en général, pas faisable pour les tenseurs de grande taille. Voici un résultat soutenant cette position:
\begin{prop}\label{proprank}
	\emph{\\}
	En général, la détermination du rang tensoriel est un problème NP-difficile \cite[p.2]{haastad1990tensor}.
\end{prop}


La définition du rang tensoriel est un analogue exact de la définition du rang matriciel, mais les propriétés des rangs matriciels et tensoriels sont très différentes. Une différence est que le rang d'un tenseur à valeurs réelles peut en fait être différent sur $ \R $ et $ \C $ \cite[p.464]{Hong2008}. De plus, contrairement au rang des matrices, le rang des tenseurs peut dépasser toutes les dimensions. Par exemple \cite[p.4]{comon2014tensors}, 
\begin{ex}\label{exo2.7}
	\emph{\\}
	Considérons le tenseur symétrique $\mbf{G} \in \R^{2\times 2 \times 2}$ défini par son déploiement en mode 1 (voir \eqref{(2.15)}):
	%$$A^{(1)}=
	%\begin{bmatrix}
	%\begin{array}{*{5}{c}}
	%1&0&\vline&1&0\\
	%0&0&\vline&0&0\\
	%\end{array}
	%\end{bmatrix}$$
	%et
	$$
	G^{(1)}=
	\begin{bmatrix}
	\begin{array}{*{5}{c}}
	2&0&\vline&0&-2\\
	0&-2&\vline&-2&0\\
	\end{array}
	\end{bmatrix}.
	$$
	%Dans le champ réel, le tenseur $\mbf{A}$ est de rang $ 1 $ car il peut être écrit $$\mbf{A} = 
	%\begin{pmatrix}
	%1\\
	%0\\
	%\end{pmatrix}
	%\otimes
	%\begin{pmatrix}
	%1\\
	%0\\
	%\end{pmatrix}
	%\otimes
	%\begin{pmatrix}
	%1\\
	%1\\
	%\end{pmatrix}. $$
	Dans le champ réel, le tenseur $\mbf{G}$ nécessite trois tenseurs élémentaires:
	$$
	\mbf{G} = 4
	\begin{pmatrix}
	1\\
	0\\
	\end{pmatrix}
	^{\otimes 3}+
	\begin{pmatrix}
	-1\\
	-1\\
	\end{pmatrix}
	^{\otimes 3}+
	\begin{pmatrix}
	-1\\
	1\\
	\end{pmatrix}^{\otimes 3}.
	$$
	où la notation $ \x^{\otimes k} = \overbrace{\x \otimes \x \ldots \otimes \x}^{k facteurs}$ est le produit tensoriel de k facteurs identiques.
	
	Cependant, si l'on admet la décomposition dans le champ complexe, en considérant $\sqrt{-1} = i,$  on peut écrire $$
	\mbf{G} =
	\begin{pmatrix}
	1\\
	i\\
	\end{pmatrix}
	^{\otimes 3}+
	\begin{pmatrix}
	1\\
	-i\\
	\end{pmatrix}
	^{\otimes 3}.
	$$
	Nous concluons que le tenseur $\mbf{G}$  a un rang canonique $ 3 $ dans $ \R $ et $ 2 $ dans $ \C $.
\end{ex}

A partir de cet exemple, on peut observer que le fait d'imposer la décomposition dans $ \R $ peut augmenter le rang. Cette situation est générale et pour cette raison, de nombreux rangs ont été définis, tels que le rang non négatif, le rang symétrique, le rang de Kruskall, le rang limite, le rang structuré, le rang de Tucker, le rang hiérarchique de Tucker, le rang du train tensoriel ou le rang du noyau tensoriel. Pour plus de détails, voir \cite{comon2014tensors}.  Une autre différence majeure entre le rang d'une matrice et celui d'un tenseur est qu'il n'existe pas d'algorithme direct pour  déterminer le rang d'un tenseur spécifique donné.


Une autre particularité des tenseurs concerne les rangs maximal et typique. Le rang \textit{maximal} est défini comme le plus grand rang possible, tandis que le rang  \textit{typique} est tout rang qui se produit avec une probabilité supérieure à zéro (c'est-à-dire sur un ensemble avec une mesure de Lebesgue positive).

Pour la collection des matrices $ I\times J $, les rang maximal et minimal sont identiques et égaux à $\min\{I,J\}  $. Pour les tenseurs, les deux rangs peuvent être différents. %\cite[p.465]{Hong2008}.

La \tabref{tab:maxrank} montre les rangs maximaux connus pour des tenseurs de tailles spécifiques. Le résultat le plus général concerne les tenseurs d'ordre 3 avec seulement deux tranches. De même, la \tabref{tab:minrank} présente quelques formules connues pour les rangs typiques de certains tenseurs tridimensionnels sur $ \R $ et sur $ \C $ pour certains.

\begin{table}[htbp]
	\centering
	\caption{Rangs maximaux sur $ \R $ pour un tenseur tridimensionnel \cite[p.465]{Hong2008}.}
	\label{tab:maxrank}
	\begin{tabular}{|c|c|}
		\hline
		\hline
		Taille du tenseur&Rang maximal\\
		\hline
		$ I\times J\times 2 $&$ \min\{I,J\}+\min\left\{I,J,\f{1}{2}\max\{I,J\}\right\} $\\
		\hline
		$ 3\times 3\times 3 $&$ 5 $\\
		\hline
		\hline
	\end{tabular}
\end{table}

\begin{table}[htbp]
	\centering
	\caption{Rangs typiques sur $ \R $ et $ \C $ pour un tenseur tridimensionnel \cite[p.465]{Hong2008}.}
	\label{tab:minrank}
	\begin{tabular}{|l|c|c|}
		\hline
		\hline
		Taille du tenseur&Rang typique sur $ \R $&Rang typique sur $ \C $\\
		\hline
		$ 2\times 2\times 2 $&$ \{2,3\} $&\\
		\hline
		$ 3\times 3\times 2 $&$ \{3,4\} $&\\
		\hline
		$ 5\times 3\times 3 $&$ \{5,6\} $&\\
		\hline
		$ I\times J\times 2 $ avec $ I\geq 2J $ (très grand)&$ 2J$&\\
		\hline
		$ I\times J\times 2 $ avec $ I>J $ &$\min\{I,2J\} $&$\min\{I,2J\} $\\
		\hline
		$ I\times J\times 2 $ avec $ J<I<2J $ (grand)&$ I $&\\
		\hline
		$ I\times I\times 2 $ (compact)&$ \{I,I+1\} $&$ I $\\
		\hline
		$ I\times J\times K $ avec $ I\geq JK $ (très grand)&$JK $&\\
		\hline
		$ I\times J\times K $ avec $ JK-J<I<JK $ (grand)&$ I $&\\
		\hline
		$ I\times J\times K $ avec $ I=JK-J $ (compact)&$ \{I,I+1\} $&\\
		\hline
		\hline
	\end{tabular}
\end{table}

En plus du rang tensoriel canonique, nous pouvons définir un rang multilinéaire comme suit.

\begin{defi}[Rang multilinéaire]
	\emph{\\}
	Étant donné un tenseur $\mathbf{A}$ d'ordre $ d $, son rang multilinéaire, noté \textit{ml-rang}($\mathbf{A}$), est le d-uplet $(r_1,r_2,\ldots,r_d) =\left(rang(A^{(1)}),rang(A^{(2)}),\ldots, rang(A^{(d)}) \right), $ où chaque $r_k$ est le nombre de vecteurs colonnes (ou lignes) linéairement indépendants de la matrice obtenue après matricialisation du tenseur en mode-k. C'est-à-dire 	\begin{equation}
	r_k=  rang(A^{(k)}),\;\;   k = 1,2,\ldots,d.
	\end{equation} 
\end{defi}
Le rang multilinéaire intervient surtout dans la décomposition de Tucker qui sera introduite dans la section suivante.
\section{Décompositions tensorielles}
\label{sec:2.4formats}
Les décompositions tensorielles généralisent aux tenseurs le concept de décompositions
matricielles. Lorsque les tenseurs sont de rang faible, elles permettent de les représenter
exactement dans des formats compacts. Ces factorisations permettent donc potentiellement
de résoudre le problème de la complexité de stockage \cite[p.53]{olivier2017decompositions}. L'objectif principal d'une décomposition tensorielle standard est de factoriser un tenseur de données en matrices factorielles physiquement interprétables ou significatives et en un tenseur central unique qui indique les liens entre les composantes (vecteurs des matrices factorielles) dans différents modes \cite[p.7]{cichocki2014era}. Cette section permet d’introduire cinq formats tensoriels dont le format plein et les
quatre formats  les plus couramment utilisés dans l'analyse numérique des tenseurs, à savoir: la décomposition Polyadique Canonique, la décomposition de Tucker, la décomposition  hiérarchique de Tucker et la décomposition en train de tenseurs. Ces formats sont des représentations du tenseur d'une manière qui permet un traitement efficace de ses éléments. 
%Sauf mention explicite du contraire, l'essentiel de cette section s'inspire des ouvrages \cite[pp.53-61]{olivier2017decompositions}, \cite[pp.137-152]{Nyenyezi2018} et \cite[pp.199-350]{hackbusch2012tensor}.
\subsection{Tenseur en format plein}
Soit $ \left(V^{(k)}\right),\, k=1:d $ des espaces vectoriels et $ B^{(k)}=\left(\e_{j_{k}}^{(k)}\right),\,1\leq j_{k}\leq n^{(k)} $ leurs bases respectives. Un tenseur $ \vey $, élément de l'espace tensoriel $ \V $ généré par les espaces vectoriels $ V^{(k)} $, peut-être écrit sous la forme 
\begin{equation}\label{(5.5)}
\vey =\sum_{j_{1}=1}^{n^{(1)}}\cdots\sum_{j_{d}=1}^{n^{(d)}}v_{j_{1}j_{2}\cdots j_{d}}\tensor \e_{j_{k}}^{(k)},\: v_{j_{1}j_{2}\cdots j_{d}}\in \R 
\end{equation}
Le tenseur est dit en format plein lorsqu'il est défini par tous les coefficients 
$$ \left(v_{j_{1}j_{2}\cdots j_{d}}\right)_{, 1\leq j_{k}\leq n^{(k)},\: 1\leq k\leq d} .$$
En d'autres termes, il est représenté comme un tableau à d dimensions avec toutes ses entrées. De la relation \eqref{(5.5)}, une base B de $ \V $ est définie par le produit tensoriel des bases de l'espace vectoriel comme $$ B=B^{(1)}\times B^{(2)}\times\cdots\times B^{(d)} \text{ et } \dim \left(\V\right)=\prod_{k=1}^{d}n^{(k)}. $$
La \figref{Tensor:fulFormat} illustre un tenseur tridimensionnel sous la forme d'un tableau de taille  $3 \times 3 \times 3$ (\ref{sub:(a)}) ou comme des fibres de taille $3\times 3$ (\ref{sub:(b)}), où chaque fibre est un vecteur de  $3$ entrées, ou comme un tas de  $3$ tranches (\ref{sub:(c)}) où chaque tranche est une matrice d'ordre $3 \times 3$ \cite[p.135]{Nyenyezi2018}.
\begin{figure}[!h]
	\encad{
		\centering
		\begin{subfigure}[]{0.3\textwidth}
			\centering
			\begin{tikzpicture}[scale=0.4]
			\node at (1,5,2) {\large{ $27$ entrées}};
			\cuboid{0}{0}{0}{1}{1}{1}{fill=violet!80};
			\cuboid{1}{0}{0}{1}{1}{1}{fill=violet!80};
			\cuboid{2}{0}{0}{1}{1}{1}{fill=violet!80};
			\cuboid{0}{1}{0}{1}{1}{1}{fill=violet!80};
			\cuboid{1}{1}{0}{1}{1}{1}{fill=violet!80};
			\cuboid{2}{1}{0}{1}{1}{1}{fill=violet!80};
			\cuboid{0}{2}{0}{1}{1}{1}{fill=violet!80};
			\cuboid{1}{2}{0}{1}{1}{1}{fill=violet!80};
			\cuboid{2}{2}{0}{1}{1}{1}{fill=violet!80};
			
			\cuboid{0}{0}{1}{1}{1}{1}{fill=violet!80};
			\cuboid{1}{0}{1}{1}{1}{1}{fill=violet!80};
			\cuboid{2}{0}{1}{1}{1}{1}{fill=violet!80};
			\cuboid{0}{1}{1}{1}{1}{1}{fill=violet!80};
			\cuboid{1}{1}{1}{1}{1}{1}{fill=violet!80};
			\cuboid{2}{1}{1}{1}{1}{1}{fill=violet!80};
			\cuboid{0}{2}{1}{1}{1}{1}{fill=violet!80};
			\cuboid{1}{2}{1}{1}{1}{1}{fill=violet!80};
			\cuboid{2}{2}{1}{1}{1}{1}{fill=violet!80};
			
			\cuboid{0}{0}{2}{1}{1}{1}{fill=violet!80};
			\cuboid{1}{0}{2}{1}{1}{1}{fill=violet!80};
			\cuboid{2}{0}{2}{1}{1}{1}{fill=violet!80};
			\cuboid{0}{1}{2}{1}{1}{1}{fill=violet!80};
			\cuboid{1}{1}{2}{1}{1}{1}{fill=violet!80};
			\cuboid{2}{1}{2}{1}{1}{1}{fill=violet!80};
			\cuboid{0}{2}{2}{1}{1}{1}{fill=violet!80};
			\cuboid{1}{2}{2}{1}{1}{1}{fill=violet!80};
			\cuboid{2}{2}{2}{1}{1}{1}{fill=violet!80};
			\end{tikzpicture}
			\caption{}
			\label{sub:(a)}
		\end{subfigure}
		\begin{subfigure}[]{0.3\textwidth}
			\centering
			\begin{tikzpicture}[scale=0.4]
			\node at (1,5,2) {\large{$9$ fibres }};
			\cuboid{0}{0}{0}{1}{3}{1}{fill=violet!80};
			\cuboid{1}{0}{0}{1}{3}{1}{fill=violet!80};
			\cuboid{2}{0}{0}{1}{3}{1}{fill=violet!80};
			\cuboid{0}{0}{1}{1}{3}{1}{fill=violet!80};
			\cuboid{1}{0}{1}{1}{3}{1}{fill=violet!80};
			\cuboid{2}{0}{1}{1}{3}{1}{fill=violet!80};
			\cuboid{0}{0}{2}{1}{3}{1}{fill=violet!80};
			\cuboid{1}{0}{2}{1}{3}{1}{fill=violet!80};
			\cuboid{2}{0}{2}{1}{3}{1}{fill=violet!80};
			\end{tikzpicture}
			\caption{}
			\label{sub:(b)}
		\end{subfigure}
		\begin{subfigure}[]{0.3\textwidth}
			\centering
			\begin{tikzpicture}[scale=0.4]
			\node at (1,5,2) {\large{$3$ tranches}};
			\cuboid{0}{0}{0}{3}{3}{1}{fill=violet!80};
			\cuboid{0}{0}{1}{3}{3}{1}{fill=violet!80};
			\cuboid{0}{0}{2}{3}{3}{1}{fill=violet!80};
			\hspace*{0.4cm}
			\end{tikzpicture}
			\caption{}
			\label{sub:(c)}
		\end{subfigure}
		\emph{\\}
		\vspace{0.2cm}
	}
	\caption{Tenseur tridimensionnel en format plein}
	\label{Tensor:fulFormat}
\end{figure}

\subsection{Décomposition Polyadique Canonique (CPD)}\label{CPDsec}


En 1927, Hitchcock a proposé l'idée de la forme polyadique d'un tenseur, c'est-à-dire l'expression d'un tenseur comme la somme d'un nombre fini de tenseurs de rang un; et en 1944, Cattell a proposé des idées pour l'analyse proportionnelle parallèle et l'idée de multiples axes d'analyse (circonstances, objets et caractéristiques). Le concept est finalement devenu populaire après sa troisième introduction, en 1970, dans la communauté psychométrique, sous la forme de CANDECOMP (décomposition canonique) par Carroll et Chang et de PARAFAC (facteurs parallèles) par Harshman \cite[p.462]{Hong2008}. 

%
%La  décomposition polyadique canonique (CPD) a été redécouverte indépendamment et mise dans un contexte d'application sous les noms de décomposition canonique (CANDECOMP) en psychométrie et de modèle factoriel parallèle (PARAFAC) en linguistique. Elle est aussi appelée \textit{décomposition CP} pour \textit{Canonical decomposition/Parallel factors}.\\
%Avant de nous plonger dans le vif du sujet, rappelons la définition suivante:

\begin{defi}[Format canonique polyadique]
\emph{\\}
Un tenseur est donné sous format canonique, ou format canonique polyadique, lorsqu'il est écrit comme une combinaison linéaire de $ R $ tenseurs élémentaires. C'est-à-dire qu'un tenseur $\mathbf{F} \in \mathcal{V} = \gktensor V^{(k)}$ où $ V^{(k)} = \mathbb{R}^{I_k}$ s'écrit comme suit
\begin{equation}
\label{CPD1}
\mathbf{F}=\sum_{i=1}^{R}g_i{\mathbf{u}_i^{(1)}\otimes{\mathbf{u}_i^{(2)}}
	\ldots{\otimes{\mathbf{u}_i^{(d)}}}},
\end{equation}
où $ R $ est le rang canonique de $\mathbf{F}$ et $\mathbf{u}_i^{(k)},\,i = 1,\ldots,n^{(k)}$ sont des vecteurs de l'espace vectoriel $ V^{(k)}$ de dimension $n^{(k)}$.
\end{defi}
 Puisqu'un produit scalaire est supposé être défini sur l'espace tensoriel considéré, on peut imposer que les vecteurs $\mathbf{u}_i^{(k)},k = 1:d,\;\;i = 1:n^{(k)}$ soient unitaires. Dans ce contexte, les scalaires $g_i,\;\;i = 1:R$ forment un tableau $\mathbf{G}$ à $ d $ dimensions dont la matricialisation en tout mode est diagonale \cite[p.137]{Nyenyezi2018}. Ainsi, la décomposition peut être écrite de manière équivalente sous forme de matrice
\begin{equation}
\label{CPD2}
\mathbf{F}=\mathbf{G}\times_1{U^{(1)}\times_2{U^{(2)}}\ldots{\times_d{U^{(d)}}}},
\end{equation}
où $\mathbf{G}=diag_d(g_1,g_2,...,g_R)$ est le tenseur central qui est d-diagonal. Cela signifie qu'il est diagonal dans un espace à $ d $ dimensions (sa matricialisation dans tout mode est diagonale). Les facteurs $(U^{(k)})_{k=1:d}$ sont des matrices de facteurs construites avec les vecteurs $\mathbf{u}_i^{(k)}$ et la notation $\times_k$ représente le produit en mode k \eqref{modekProd1}. En utilisant le produit de Khatri-Rao et la matricialisation du tenseur $\mathbf{F},$ la relation \eqref{CPD2} s'écrit:
\begin{equation}
F^{(k)}=U^{(k)}{G^{(k)}}(U^{(d)}\odot...\odot{U^{(k+1)}}\odot{U^{(k-1)}}...\odot{U^{(1)}})^T,\; k=1:d,
\end{equation}
où $F^{(k)}$ est la représentation matricielle du tenseur dans le mode k.

La \figref{CP_illustr} illustre la décomposition polyadique canonique d'un tenseur d'ordre 3, où la décomposition est présentée comme une combinaison linéaire de tenseurs de rang un. Le tenseur de base $\mbf{G}$ est une matrice $  d $-diagonale.\\

\begin{figure}[!h]
	\encad{\begin{minipage}{1.0\textwidth}
			\centering
			\begin{subfigure}[a]{1.00\textwidth}
				\label{CP_illustr_sumRk}
				\begin{tikzpicture}[scale=0.4]
				\node at (1,5.5,2) {Décomposition polyadique};
				\cuboid{0}{0}{0}{3}{3}{3}{fill=violet!80};
				\node at (6,1.5,1.5) {$\approx\quad g_1$};
				\node at (10,5.5,2) {$\mathbf{u_1}^{(2)}$};
				\cuboid{9.5}{2.0}{2.0}{3}{0.5}{0.5}{fill=violet!80};
				\node at (12,3.5,2) {$\mathbf{u_1}^{(1)}$};
				\cuboid{9.5}{2.0}{2.0}{0.5}{3}{0.5}{fill=violet!80};
				\node at (11,1.0,3) {$\mathbf{u_1}^{(3)}$};
				\cuboid{9.5}{2.0}{2.0}{0.5}{0.5}{3}{fill=violet!80};
				\node at (15,1.5,1.5) {$+\ldots+g_R$};
				\node at (20,5.5,2) {$\mathbf{u_R}^{(2)}$};
				\cuboid{19.5}{2.0}{2.0}{3}{0.5}{0.5}{fill=violet!80};
				\node at (22,3.5,2) {$\mathbf{u_R}^{(1)}$};
				\cuboid{19.5}{2.0}{2.0}{0.5}{3}{0.5}{fill=violet!80};
				\node at (21,1.5,3) {$\mathbf{u_R}^{(3)}$};
				\cuboid{19.5}{2.0}{2.0}{0.5}{0.5}{3}{fill=violet!80};
				\end{tikzpicture}
				\caption{Illustration du  Format canonique polyadique  d'un tenseur d'ordre $d=3$ en $R$  tenseurs de rang 1}
			\end{subfigure}
			
			\begin{subfigure}[a]{1.00\textwidth}
				\label{CP_illustr_matrix_fmt}
				\begin{tikzpicture}[scale=0.4]
				\node at (1,7,2) {Décomposition polyadique};
				\cuboid{0}{0}{0}{3}{3}{3}{fill=violet!80};
				\node at (6,1.5,1.5) {$\approx$};
				\node at (10,5.5,1) {$U^{(2)}$};
				\cuboid{9.5}{0.0}{2.0}{3}{5}{0}{fill=violet!80};
				\node at (15,7.5,2) {$U^{(3)}$};
				\cuboid{14}{2.0}{2.0}{3}{3}{3}{fill=white!60};
				\node at (19,1,1) {$U^{(1)}$};
				\cuboid{14.5}{6.0}{2.0}{3}{0.0}{-5}{fill=violet!80};
				\node at (14,5,1) {$\mathbf{G}$};
				\cuboid{17.5}{2.0}{2.0}{5}{3}{0}{fill=violet!80};
				\draw [thick,line width = 2](14,5,5) --++(3,-3,-3); 
				\end{tikzpicture}
				\caption{Forme matricielle de la décomposition canonique polyadique}
			\end{subfigure}
		\end{minipage}
	}
	\caption{Décomposition  canonique polyadique en $3D$}
	\label{CP_illustr}
\end{figure}
%
%Cependant, cette décomposition possède deux défauts majeurs: D’une part, pour un tenseur de rang
%R il n’existe pas généralement d’unicité de la décomposition CP. D’autre part pour un tenseur arbitraire,
%le calcul du rang est un problème NP-difficile (cfr \propref{proprank}). 
%Lorsque ce format est utilisé pour
%construire des approximations tensorielles, les algorithmes vont typiquement avoir tendance
%à converger vers des minima locaux. En outre, le rang associé à des approximations de
%tenseurs complexes est en pratique souvent important et peut ne pas permettre de résoudre
%le problème de complexité de stockage \cite[p.59]{olivier2017decompositions}.\\
%
%La CPD s'est déjà imposée comme un outil avancé pour la séparation des signaux dans des branches très diverses du traitement du signal et de l'analyse des données, comme le traitement audio et vocal, l'ingénierie biomédicale, la chimiométrie et l'apprentissage automatique. Un autre exemple est celui des communications sans fil, où les signaux transmis par différents utilisateurs correspondent aux termes de rang-1 dans le cas d'une propagation en visibilité directe \cite[p.150]{cichocki2015tensor}.\\

Ce format présente l'avantage selon lequel le tenseur central est diagonal, mais l'inconvénient est que les facteurs matriciels $(U^{(k)})_{k=1:d}$ ne sont pas nécessairement orthogonaux, et que le rang $ R $ peut être trop grand. On peut imposer une contrainte d'orthogonalité sur les facteurs matriciels alors que le tenseur de base n'est peut-être pas encore diagonal. C'est l'idée du format de Tucker.
\subsection{Décomposition de Tucker}
\label{DefFormatTucker}
La décomposition de Tucker est une généralisation possible de la
décomposition canonique faisant intervenir un tenseur de corps noté $ \tilde{\mathbf{G}}\in\R^{r_{1}\times\cdots\times r_{d}} $ \cite[p.60]{olivier2017decompositions}.
\begin{defi}
\emph{\\}
On dit qu'un tenseur est dans un \textit{format de Tucker} lorsqu'il est écrit comme un produit d'un tenseur de base, de taille plus petite que le tenseur initial, avec des facteurs matriciels orthogonaux.
\end{defi}
 Ainsi, étant donné un tenseur $\mathbf{F} \in \mathcal{V} = \gktensor V^{(k)}$ où $ V^{(k)} = \mathbb{R}^{I_k}$ avec $n^{(k)} = \#I_k$ grand et $r_{k}< n^{(k)},\,k=1:d$, on écrit:\\

\begin{equation}
\label{TD}
\mathbf{F}=\sum_{i_1=1}^{r_1}\sum_{i_2=1}^{r_2}..\ldots \sum_{i_d=1}^{r_d}g_{i_1i_2...i_d}{\mathbf{u}_{i_1}^{(1)}\otimes{\mathbf{u}_{i_2}^{(2)}}\ldots{\otimes{\mathbf{u}_{i_d}^{(d)}}}},
\end{equation}
où $\mathbf{u}_i^{(k)},\,i = 1,\ldots,n^{(k)}$ sont des vecteurs des espaces vectoriels $ V^{(k)}$ de dimension $n^{(k)}$ qui sont, en général, orthonormés. En fixant
 $$\tilde{\mathbf{G}} = \sum_{i_1=1}^{r_1}\sum_{i_2=1}^{r_2}..\ldots \sum_{i_d=1}^{r_d}g_{i_1i_2...i_d}$$ on obtient un tenseur appelé \textit{tenseur central de Tucker ou tenseur noyau ou tenseur de corps} d'ordre $\left(r_1,r_2,\ldots,r_d\right)$ plus petit que le tenseur original. Sous forme de matrice, il s'écrit
\begin{equation}
\label{TD2}
\mathbf{F}=\tilde{\mathbf{G}} \times_1 U^{(1)}\times_2 U^{(2)}\ldots \times_d U^{(d)}.
\end{equation}
La \figref{TuckerFmt} illustre le format de décomposition de Tucker d'un tenseur d'ordre 3, où l'on peut considérer le tenseur central $\tilde{\mathbf{G}}$ comme un format compressé de $\mathbf{F}$  puisque les facteurs matriciels sont orthogonaux.
\begin{figure}[!h]

	\encad{	\centering
		\begin{tikzpicture}[scale=0.4]
		\node at (1,7.5,2) {Décomposition de Tucker};
		\node at (-1,4.5,2) {$\mathbf{F}$};
		\cuboid{0}{0}{0}{4}{4}{4}{fill=violet!80};
		\node at (6,1.5,1.5) {$\approx$};
		\node at (9.5,5.5,2) {$U^{(2)}$};
		\cuboid{9}{0.0}{2.0}{3}{5}{0}{fill=violet!80};
		\node at (19,6.5,0) {$U^{(3)}$};
		\cuboid{13.5}{2.0}{2.0}{2}{2}{2}{fill=violet!80};
		\node at (19,1.5,3) {$U^{(1)}$};
		\cuboid{14.5}{6.0}{2.0}{3}{0.0}{-5}{fill=violet!80};
		\node at (15,5,2) {$\tilde{\mathbf{G}}$};
		\cuboid{18}{2.0}{2.0}{5}{3}{0}{fill=violet!80};
		\end{tikzpicture}
	\emph{\\}\vspace{0.3cm}}
	\caption{Illustration  de la décomposition de Tucker \cite[p.475]{Hong2008}.}
	\label{TuckerFmt}
	
\end{figure}
\emph{\\}
Le format de Tucker est intéressant dans la mesure où les facteurs matriciels sont souvent choisis orthogonaux. L'avantage étant que plusieurs opérations seront limitées au seul tenseur de base qui est alors une forme compressée du tenseur initial. Plus la compression est importante ($r_k << n^{(k)}, k=1:d$) sans perte significative, plus le format de Tucker est efficace.

Ce format est stable mais, il présente l'inconvénient d'avoir toujours un tenseur de base qui est multidimensionnel et non diagonal. Ainsi, la complexité algorithmique croît toujours de manière exponentielle par rapport à la dimension $ d $; $\op(dnr+r^{d})$\cite[p.2296]{oseledets2011tensor}.
Il convient pour les "petites" dimensions, en particulier pour les modèles tridimensionnels. Par conséquent, l'utilisation du format hiérarchique de Tucker ou du train tensoriel (TT) qui tentent de combiner les avantages des représentations CP et Tucker est nécessaire. 


%
%Le format de Tucker se réduit au format CP lorsque le tenseur de corps est diagonal,
%i.e :
%C(k1, . . . , kd) =
%8>>><
%>>>:
%1 si k1 = · · · = kd
%0 sinon
%Le diagramme tensoriel d’une décomposition de Tucker d’un tenseur d’ordre 5 est donné
%en figure III.8. Notons que le format CP et le format de Tucker d’un tenseur de même ordre
%i1
%i5
%i3 i4
%i2
%Figure III.8 – Diagramme tensoriel de la décomposition de Tucker.
%sont représentés par des diagrammes tensoriels identiques. La principale difficulté posée
%par ce tenseur est qu’il reste soumis à la malédiction de la dimension. En effet, le tenseur
%de corps C est de dimension d par conséquent même lorsque d est faible la complexité de
%stockage peut être très grande.


\subsection{Décomposition hiérarchique de Tucker}
\label{subsec:tens_Ht-TT}
L'idée principale de la représentation hiérarchique de Tucker est la manière d'approximer un tenseur numérique
$$\mbf{A}\in \mathcal{V}=\gktensor V^{(k)}$$
par un autre tenseur
\begin{equation}
\tilde{\mathbf{A}} \in \tilde{\mathcal{V}}=\gktensor U^{(k)},
\end{equation}
construit via des sous-espaces vectoriels $U^{(k)}$ de rang minimal pour chaque $V^{(k)}$.

 Pour rendre cette approximation efficace, un arbre dimensionnel est utilisé. Cet arbre permet de réduire la dimensionnalité du tenseur d'ordre élevé en utilisant la décomposition en valeurs singulières d'ordre élevé (HOSVD) et la troncature \cite[p.140]{Nyenyezi2018}.
 
L'arbre binaire dimensionnel $ T $ est le plus utilisé. On considère que l'ensemble de dimension $D=\lbrace{1,...,d}\rbrace$ est la racine de l'arbre, il est au niveau 0. Chaque nœud de $ T $ est un sous-ensemble non vide de $ D $. Tout nœud $ t $ de $ T $ tel que $ \# t = 1 $ est appelé \textit{feuille} de $ T $ et l'ensemble de toutes les feuilles est noté  $\mathit{L(T)}.$ La notation $\mathit{I(T)}=T \setminus{\mathit{L(T)}}$ représente l'ensemble des nœuds $ t $ tels qu'il existe deux enfants $t_1$ et $t_2$, chacun non vide avec $t=t_1 \cup t_2$ et $t_1 \cap t_2=\emptyset$ \cite[p.319]{hackbusch2012tensor}. A chaque nœud $t \in T$ on définit un rang maximal $ r $ tel qu'à chaque niveau $ l $, le $ t $-rang $r_t$ est plus petit que $ r $. Le $ t $-rang    $r_t$ est défini dans \eqref{matricisation1}. Si un nœud $ t $ est au niveau $ l $, ses enfants $\lbrace{t_1,t_2}\rbrace$ sont au niveau $l+1.$\\

L'ensemble de tous les tenseurs hiérarchiques de Tucker est donné par
\begin{equation}
\mathcal{H}_r^T(\mathcal{V})=\lbrace{\mathbf{A}\in \mathcal{V}:r_t(\mathbf{A})\leq r,\quad \forall t\in T}\rbrace.
\end{equation}

Tout élément de $\mathcal{H}_r^T(\mathcal{V})$ peut être écrit sous la forme
\begin{equation}
\mathbf{A}=\sum_{i=1}^{r_D}{\sum_{j=1}^{r_D}\alpha_{ij}^{D}\mathbf{u}_i^{D_1}\otimes {\mathbf{u}_j^{D_2}}}, \; D=D_1\cup{D_2},\; D_1\cap{D_2}=\emptyset, \; \alpha_{ij}^{D} \in \mathbb{R},
\end{equation}
et
\begin{equation}
\mathbf{u}_k^{(t)}=\sum_{i=1}^{r_t}{\sum_{j=1}^{r_t}\alpha_{ijk}^{(t)}\mathbf{u}_i^{(t_1)}\otimes{\mathbf{u}_j^{(t_2)}}}, \; t=t_1\cup{t_2},\; t_1\cap{t_2}=\emptyset, \; \alpha_{ijk}^{(t)} \in \mathbb{R},\; \forall t\in \mathit{I(T)},
\end{equation}

où $\mathit{I(T)}=T\setminus{D}$. Un tenseur est déterminé au format de Tucker hiérarchique en définissant le tenseur de transfert $(\alpha^{(t)})_{t\in \mathit{I(T)}}$ et sa base  de l'espace vectoriel $(\mathbf{u}_i^{(t)})_{t\in \mathit{L(T)},i\in \lbrace{1,...,r_t}\rbrace}$.


Une façon de déterminer les bases
\begin{equation}
\label{basesHOSVD}
B^{(t)} = \left[\mathbf{u}_i^{(t)}\right] \in U^{(t)}, i = 1,2,\ldots, r_t
\end{equation}
et les tenseurs de transfert $\alpha_{ijk}^{(t)}$ font référence à la décomposition en valeurs singulières d'ordre supérieur que nous présentons ci-dessous.
\subsubsection*{Décomposition en valeurs singulières d'ordre élevé (HOSVD)}
%Généralisant la décomposition matricielle de la valeur singulière (SVD), le cheval de bataille de l'algèbre linéaire numérique, la MLSVD (pour \textit{MultiLinear Singular Value Decomposition} en anglais ou Décomposition Multilinéaire à Valeurs Singulières en français) a stimulé l'intérêt pour les tenseurs en mathématiques appliquées et en calcul scientifique en très haute dimension. Parallèlement, la MLSVD a été adoptée avec succès comme outil pour le traitement des réseaux de capteurs et la séparation déterministe des signaux dans les communications sans fil \cite[p.146]{cichocki2015tensor}. 
%

L'application de matricialisation \eqref{matricisation1} nous permet de définir une matrice qui correspond au tenseur $\mathbf{A}$ à chaque nœud de l'arbre dans le mode $ t $, notée $M_t(\mathbf{A})$ ( où $ t $ est un sous-ensemble de l'ensemble de dimension $ D $). En considérant les vecteurs singuliers gauches $\mbf{u}_i^{(t)}$ de $M_t(\mathbf{A}),$ on forme une base orthonormale
$$B^{(t)} = [\mbf{u}_1^{(t)},\mbf{u}_2^{(t)},\ldots,\mbf{u}_{r_t}^{(t)}]$$
de telle sorte que $$U^{(t)} = \spam\{\mbf{u}_i^{(t)}, 1 \leq i \leq r_t\}$$ pour tout $t \in T$.

 De telles bases à chaque sommet de l'arbre forment les bases dites HOSVD (voir ~\cite[p.339]{hackbusch2012tensor}). Cependant, si l'on devait stocker toutes les entrées de ces bases, cela nécessiterait un stockage énorme. Au lieu de cela, la représentation hiérarchique utilise quelques cadres récursifs qui permettent d'exprimer les bases 
 au sommet $t \in T$ au moyen de ses bases fils $B^{(t_1)}$ et $B^{(t_2)}$. Dans ce cas, seules les bases des nœuds feuilles ($t\in L(T)$ est un singleton) doivent être stockées explicitement alors que les bases des niveaux supérieurs sont implicitement formées via le produit de Kronecker et les tenseurs dits de transfert. Les coefficients $\alpha_{ijk}^{(t)}$ peuvent être construits par les valeurs singulières gauches de la matrice $M_t(\mathbf{A})$ ou par les valeurs propres de la matrice $$M_t(\mathbf{A})M_t(\mathbf{A})^T$$
 avec quelques troncatures récursives. La procédure est répétée pour chaque sous-ensemble $t \subset D$. Cette procédure fait partie de la décomposition en valeurs singulières d'ordre élevé HOSVD qui est une généralisation de la SVD matricielle. Pour plus de détails, voir  \cite[Chap.11]{hackbusch2012tensor} pour cette construction, tandis que d'autres HOSVD dans d'autres formats sont présentées, par exemple dans \cite{de2000multilinear}.\\
 La \figref{tens_d4} illustre un arbre tensoriel hiérarchique: \figref{dimTree} d'un tenseur $\mbf{A}$ à 7 dimensions et sa représentation en format hiérarchique, \figref{tens_d3} suivant les concepts de la section \ref{sec:2.1visual}.
 
  Le premier nœud au niveau 0 est la racine de l'arbre avec $D = \{1,\ldots,7\}$ et chaque niveau suivant est formé par deux sous-ensembles disjoints du précédent. Dans la \figref{tens_d3}, le $\bullet$ indique un produit contracté entre deux tenseurs et le nombre de segments barrés indique la dimension du tenseur résultant. On peut voir que (en observant la \figref{tens_d3}), à la racine du tenseur, le tenseur résultant est une matrice (à deux dimensions) tandis que chaque nœud non feuille et non racine représente un tenseur à trois dimensions. Les feuilles représentent les sous-espaces unidimensionnels $U^{(k)}, \;k =1:d$ à partir desquels sont construites les bases unidimensionnelles $B^{(k)}$ définies dans \eqref{basesHOSVD}.

\begin{figure}[!h]
	\encad{\centering\begin{subfigure}[a]{0.99\textwidth}
				\centering
			\begin{tikzpicture}[scale=0.5]
			\node[draw](A) at (0,-2) {$\lbrace{1,2,3,4,5,6,7}\rbrace$};
			\node[draw](B) at (-6,-4) {$\lbrace{1,2,3}\rbrace$};
			\node[draw](C) at (8,-4) {$\lbrace{4,5,6,7}\rbrace$};
			\node[draw](D) at (-10,-6) {$\lbrace{1}\rbrace$};
			\node[draw](P) at (-4,-6) {$\lbrace{2,3}\rbrace$};
			\node[draw](F) at (4,-6) {$\lbrace{4,5}\rbrace$};
			\node[draw](G) at (10,-6) {$\lbrace{6,7}\rbrace$};
			\node[draw](H) at (-6,-8) {$\lbrace{2}\rbrace$};
			\node[draw](U) at (-2,-8) {$\lbrace{3}\rbrace$};
			\node[draw](I) at (2,-8) {$\lbrace{4}\rbrace$};
			\node[draw](J) at (6,-8) {$\lbrace{5}\rbrace$};
			\node[draw](K) at (8,-8) {$\lbrace{6}\rbrace$};
			\node[draw](L) at (12,-8) {$\lbrace{7}\rbrace$};
			\draw[->,>=latex,](A)--(B);
			\draw[->,>=latex,](A)--(C);
			\draw[->,>=latex,](B)--(D);
			\draw[->,>=latex,](B)--(P);
			\draw[->,>=latex,](C)--(F);
			\draw[->,>=latex,](C)--(G);
			\draw[->,>=latex,](P)--(H);
			\draw[->,>=latex,](P)--(U);
			\draw[->,>=latex,](F)--(J);
			\draw[->,>=latex,](F)--(I);
			\draw[->,>=latex,](G)--(K);
			\draw[->,>=latex,](G)--(L);
			\end{tikzpicture}
		\caption{Arbre dimensionnel: $\mathbf{A}\; (d=7)$.}
		\label{dimTree}
	\end{subfigure}
	
	\begin{subfigure}[a]{0.99\textwidth}
		\centering
			\begin{tikzpicture}[scale=0.6]
			\node[scale = 2]  (0,0) {\cdb{$\bullet$}};
			\draw (0,0) -- (-3,-2) node [midway,scale=1.2] {$/$};
			\draw (0,0) -- (3,-2)  node [midway,scale=1.2] {$/$};
			\draw node[scale=2] at (-3,-2){\cdb{$\bullet$}};
			\draw (-3,-2) -- (-4,-4) node [midway,scale=1.2] {$|$};
			\draw (-3,-2) -- (-1,-4)  node [midway,scale=1.2] {$/$};
			\draw node[scale=2] at (3,-2)  {\cdb{$\bullet$}};
			\draw (3,-2) -- (2,-4) node [midway,scale=1.2] {$/$};
			\draw (3,-2) -- (5,-4)  node [midway,scale=1.2] {$/$};
			\draw node[scale=2] at (5,-4)  {\cdb{$\bullet$}};
			\draw (-4,-4) -- (-5,-6) node [midway,scale=1.2] {$|$};
			\draw node[scale=2] at (2,-4)  {\cdb{$\bullet$}};
			\draw (-1,-4) -- (-3,-6) node [midway,scale=1.2] {$|$};
			\draw (-1,-4) -- (0,-6)  node [midway,scale=1.2] {$/$};
			\draw node[scale=2] at (-1,-4)  {\cdb{$\bullet$}};
			\draw (2,-4) -- (1,-6) node [midway,scale=1.2] {$|$};
			\draw (2,-4) -- (3,-6)  node [midway,scale=1.2] {$/$};
			\draw node[scale=2] at (-4,-4)  {\cdb{$\bullet$}};
			\draw (5,-4) -- (4,-6) node [midway,scale=1.2] {$|$};
			\draw (5,-4) -- (6,-6)  node [midway,scale=1.2] {$/$};
			\node[](A) at (-5,-6.5) {$U^{(1)}$};
			\node[](B) at (-3,-6.5) {$U^{(2)}$};
			\node[](C) at (0,-6.5) {$U^{(3)}$};
			\node[](D) at (1.1,-6.5) {$U^{(4)}$};
			\node[](E) at (3,-6.5) {$U^{(5)}$};
			\node[](F) at (4.1,-6.5) {$U^{(6)}$};
			\node[](G) at (6,-6.5) {$U^{(7)}$};
			\end{tikzpicture}
		
		\caption{Décomposition tensorielle hiérarchique $\mathbf{A}\; (d=7)$}
		\label{tens_d3}
	\end{subfigure}\emph{\\}\vspace{0.2cm}}
	\caption{Arbre hiérarchique de compression ($d = 7$)\cite[p.141]{Nyenyezi2018}.}
	\label{tens_d4}
\end{figure}

%\subsubsection*{Tucker-rank generalization of SVD }
%%Another example, the sum of two tensors may produce a tensor whose rank is sometimes greater than the sum of the ranks of the two components \cite{Hack2012}.
%Consider the index set $D = \{1,2,\ldots,d\}$ and $t,t^c \subset D$ such that $D=t \cup t^c$ and $t \cap t^c=\emptyset.$
%Recall that, as we stated in Section~\ref{TensorOp} ~\eqref{matricisation1} the  $t$-rank of $\mathbf{A}$ is defined by the rank of the matrix  $A^{(t)}$ whose rows are determined by indices in $t$ while  columns are determined by the indices in $t^c.$ That is $(\mathbf{A}^{(t)} \in \mathbb{R}^{I_{t},I_{t^c}},\; \text{where}\; I_t=\times_{j\in{t}}I_j,\quad I_{t^c}=\times_{j\in{t^c}}I_j)$.
%
%The Hierarchical Tucker rank is defined by the $d$-uplet $(r_1,...,r_d)$ where the $r_t\; \; 1 \leq k \leq d$ is the matrix rank of the tensor at node $t \in D$ obtained by matricizing $(\mathbf{A})$ in mode $k$ at this node.







%\subsubsection*{Troncature}

\subsection{Décomposition en train tensoriel et approximation de faible rang}
\label{sec:tens_Ht-TT}
Parmi les multiples formats de décompositions tensorielles existants,  généralement le choix 
d’étudier  particulièrement le format en train de tenseurs est fréquent. Ce choix est guidé par des résultats pratiques
mettant en évidence son efficacité en termes de compression de données pour représenter des tenseurs de grande dimension \cite[p.61]{olivier2017decompositions}.
\subsubsection{Décomposition en train des tenseurs (TT)}

%Ce format permet d’une
%part une «compression» importante de l’information pour des tenseurs de référence de
%faibles rangs et d’autre part un accès quasi temps réel à l’ensemble de ses éléments.
%

Les principales motivations pour l'utilisation du format TT sont à la fois la possibilité de représenter exactement un tenseur d'ordre élevé avec peu de paramètres ou la possibilité de l'approximer par un tenseur de bas rang avec une précision donnée par moins de paramètres. En outre, des algorithmes efficaces ont été étudiés pour calculer les rangs optimaux $r_k$ permettant à un tenseur d'ordre élevé donné d'être représenté dans le format TT avec une précision requise. Par ailleurs, le remodelage des matrices et des vecteurs à grande échelle en tenseurs d'ordre élevé et leur approximation dans le format TT permettent d'effectuer certaines opérations algébriques, telles que l'addition et le produit matrice-vecteur, avec une complexité temporelle logarithmique \cite[p.142]{Nyenyezi2018}.

La représentation TT d'un vecteur ou d'une matrice tensorisé(e) $ \mbf{A} $ peut être considérée comme une représentation à faible rang effectuée successivement sur ses matricialisations $A^{(k)}, k = 1:d.$ Ces matricialisations conduisent souvent à une dépendance linéaire entre les lignes ou les colonnes qui peut être supprimée par une transformation standard des lignes et des colonnes. Ainsi, la décomposition TT des grands vecteurs et des matrices à grande échelle utilise une représentation récursive du produit de Kronecker avec utilisation de bases communes.

Pour représenter un tenseur $\mbf{A} \in \RItd$ au format TT, il faut déterminer $ l $ composantes\footnote{La nouvelle dimension $ l $ est utilisée comme une dimension artificielle et doit être distinguée de la dimension réelle $ d $. Lorsque $ l = d $, alors la dimension réelle du problème est utilisée.} 
($l \geq d$)  $U^{(k)},\; k = 1,...,l$ telles que 
\begin{equation}
\label{TT:form1}
\mbf{A}(i_1,i_2,\ldots,i_l) \approx \sum_{\alpha_0,\alpha_1,\ldots,\alpha_l} \mbf{U}^{^{(1)}}(\alpha_{_0},i_{_1},\alpha_{_1})  \mbf{U}^{^{(2)}}(\alpha_{_1},i_{_2},\alpha_{_2})  \cdots  \mbf{U}^{^{(l)}}(\alpha_{_{l-1}},i_{_l},\alpha_{_l}) ,
\end{equation}
où les composantes $\mbf{U}^{(k)}, \; 1 \leq k \leq l$ sont appelées tenseurs noyaux. En général, chaque $\mbf{U}^{(k)}$ est un tableau à 3 dimensions pour les grands vecteurs. Par exemple, considérons un vecteur
$$\mbf{x} \in \R^{n^{(1)}.n^{(2)}\cdots .n^{(d)}}.$$
Ce vecteur peut être remodelé dans un tableau $\mbf{x} \in \RItd$, où $ \#I_k =  n^{(k)},\; \forall k \in \lbrace{1,2,\ldots,l}\rbrace.$  Pour convertir ce vecteur sous la forme \eqref{TT:form1}, chaque noyau va dépendre d'un indice initial $i_k \in I_k$ et de deux variables d'indices auxiliaires $$\alpha_{k-1} \in K_{k-1}, \alpha_k \in K_k, \forall\; k \in K_k = \lbrace{1,2,\ldots, r_k}\rbrace, k = 1:l.$$  Les nombres $r_k$ sont appelés le rang-TT du vecteur et $r_1 = r_l = 1$ par définition. Nous pouvons nous référer à ce vecteur comme un vecteur-TT et en utilisant le produit de Kronecker des fibres du mode 2 (colonnes), nous pouvons écrire
$$
\mbf{x} \approx \sum_{\alpha_1= 1}^{r_1} \sum_{\alpha_2= 1}^{r_2} \ldots \sum_{\alpha_{l-1}= 1}^{r_{l-1}} 
= \mbf{U}^{^{(1)}}(1,i_{_1},\alpha_{_1})\otimes \mbf{U}^{^{(2)}}(\alpha_{_1},i_{_2},\alpha_{_2})\otimes \ldots \otimes  \mbf{U}^{^{(l)}}(\alpha_{_{l-1}},i_{_l},1).$$
\vspace*{0.2cm}
Pour une matrice à grande échelle $A \in \R^{m\times n}$, la représentation TT est obtenue de la même manière par extension de \eqref{TT:form1} où chaque noyau est maintenant un tableau à 4 dimensions. La matrice $ A $ peut être remodelée comme un tableau
$$\mbf{A} \in \R^{I_1\times I_2 \times \ldots \times I_{d_1} \times J_1 \times J_2 \times \ldots \times J_{d_2}},\;\; d_1, d_2 \in \N. $$
appelée matrice-TT, où $$ m = \prod_{k = 1}^{d_1} \#I_k$$ et $$ n = \prod_{k = 1}^{d_2} \#J_k.$$

Ainsi, on peut approximer sa représentation TT en utilisant le produit de Kronecker des tranches (matrices) mode-2 et écrire
$$\mbf{A}\approx\sum_{\alpha_1= 1}^{r_1} \sum_{\alpha_2= 1}^{r_2} \ldots \sum_{\alpha_{l-1}= 1}^{r_{l-1}} \mbf{U}^{^{(1)}}(1,i_{_1},j_{_1},\alpha_{_1})\otimes \mbf{U}^{^{(2)}}(\alpha_{_1},i_{_2},j_{_2},\alpha_{_2})\otimes \ldots \otimes  \mbf{U}^{^{(l)}}(\alpha_{_{l-1}},i_{_l},j_{_l},1).$$


La \figref{TT-formatFig} visualise le format TT où, pour les vecteurs TT, les conditions aux limites donnent lieu à des matrices aux extrêmes et à des tenseurs d'ordre 3 partout ailleurs, liés par le produit contracté mode-1. La forme d'un train est représentée par la \figref{TT-Boxes} tandis que la forme équivalente mais plus générale est représentée par la \figref{TT-order-doted}. La \figref{TT-order-doted2} visualise la représentation TT des grandes matrices où nous avons des tableaux à 3 dimensions aux extrémités et des tableaux à 4 dimensions partout ailleurs.

\begin{figure}[!h]
	\encad{	\centering
		\begin{subfigure} [a]{0.99\textwidth}
			\centering
	        \begin{tikzpicture}[scale=0.2]
			\cuboid{-24}{-1}{0}{4}{3}{0}{fill=violet!80};
			\cuboid{-16}{0}{0}{3}{3}{3}{fill=violet!80};
			\cuboid{-8}{0}{0}{3}{3}{3}{fill=violet!80};
			\cuboid{0}{0}{0}{3}{3}{3}{fill=violet!80};
			\cuboid{8}{0}{0}{3}{3}{3}{fill=violet!80};
			\cuboid{16}{0}{0}{3}{3}{3}{fill=violet!80};
			\cuboid{24}{-1}{0}{3}{4}{0}{fill=violet!80};
			\draw [thick,line width = 1pt](-20,1,0) --(-17.1,1,0);
			\draw [thick,line width = 1pt](-14.1,1,0) --(-9.1,1,0);
			\draw [thick,line width = 1pt](-6.1,1,0) --(-1.1,1,0);
			\draw [thick,line width = 1pt](2.1,1,0) --(7.1,1,0);
			\draw [thick,line width = 1pt](10.1,1,0) --(15.1,1,0);
			\draw [thick,line width = 1pt](18.1,1,0) --(24.1,1,0);
			\end{tikzpicture}
		\caption{Format en train de tenseur d'un vecteur TT d'ordre $7$}
		\label{TT-Boxes}
	\end{subfigure}
	\begin{subfigure}[a]{0.99\textwidth}
		\centering
			\begin{tikzpicture}[scale=1.0]
			\draw node[scale = 2,color = violet!80] at (0,1) {$\bullet$};
			\draw [line width = 1](0,1) -- (-1,1) node [midway,scale=1.5] {$/$};
			\draw node[scale = 2, color = violet!80] at (1,1) {$\bullet$};
			\draw [line width = 1](1,1) -- (0,1) node [midway,scale=1.5] {$/$};
			\draw [line width = 1](1,1) -- (1,0)  node [midway,scale=1.5] {$/$};
			\draw [line width = 1](1,1) -- (2,1)  node [midway,scale=1.5] {$/$};
			\draw node[scale=2,color = violet!80] at (2,1)  {$\bullet$};
			\draw [line width = 1](2,1) -- (2,0)  node [midway,scale=1.5] {$/$};
			\draw [line width = 1](2,1) -- (3,1)  node [midway,scale=1.5] {$/$};
			\draw node[scale=2,color = violet!80] at (3,1)  {$\bullet$};
			\draw [line width = 1](3,1) -- (3,0)  node [midway,scale=1.5] {$/$};
			\draw [line width = 1](3,1) -- (4,1)  node [midway,scale=1.5] {$/$};
			\draw node[scale=2,color = violet!80] at (4,1)  {$\bullet$};
			\draw [line width = 1](4,1) -- (4,0)  node [midway,scale=1.5] {$/$};
			\draw [line width = 1](4,1) -- (5,1)  node [midway,scale=1.5] {$/$};
			\draw node[scale=2,color = violet!80] at (5,1)  {$\bullet$};
			\draw [line width = 1](5,1) -- (5,0)  node [midway,scale=1.5] {$/$};
			\draw [line width = 1](5,1) -- (6,1)  node [midway,scale=1.5] {$/$};
			\draw node[scale=2,color = violet!80] at (6,1)  {$\bullet$};
			\draw [line width = 1](6,1) -- (6,0)  node [midway,scale=1.5] {$/$};
			\draw [line width = 1](6,1) -- (7,1)  node [midway,scale=1.5] {$/$};
			\draw node[scale=2,color = violet!80] at (7,1)  {$\bullet$};
			\draw [line width = 1](7,1) -- (8,1)  node [midway,scale=1.5] {$/$};
			\end{tikzpicture}
		\caption{Format tensoriel TT d'un vecteur TT d'ordre  $7$}
		\label{TT-order-doted}
	\end{subfigure}
	
	\begin{subfigure}[a]{0.99\textwidth}
		\centering
			\begin{tikzpicture}[scale=1.0]
			\draw node[scale = 2,color = violet!80] at (0,1) {$\bullet$};
			\draw [line width = 1](0,1) -- (-1,1) node [midway,scale=1.5] {$/$};
			\draw [line width = 1](0,1) -- (0,0) node [midway,scale=1.5] {$/$};
			\draw node[scale = 2,color = violet!80] at (1,1) {$\bullet$};
			\draw [line width = 1](1,1) -- (0,1) node [midway,scale=1.5] {$/$};
			\draw [line width = 1](1,1) -- (1,0)  node [midway,scale=1.5] {$/$};
			\draw [line width = 1](1,1) -- (2,1)  node [midway,scale=1.5] {$/$};
			\draw [line width = 1](1,1) -- (1,2)  node [midway,scale=1.5] {$/$};
			\draw node[scale=2,color = violet!80] at (2,1)  {$\bullet$};
			\draw [line width = 1](2,1) -- (2,0)  node [midway,scale=1.5] {$/$};
			\draw [line width = 1](2,1) -- (3,1)  node [midway,scale=1.5] {$/$};
			\draw [line width = 1](2,1) -- (2,2)  node [midway,scale=1.5] {$/$};
			\draw node[scale=2,color = violet!80] at (3,1)  {$\bullet$};
			\draw [line width = 1](3,1) -- (3,0)  node [midway,scale=1.5] {$/$};
			\draw [line width = 1](3,1) -- (4,1)  node [midway,scale=1.5] {$/$};
			\draw [line width = 1](3,1) -- (3,2)  node [midway,scale=1.5] {$/$};
			\draw node[scale=2,color = violet!80] at (4,1)  {$\bullet$};
			\draw [line width = 1](4,1) -- (4,0)  node [midway,scale=1.5] {$/$};
			\draw [line width = 1](4,1) -- (5,1)  node [midway,scale=1.5] {$/$};
			\draw [line width = 1](4,1) -- (4,2)  node [midway,scale=1.5] {$/$};
			\draw node[scale=2,color = violet!80] at (5,1)  {$\bullet$};
			\draw [line width = 1](5,1) -- (5,0)  node [midway,scale=1.5] {$/$};
			\draw [line width = 1](5,1) -- (6,1)  node [midway,scale=1.5] {$/$};
			\draw [line width = 1](5,1) -- (5,2)  node [midway,scale=1.5] {$/$};
			\draw node[scale=2,color = violet!80] at (6,1)  {$\bullet$};
			\draw [line width = 1](6,1) -- (6,0)  node [midway,scale=1.5] {$/$};
			\draw [line width = 1](6,1) -- (7,1)  node [midway,scale=1.5] {$/$};
			\draw [line width = 1](6,1) -- (6,2)  node [midway,scale=1.5] {$/$};
			\draw node[scale=2,color = violet!80] at (7,1)  {$\bullet$};
			\draw [line width = 1](7,1) -- (8,1)  node [midway,scale=1.5] {$/$};
			\draw [line width = 1](7,1) -- (7,0)  node [midway,scale=1.5] {$/$};
			\end{tikzpicture}
		
		\caption{Format en train des tenseurs d'une TT matrice d'ordre $ 7 $ représentée.}
		\label{TT-order-doted2}
	\end{subfigure}\emph{\\}\vspace{0.2cm}
}
	\caption{Forme d'un train tensoriel d'ordre 7 \cite[p.164]{Nyenyezi2018}.}
	\label{TT-formatFig}
\end{figure}


Montrons maintenant comment ces noyaux sont construits. Tout d'abord, rappelons que lorsque la séparabilité de certaines variables est possible, toute matrice $A \in \R^{m \times n}$ peut être écrite comme une somme de produit de Kronecker de certains vecteurs:
$$A = \sum_{j = 1}^r \u_j \otimes \v_j = \sum_{j = 1}^r\u_j\v_j^T,$$ 

où $ r $ est le rang de la matrice $ A $. Cela permet d'écrire une matrice $ A $ sous la forme de décomposition dyadique (skeleton; voir \cite[p.49]{olivier2017decompositions}): $A = UV^T.$ En d'autres termes, la matrice $ A $ dont les entrées peuvent être notées $A(i,j),\; i = 1:m,\; j=1:n,$ peut être considérée comme un produit de Kronecker de deux
matrices $G^{(1)}\in \R^{m^{(1)} \times n^{(1)} }$ et  $G^{(2)}\in \R^{m^{(2)} \times n^{(2)} }$  de telle sorte que $(m,n) = (m^{(1)}.m^{(2)},n^{(1)}.n^{(2)}).$ Cela signifie que nous pouvons écrire
\begin{equation}
A =  G^{(1)} \otimes G^{(2)} = [G^{(1)}(i_1,j_1)G^{(2)}]\;\; i_1 = 1:m^{(1)},\;j_1 = 1:n^{(1)}.
\end{equation}
Ainsi, la décomposition repose sur un certain fractionnement des indices spatiaux et peut être effectuée de manière récursive. Pour comprendre ce fractionnement, considérons les indices de ligne et de colonne de $ A $ comme des multi-indices:
$$(i,j) = (i_1i_2,j_1j_2),\;\;\text{and}\;\;A(i,j) = A(i_1i_2,j_1j_2). $$


On peut observer que les indices $ i $ et $ j $ du produit de Kronecker $A = G^{(1)}\otimes G^{(2)}$ ne sont pas séparés mais sont des produits d'indices de mode. L'idée de la séparation des variables est de remodeler la matrice A en une nouvelle matrice $\tilde{A}$ avec des multi-indices spéciaux qui admettent la séparation. Cela signifie que nous devons remodeler la matrice A sous la forme
\begin{equation}
\tilde{A}(i_1j_1,i_2j_2)\approx A(i_1i_2,j_1j_2),\, i_1 = 1:m^{(1)},i_2 = 1:m^{(2)},j_1 = 1:n^{(1)},j_2 = 1:n^{(2)},
\end{equation}
tel que $\tilde{A}(i_1j_1,i_2j_2)$ est une matrice de faible rang. Puisque la plupart des matrices ne sont pas représentées par un seul produit de Kronecker, nous pouvons avoir une somme de produits de Kronecker qui écrit
\begin{equation}
\label{TTKronSum}
\tilde{A} = \sum_{\alpha = 1}^rG_{\alpha}^{(1)} \otimes G_{\alpha}^{(2)},
\end{equation}
où $G_{\alpha}^{(k)} $ est une matrice. L'entier naturel $ r $ est connu comme le rang de Kronecker de la représentation alors qu'il est appelé rang de Kronecker de la matrice $\tilde{A}$ lorsqu'il est le minimum possible pour toutes les représentations de la forme \eqref{TTKronSum}. Le côté droit de l'équation \eqref{TTKronSum} permet d'écrire la matrice A avec
 $$r(m^{(1)}n^{(1)}+m^{(2)}n^{(2)})$$
 entrées qui peuvent être significativement plus petites que $$mn = m^{(1)}n^{(1)}m^{(2)}n^{(2)},$$
 pour un petit $ r $. Ainsi, le nombre de paramètres pour représenter la matrice $ A $ a été réduit. Afin de comprimer davantage la représentation, nous pouvons augmenter le nombre de facteurs de Kronecker tout en diminuant la taille de chaque facteur de matrice. C'est-à-dire
\begin{equation}
\label{TTKronSum2}
\tilde{A} = \sum_{\alpha = 1}^r \otimes_{k = 1}^l G_{\alpha}^{(k)},
\end{equation}
où $G_{\alpha}^{(k)}$ est une matrice dépendant des paramètres. Avec cette formulation, la taille de chaque $G_{\alpha}^{(k)}$ est diminuée mais $ l $,  $ l\geq d $, facteurs sont utilisés. Cela signifie que nous avons agrandi la dimension $ d $ tout en créant de petits blocs qui forment le tenseur central. Ce tenseur central devrait avoir un petit rang $ r $. Considérons, pour simplifier, que la matrice $ A $ est d'ordre $ n $. On peut observer que le principal avantage dépend du taux d'augmentation de $ r $ lorsque $ n $ diminue. Si $ r $ augmente lentement alors que $ n $ a considérablement diminué, on obtient une réduction importante des paramètres.

 Au lieu de manipuler $n^2$ entrées de $ A $, on peut manipuler $N_p^{TT}$ paramètres où
$$N_p^{TT} << n^2.$$

En particulier, lorsque l'ordre de $ A $ est une puissance de $ 2 $, c'est-à-dire $n = 2^d,$ on peut élargir la dimension à la dimension tensorielle maximale $ d $ alors que chaque $G_{\alpha}^{(k)}$ a maintenant $2 \times 2$ blocs. Dans ce cas, la représentation est appelée \textbf{représentation en train tensoriel quantifiée (QTT)}. Une telle représentation permet de réduire significativement le nombre de paramètres dans le tenseur. Le nombre de paramètres est alors donné  par
\begin{equation}
N_p^{QTT} = 4rlog_2n.
\end{equation}

Considérons maintenant que nous devons approximer un tenseur à $ d $ dimensions de taille $[n,n,\ldots,n]$ et $n^d$ entrées, par un nouveau tenseur avec peu de paramètres, par rapport à $n^d$. Pour cela, un vecteur-TT $\mbf{a}$ et une matrice-TT $\mathbf{A}$ doivent être approximés par de nouveaux tenseurs $\tilde{\mathbf{a}}$ et $\tilde{\mathbf{A}}$ de formes respectives. 
\begin{equation}
\label{TT:decomp1}
\tilde{\mathbf{a}} = \G^{(1)}(j_1)\G^{(2)}(j_2)\ldots \G^{(d)}(j_d),
\end{equation}
et
\begin{equation}
\label{TT:decomp2}
\tilde{\mathbf{A}} = \G^{(1)}(i_1,j_1)\G^{(2)}(i_2,j_2)\ldots \G^{(d)}(i_d,j_d),
\end{equation} 
où chaque
 $$\G^{(k)}(j_k) = \mbf{U}_{r_{k-1},j_k,r_k}^{(k)}\;\;\;\; \text{et}\;\;\;\; \G^{(k)}(i_k,j_k) = \mbf{U}_{r_{k-1},i_k,j_k,r_k}^{(k)}$$
respectivement. Chaque facteur de cette notation est une sorte de matrice de bloc dépendant des paramètres avec $r_{k-1}\times r_{k}$ blocs où le d-uplet $(r_0,r_1,\ldots,r_d)$ est appelé rang-TT et les relations $r_0 = r_d = 1$ sont imposées comme conditions limites. Le rang maximal
$$r = \max\{r_k\},\; k = 1:d-1$$
est censé être petit alors que, compte tenu d'un seuil $\epsilon$ ($0 < \epsilon < 1$), l'approximation vérifie $$\|\mbf{a}-\tilde{\mathbf{a}}\|_{F} < \epsilon, $$respectivement,
$$\|\mbf{A}-\tilde{\mathbf{A}}\|_{F}< \epsilon$$
où $\|.\|_{F}$ désigne la norme de Frobenius. En général, l'approximation se fait par k-matricialisation \eqref{matricisation1} dans chaque mode.

Une limite supérieure sur le rang $ r_{k} $ est donnée par le théorème ci-dessous tiré de \cite[p.145]{Nyenyezi2018}.
\begin{theo}
\emph{\\}
Si pour chaque matrice de dépliage de la forme \eqref{matricisation1} d'un tenseur $\mathbf{A}$ à $ d $ dimensions tel que $rang(A^{(k)}) = r_k,$ alors il existe une décomposition \eqref{TT:decomp1},  respectivement\eqref{TT:decomp2}, avec des rangs-TT non supérieurs à $r_k$ où $r_k$ est le rang de la k-matricialisation, $A^{(k)}$, du tenseur $\mathbf{A}$.
\end{theo}
 
Donc la décomposition peut être effectuée via des décompositions en  valeurs singulières ou QR (cfr point \ref{ssec:matrixdecomp}).
\vspace*{0.2cm}


Il peut maintenant être important de savoir dans quelle mesure l'approximation est meilleure dans un format de tenseur TT à faible rang. Le théorème et les corollaires suivants dont les démonstrations  peuvent être retrouvées dans \cite[p.2300]{oseledets2011tensor} montrent la précision attendue lors de l'approximation d'un tenseur sous format du train tensoriel de faible rang avec une précision donnée en utilisant la décomposition en valeurs singulières.

\begin{theo}\label{theo2.2}
	\emph{\\}
Supposons que la matrice de dépliage $A^{(k)}$ du tenseur $\mathbf{A}$ vérifie
		\begin{equation}
	A^{(k)} = B^{(k)}+E^{(k)},\;\ \text{avec} \;\ rang(A^{(k)}) = r_k,
	\; \text{et} \;\; \|E^{(k)}\|_{F} = \epsilon_k, 
	\end{equation}
$k = 1,2,\ldots,d-1.$ Alors, la TT-SVD calcule un tenseur au format TT avec des rangs-TT $r_k$ et
\begin{equation}
\|\mathbf{A}-\tilde{\mathbf{B}}\|_{F} \leq \sqrt{\sum_{k =1}^{d-1}\epsilon_k^2}.
\end{equation}
\end{theo}

Deux corollaires fournissent des informations sur l'amélioration de l'approximation TT par rapport à l'approximation CP et à la meilleure approximation via la SVD.

\begin{cor}
\emph{\\}
Si un tenseur $\mathbf{A}$ admet une approximation canonique avec $ R $ termes et une précision $\epsilon,$ alors il existe une approximation TT avec des rangs-TT $r_k \leq R$  et une précision $\sqrt{d-1}\epsilon.$
\end{cor}
\begin{cor}
\emph{\\}
Étant donné un tenseur $\mathbf{A}$ et des limites de rangs-TT $r_k,$, la meilleure approximation de A dans la norme de Frobenius avec les rangs-TT, désignée par $\mathbf{A}_{best},$, existe toujours et l'approximation TT $\mathbf{B}$ calculée par l'algorithme TT-SVD est quasi-optimale et nous avons que	
	
	\begin{equation}
\|\mathbf{A}-\mathbf{B}\|_{F}  \leq \sqrt{d-1}\|\mathbf{A}-\mathbf{A}_{best}\|_{F}.
\end{equation}	
\end{cor}
Ce dernier corollaire compare l'approximation TT calculée par SVD avec la meilleure approximation en norme Frobenius.
\begin{defi}[Norme tensorielle]
	\emph{\\}
	La norme d'un tenseur $ \mathbf{A} \in \R^{I_{1}\times I_{2}\times\cdots\times I_{d}} $ est la racine carrée de la somme des carrés de tous ses éléments, c'est-à-dire,
	\begin{equation}
	\begin{Vmatrix}
	\mbf{A}
	\end{Vmatrix}=\sqrt{\sum_{j_{1}=1}^{I_{1}}\sum_{j_{2}=1}^{I_{2}}\cdots\sum_{j_{d}=1}^{I_{d}}a_{j_{1}j_{2}\cdots j_{d}}^{2}}.
	\end{equation}
\end{defi}
C'est analogue à la norme de Frobenius matricielle \eqref{frobmatrix}.
Du \theoref{theo2.2}, il s'en suit immédiatement que si les valeurs singulières des matrices de dépliage sont tronquées à $ \delta $, l'erreur de l'approximation sera de $ \sqrt{d-1} \delta$, et pour obtenir toute précision prescrite $ \epsilon $, le seuil $ \delta $ doit être fixé à $ \pf{\epsilon}{\sqrt{d-1}} $. Enfin, un algorithme pour construire l'approximation TT avec une précision (relative) prescrite est donné  dans \cite[p.2301]{oseledets2011tensor}.

%\begin{rem}
%\emph{\\}
% Le nombre de paramètres dans le format d'arbre binaire de dimensionalité ainsi que pour le
%le format $ \mathcal{H} $-Tucker de est estimé comme suit
%\begin{equation*}
%	\op(dnr+(d-2)r^{3})\text{ \cite[p.2300]{oseledets2011tensor}}.
%\end{equation*}
%%On peut modifier la décomposition TT pour réduire $ (d-2)nr^{2}+2nr $ à $ dnr+(d-2)r^{3} $ en utilisant une décomposition de Tucker auxiliaire des tenseurs de base Gk. Gk est un tenseur rk-1 ×nk ×rk, et il n'est pas difficile de prouver que son rang de mode-2 n'est pas supérieur à tk, où tk est le rang de Tucker (rang de mode) [36] de A le long du kième
%\end{rem}

\subsubsection{Troncature et arrondi dans le format TT}
L'objectif est d'analyser les erreurs d'arrondi qui peuvent survenir soit lors de la conversion d'un tenseur général au format TT, soit lors de la troncature d'un tenseur-TT après certaines opérations.
Convertir un tenseur général en un tenseur-TT est déjà coûteux pour les tenseurs à haute dimension. Toutefois, si le tenseur est donné dans un certain format structuré, la complexité de la tâche peut être considérablement réduite. Considérons un tenseur qui est déjà au format TT mais dont les rangs-TT sont sous-optimaux. Ces rangs sous-optimaux sont souvent plus grands que les rangs réels et peuvent augmenter lorsque certaines opérations linéaires (telles que l'addition, le produit tenseur-vecteur ou les produits en mode-$ k $) sont effectuées. Une façon d'éviter la croissance des rangs-TT est l'utilisation de certaines techniques de compression et de certaines troncatures pour réduire ces rangs tout en maintenant la précision aussi bonne que possible. De nombreuses opérations tensorielles itératives ont tendance à augmenter le rang du tenseur résultant. Lorsque ces opérations sont répétées de nombreuses fois, le rang continue à augmenter et peut exploser. Pour cette raison, il est judicieux de remplacer après chaque opération un résultat $ \mbf{A}_{s} $ de rang $ s $ suffisamment grand par son approximation $ \mbf{B}_{r} $ de rang $ r $ relativement plus petit.
L'opérateur de troncature $ T_{r,s} $ est défini soit en fixant le rang de destination $ r $, soit en fixant un seuil $ \epsilon> 0 $ tel que $ \parallel \mbf{A}_{s}-\mbf{B}_{r}\parallel<\epsilon $ et $ r < s $. Ainsi,
	\begin{equation}
\label{troncDef}
\mbf{B}_r=T_{r,s}(\mbf{A})\Leftrightarrow ||\mbf{A}_s-\mbf{B}_r||<\epsilon, \quad r < s, 
\end{equation}	
dans une norme appropriée. Lorsque le rang de destination $ r $ est fixé, nous pouvons désigner l'opérateur de troncature simplement par $ T_{r} $. Un résultat dans \cite[p.354, Théorème 11.56]{hackbusch2012tensor} prouve l'existence d'une meilleure approximation pour les problèmes au format hiérarchique et donc au format TT. Lorsque cette approximation $ (T_{r,s}) $ utilise le HOSVD, c'est-à-dire que $ (s-r) $ plus petites valeurs propres et leurs vecteurs propres associés sont négligés dans chaque k-matricialisation, le théorème 11.64 dans \cite[p.362]{hackbusch2012tensor} montre que l'approximation vérifie
	\begin{equation}
	\label{troncApprox}
	||\mbf{A}-\mbf{B}_r||\leq \sqrt{2d-3}||\mbf{A}-\mbf{B}_{best}||.
	\end{equation}
	où $ d $ est la dimension de ces tenseurs, $ s $ est le rang maximal du tenseur actuel $ \mbf{A} $ et $ r $ est le rang de destination, c'est-à-dire le rang maximal fixe du tenseur $ \mbf{B} $ qui se rapproche de $ \mbf{A} $ \cite[p.151]{Nyenyezi2018}.
	
	Ici, nous présentons une autre technique proposée dans \cite[p.2302]{oseledets2011tensor} pour la décomposition TT. Cette technique utilise les méthodes successives de décompositions QR et SVD.
	
	 Considérons un tenseur de la forme \eqref{TT:decomp2} avec des rangs-TT $(r_k),\; k = 1,\ldots,d-1.$ L'objectif est d'estimer le
	 plus petit rang optimal $(r_k^{'}),\;\; r_k^{'} \leq r_k, \; k = 1,\ldots,d-1$ tout en maintenant une précision aussi élevée que possible. Une telle procédure sera appelée \textit{arrondi} (elle peut également être appelée \textit{troncature} ou \textit{recompression}), car elle est analogue à l'arrondi lorsque l'on travaille avec des nombres à virgule flottante, mais au lieu de chiffres et de
	 mantisse, nous avons une représentation peu paramétrique d'un tenseur.
	 
	Considérons le tenseur  $\mbf{A} = \G^{(1)}(j_1)\G^{(2)}(j_2)\ldots \G^{(d)}(j_d)$ décrit dans \eqref{TT:decomp1}. Pour comprimer ce tenseur, on peut d'abord le matricialiser, par exemple en mode-un, et appliquer sa décomposition dyadique:
		\begin{equation}
	\label{TT-dyadicMod1}
	A^{(1)} = UV^T, 
	\end{equation}
où
	\begin{equation}
	\label{dyadicDecomp1}
	U(j_1,\alpha_1) = \G^{(1)}(j_1,\alpha_1)
	\end{equation}
	et
	\begin{equation}
	\label{dyadicDecomp2}
	V(j_2,\ldots,j_d,\alpha_1) = \G^{(2)}(\alpha_1,j_2)\G^{(3)}(j_3)\ldots \G^{(d)}(j_d).
	\end{equation}
	D'une part, puisque la taille du tenseur complet $\mbf{A}$ est $(n^{(1)},n^{(2)},\ldots,n^{(d)}),$ la taille de la matricialisation du mode-1 est $(n^{(1)}, \prod_{k =2}^d n^{(k)}).$
	
	Ainsi, la matrice U est petite et sa décomposition QR peut être directe et moins chère. D'autre part, la matrice V est très grande et sa décomposition QR nécessite des procédures structurées. Dans le cas où les décompositions
	\begin{equation}
	\label{qrTTred}
	U = Q_uR_u,\quad \text{et}\quad V = Q_vR_v,
	\end{equation}
	sont disponibles, on peut construire une petite matrice   $P = R_uR_v^T$ de taille  $r \times r$ dont la SVD est facile à calculer:
	\begin{equation}
	\label{svdTTred}
	P = X\Sigma Y^T.
	\end{equation}
	La matrice $\Sigma$ est diagonale de taille $\hat{r} \times \hat{r}$ où $\hat{r} \leq r$  est obtenue après troncature\footnote{La troncature peut se faire en fixant $\hat{r}$ ou en suivant un seuil $\hat{\epsilon}.$ De plus, on remarque que dans ce cas $\hat{r}$ peut être le plus petit rang $ r' $  ou peut être réduit encore une fois en fonction du seuil \cite[p.152]{Nyenyezi2018}.} tandis que $ X $ et $ Y $ sont de taille  $r \times \hat{r}$ avec des colonnes orthonormées.
	
	 Pour la matrice $A^{(1)},$ les matrices des vecteurs singuliers dominants sont $\hat{U} = Q_uX$ et $\hat{V} = Q_vY.$
	 
	Puisque la décomposition QR de U peut être directe, décrivons maintenant comment la décomposition QR de V peut être faite structurellement. Pour ce faire, il est nécessaire d'orthogonaliser certains facteurs. Le lemme ci-dessous tiré de \cite[p.2302]{oseledets2011tensor} indique que pour une décomposition TT avec des noyaux orthogonaux, les matrices correspondantes ont des lignes orthonormales.

\begin{lem}\emph{\\}
Si un tenseur $\mathbf{Z}$ est exprimé sous la forme
\begin{equation}
\mathbf{Z} (\alpha_1,j_2,\ldots,j_d) = Q^{(2)}(j_2)Q^{(3)}(j_3)\ldots Q^{(d)}(j_d),
\end{equation}
où $Q^{(k)}(j_k)$ est une matrice, $k = 1,2,\ldots,d,$ et $r_{d} =1$ (pour $j_k$ fixé, $k=2,\ldots,d$, le produit se réduit à un vecteur de longueur $ r_{1} $,
qui est indexé par $\alpha_1$), et les matrices $Q^{(k)}(j_k)$ satisfont la condition d'orthogonalité 
\begin{equation}
\label{TT:Q_orthog}
\sum_{j_k}Q^{(k)}(j_k)\left(Q^{(k)}(j_k)\right)^T = I_{r_{k-1}}
\end{equation}
où $I_{r_{k-1}}$ est la matrice identité de taille $r_{k-1},$ et $\mathbf{Z}$, considérée comme une matrice $Z$ de taille $r_1 \times \prod_{k = 2}^dn^{(k)}$, a des lignes orthonormées$$
(ZZ^T)_{\alpha_1\hat{\alpha}_1} = \sum_{j_2,\ldots,j_d}Z(\alpha_1,j_2,\ldots,j_d)Z(\hat{\alpha}_1,j_2,\ldots,j_d) = \delta_{\alpha_1\hat{\alpha}_1}.$$
\end{lem}
En utilisant ce lemme, on peut aborder une décomposition $ QR $ structurée de la matrice $ V $ donnée au format TT. Considérons	$$V(j_2,\ldots,j_d) = \G^{(2)}(j_2)\ldots \G^{(d)}(j_d).$$
Par un algorithme qui balaie tous les noyaux de droite à gauche, on peut orthogonaliser les noyaux $\G^{(k)}(j_k),\; k = \lbrace{d,d-1,\ldots,2}\rbrace$, en suivant les étapes ci-dessous.\\


Premièrement, puisque $r_d = 1$, $\G^{(d)}(j_d)$ est une matrice $(r_{d-1},n^{(d)})$ dont la décomposition $ QR $ écrit

$$\G^{(d)}(j_d) = R^{(d)}Q^{(d)}(j_d),$$ où la taille de $R^{(d)}$ est $(r_{d-1},r_{d-1})$ et $Q^{(d)}(j_d)$ de taille $(r_{d-1},n^{(d)})$  a des lignes orthogonales:
$$\sum_{j_d}Q^{(d)}(j_d)\left(Q^{(d)}(j_d)\right)^T = I_{r_{d-1}}.$$
Ensuite, en fixant$$\G^{'(d-1)}(j_{d-1}) = \G^{(d-1)}(j_{d-1})R^{(d)}$$
conduit à un tenseur-noyau de même taille en utilisant un produit de noyau contracté avec une transformation de taille
 $$\left(r_{d-2},n^{(d-1)},r_{d-1}\right) \times \left(r_{d-1},r_{d-1}\right) \longrightarrow  \left(r_{d-2},n^{(d-1)},r_{d-1}\right).$$ 
 Cela nous permet d'écrire
 	
 $$V(j_2,\ldots,j_d) = \G^{(2)}(j_2)\ldots \G^{'(d-1)}(j_d)Q^{(d)}(j_d),$$
 avec $Q^{(d)}(j_d)$ qui satisfait \eqref{TT:Q_orthog}.\\
 
 Par induction, on peut supposer à l'étape $s = k+1,$ que les noyaux ($d,d-1,\ldots k+1$) sont othogonaux.
  Nous avons donc la représentation
	\begin{equation}
	V(j_2,\ldots,j_d) = \G^{(2)}(j_2)\ldots \G^{'(k)}(j_k)Q^{(k+1)}(j_{k+1})\ldots Q^{(d)}(j_d),
	\end{equation}
	
	sont des matrices $Q^{(s)}(j_s),\;\; s = k+1,\ldots,d,$ satisfont \eqref{TT:Q_orthog} et au pas $s = k,$ la transformation est toujours valable. Pour cela, il suffit de calculer	\begin{equation}
	\label{TT:QR_Gd}
	\G^{'(k)}(j_k) = R^{(k)}Q^{(k)}(j_k) 
	\end{equation}
	avec
	\begin{equation}
	\label{TT:Q_orthog2}
	\sum_{j_k}Q^{(k)}(j_k)\left(Q^{(k)}(j_k)\right)^T = I_{r_{k-1}},
	\end{equation}
et observer que la matrice $R^{(k)}$ de taille $(r_{k-1},r_{k-1})$ est indépendante de $j_k.$

Cela conduit à la conclusion que $Q^{(k)}$ et $R^{(k)}$ peuvent être calculés via l'orthogonalisation des lignes de la matrice $ G $ obtenue en remodelant $\G^{(k)}$ en une matrice de taille $r_{k-1},n^{(k)}r_k$. Ainsi, on a calculé la décomposition $ QR $ à l'aide des tenseurs-noyaux $\G^{(k)}(j_k)$ de la décomposition TT de la matrice A. Pour effectuer la compression, on peut calculer la SVD comprimée et contracter deux noyaux contenant un ensemble d'indices communs (pour plus de détails, voir \cite[p.2305]{oseledets2011tensor}).

\section*{Conclusion partielle}

 Ce chapitre portait essentiellement sur les représentations et les traitements des tenseurs numériques et ces derniers sont considérés  en effet comme des tableaux multidimensionnels; qui sont des généralisations des matrices (tranches). Il a été dit que les lignes et colonnes d'une matrice portent un nom générique de "mode" chez les tenseurs. Par ailleurs, certaines opérations  spécifiques ont été présentées; c'est le cas du produit de Kronecker, le produit de Khatri-Rao, le produit de Hadamard, le produit en mode k, le produit contracté, la vectorisation, la matricialisation, la tensorisation, etc. Certaines d'entre elles admettent comme soubassement les techniques de l'algèbre linéaire. Néanmoins, certains concepts de l'algèbre linéaire ne sont pas définis de manière unique en dimension supérieure, le rang tensoriel en étant un paradigme. Contrairement aux matrices, le rang d'un tenseur défini sur un corps $ \K $ n'est pas le même selon que $ \K=\R $ ou $ \K=\C $. En outre, certains auteurs ont montré qu'il est pratiquement difficile de déterminer le rang. Même avec l'arithmétique exact, le calcul du rang est, en général, pas faisable pour les tenseurs de grande taille. Et à \cite{haastad1990tensor} de prouver que la détermination du rang tensoriel est un problème NP-difficile.\\ Pour finir, nous avons présenté des décompositions tensorielles qui sont en fait des généralisations des décompositions matricielles. Ici, il s'agissait surtout de la décomposition canonique polyadique, la décomposition de Tucker, la décomposition hiérarchique de Tucker et la décomposition en train tensoriel.

%\chapter{Application des tenseurs numériques au traitement des signaux}
\chapter{Traitement multidimensionnel du signal par décomposition tensorielle}
\label{chap3}

\section*{Introduction}
Pendant longtemps le traitement du signal s'est essentiellement
intéressé aux signaux dépendant d'une seule
variable (\textit{le temps}). Le développement rapide des
moyens de traitement à la fois dans leur capacité de
stockage et dans leur vitesse de calcul permet de
mettre en œuvre des systèmes de traitement de signaux
dépendant de plusieurs variables (\textit{temps, espace}, ...) \cite[p.87]{LACOUME1998}. Il est proposé d'appeler signaux multidimensionnels
(signaux ND) de tels signaux.
Une modélisation multidimensionnelle peut être adoptée dans
un grand nombre de problèmes se rattachant à des domaines
aussi variés que la sociologie, l'analyse de données ou le \textit{traitement
	du signal}. En physique et traitement du signal, les
enregistrements numériques multidimensionnels sont modélisés par des tenseurs. Chaque mode d'un tenseur représente une
grandeur physique telle que l'espace (\textit{longueur, largeur, hauteur}),
le temps, le canal de couleur (\textit{longueur d'onde}), à laquelle
est associée un espace vectoriel dont la dimension est égale au
nombre d'échantillons numériques effectués dans cette dimension \cite[p.1]{MUTIfrance}.

%Par exemple, une image en couleur se modélise par un
%tenseur trimodal: \textit{deux modes sont associés aux lignes et aux
%	colonnes, et un troisième au canal de couleur }(RVB). De même,
%en sismique, ou en acoustique sous-marine, lorsqu'une antenne
%rectiligne est employée, une modélisation trimodale des données peut être adoptée: \textit{un mode est associé à l'axe spatial, un
%	mode au temps, et un dernier à la polarisation de l'onde} .
%
%Les traitements des données multidimensionnelles procèdent
%généralement à un découpage du tenseur en vecteurs, ou matrices
%d'observations, de sorte que les méthodes du second ordre
%soient applicables. Les données traitées sont ensuite fusionnées pour retrouver la dimension du tenseur initial.
%Ce processus de découpage des données multidimensionnelles
%provoque inévitablement une perte d'information par rapport
%à la quantité globale d'information contenue dans le tenseur.
%Ainsi, il est intéressant de conserver le tenseur de
%données comme entité indivisible de manière à disposer potentiellement
%de plus d'information que ce que l'on pourrait
%obtenir par le découpage du tenseur de données \cite[p.1]{MUTIfrance}. 

Dans ce chapitre, il sera question de présenter la façon dont les signaux multidimensionnels sont traités via l'approche tensorielle. Dans la mesure du possible, certains exemples seront présentés sans toutefois aller dans les détails exhaustifs. Dans la suite, d'une part, la section \ref{sec:3.1ETAT} présente un état de l'art de l'analyse multidimensionnelle. Et d'autre part, la section \ref{sec:3.3adapt} portera sur le traitement de signaux multidimensionnels.
\section{État de l'art de l'analyse multidimensionnelle}\label{sec:3.1ETAT}
Les racines de l'analyse multidimensionnelle remontent à l'étude des polynômes homogènes au XIX\up{e} siècle, à laquelle ont participé Gauss, Kronecker, Cayley, Weyl et Hilbert. Dans l'interprétation moderne, il s'agit de tenseurs entièrement symétriques. Les décompositions de tenseurs non symétriques sont étudiées depuis le début du XX\up{e} siècle, tandis que les avantages de l'utilisation de plus de deux matrices dans l'analyse factorielle (AF) sont apparus dans plusieurs communautés depuis les années 1960. La décomposition de Tucker (TKD) pour les tenseurs a été introduite en psychométrie, tandis que la décomposition polyadique canonique (CPD) a été redécouverte indépendamment et mise dans un contexte d'application sous les noms de décomposition canonique (CANDECOMP) en psychométrie et de modèle factoriel parallèle (PARAFAC) en linguistique. Les tenseurs ont ensuite été adoptés dans diverses branches de l'analyse des données, telles que la chimiométrie, l'industrie alimentaire et les sciences sociales. En ce qui concerne le \textit{traitement du signal}, le début des années 1990 a vu un intérêt considérable pour les statistiques d'ordre supérieur (HOS), et on s'est rapidement rendu compte que, pour les cas multivariés, les HOS sont effectivement des tenseurs d'ordre supérieur; en effet, les approches algébriques de l'analyse en composantes indépendantes (ICA) utilisant les HOS étaient intrinsèquement basées sur les tenseurs. Vers 2000, on a réalisé que la TKD représente une décomposition multilinéaire de la valeur singulière (MLSVD). Généralisant la décomposition matricielle de la valeur singulière (SVD), le cheval de bataille de l'algèbre linéaire numérique, la MLSVD a stimulé l'intérêt pour les tenseurs en mathématiques appliquées et en calcul scientifique en très haute dimension. Parallèlement, la MLSVD a été adoptée avec succès comme outil pour \textit{le traitement des réseaux de capteurs et la séparation déterministe des signaux dans les communications sans fil}. Par la suite, les tenseurs ont été utilisés dans \textit{le traitement de l'audio, des images et de la vidéo, l'apprentissage automatique et les applications biomédicales}, pour ne citer que quelques domaines \cite[p.146]{cichocki2015tensor}.

\section{Traitement d'un signal multidimensionnel}\label{sec:3.3adapt}
Le traitement du signal est une discipline indispensable de
nos jours. Il a pour objet l'élaboration ou l'interprétation des
signaux porteurs d'informations. Son but est donc de réussir à extraire un maximum
d'informations utiles sur un signal perturbé par du bruit en
s'appuyant sur les ressources de l'électronique et de l'informatique.
En grande partie, le contenu de cette section a été puisé dans \cite{MUTIfrance} et \cite{cichocki2015tensor}.
\subsection{Généralités}
Le signal est le support de l’information émise par une source et destinée à un
récepteur; c’est le véhicule de l’intelligence dans les systèmes. Il transporte les
ordres dans les équipements de contrôle et de télécommande, il achemine sur les
réseaux l’information, la \textit{parole} ou l’\textit{image}. Il est particulièrement fragile et doit être
manipulé avec beaucoup de soins. Le \textit{traitement} qu’il subit a pour but d’extraire des
informations, de modifier le message qu’il transporte ou de l’adapter aux moyens de
transmission; c’est là qu’interviennent les \textit{techniques numériques}.\par  En effet, si l’on
imagine de substituer au signal un ensemble de nombres qui représentent sa grandeur
ou amplitude à des instants convenablement choisis, le traitement, même dans
sa forme la plus élaborée, se ramène à une séquence d’opérations logiques et arithmétiques
sur cet ensemble de nombres, associées à des mises en mémoire.
La conversion du signal continu analogique en un signal numérique est réalisée
par des capteurs qui opèrent sur des enregistrements ou directement dans les
équipements qui produisent ou reçoivent le signal \cite[p.7]{bellanger2012traitement}.
\par La conversion d’un signal analogique sous forme numérique implique une double
approximation. D’une part, dans l’espace des temps, le signal fonction du temps
$ s(t) $ est remplacé par ses valeurs $ s(nT) $ à des instants multiples entiers d’une durée
$ T $; c’est l’opération d’\textit{échantillonnage}. D’autre part, dans l’espace des amplitudes,
chaque valeur $ s(nT) $ est approchée par un multiple entier d’une quantité élémentaire
$ q $; c’est l’opération de $ quantification $. La valeur approchée ainsi obtenue est
ensuite associée à un nombre; c’est le $ codage $, ce terme étant souvent utilisé pour
désigner l’ensemble, c’est-à-dire le passage de la valeur $ s(nT) $ au nombre qui la
représente.
\subsection{D'une matrice à un tenseur}
Les approches de l'analyse en composantes (matricielles) bidirectionnelle sont bien établies et comprennent l'analyse en composantes principales (ACP), l'ICA, la factorisation de la matrice non négative (NMF) et l'analyse en composantes éparses (SCA). Ces techniques sont devenues des outils standard pour, par exemple, \textit{la séparation aveugle des sources} (BSS), \textit{l'extraction de caractéristiques ou la classification}. \\
Les premières approches d'analyse de données multidimensionnelles reformataient le tenseur de données sous forme de matrice et recouraient à des méthodes développées pour l'analyse classique bidimensionnelle. Cependant, une telle vision aplatie du monde et les hypothèses rigides inhérentes à l'analyse à deux dimensions ne sont pas toujours adaptées aux données multidimensionnelles. On ne peut découvrir des composantes cachées dans les données multidimensionnelles que si les outils d'analyse tiennent compte des modèles multidimensionnels intrinsèques présents, ce qui motive le développement de techniques multilinéaires. Une nette différence existe entre les matrices et les tenseurs. Les matrices représentent des transformations linéaires et des formes quadratiques, tandis que les tenseurs sont liés à des applications multilinéaires et à des polynômes multivariés.
 D'après la \defref{tensornum}, un tenseur est considéré comme un tableau numérique à indices multiples, l'ordre d'un tenseur étant le nombre de ses modes ou dimensions; ceux-ci peuvent inclure l'\textit{espace}, le \textit{temps}, la \textit{fréquence}, les \textit{essais}, les \textit{classes} et les \textit{dictionnaires}. Un tenseur à valeurs réelles d'ordre $ N $ est désigné par $ \mbf{X}\in\RItn $ et ses entrées par $ x_{j_{1}j_{2}\cdots j_{N}} $.
 \subsection{Interprétation des données dans l'analyse bidirectionnelle}
 
 L'objectif de la BSS, de la FA et de l'analyse des variablesA latentes est de décomposer une matrice de données $ X\in\R^{I\times J} $ en  facteurs matriciels $ A=\left[\a_{1},\dots,\a_{R}\right]\in\R^{I\times R} $ et $ B=[\b_{1},\dots,\b_{R}]\in\R^{J\times R} $ comme
\begin{eqnarray}
\mathrm{X}&=&ADB^{T}+E\nonumber\\
&=&\sum_{r=1}^{R}\lambda_{r}\a_{r}\b_{r}^{T}+E\nonumber\\
&=&\sum_{r=1}^{R}\lambda_{r}\a_{r}\circ\b_{r}+E,\label{BSS}
\end{eqnarray}
 où $ D= diag(\lambda_{1},\lambda_{2},\dots,\lambda_{R}) $ est une matrice de mise à l'échelle (normalisation), les colonnes de B représentent les \textit{signaux sources inconnus} (facteurs ou variables latentes selon les tâches à accomplir), les colonnes de A représentent \textit{les vecteurs de mélange associés} (ou charges de facteurs), tandis que E est \textit{le bruit\footnote{Un bruit correspond à tout phénomène perturbateur gênant
 		la transmission ou l'interprétation d'un signal.} dû à une partie de données non modélisée ou à une erreur de modèle}. En d'autres termes, le modèle \eqref{BSS} suppose que la matrice de données $ \mathrm{X} $ comprend des composantes cachées $ \b_{r}(r=1,2,\dots,R) $ qui sont mélangées d'une manière inconnue par l'intermédiaire des coefficients de A, ou, de manière équivalente, que les données contiennent des facteurs qui ont une charge associée pour chaque canal de données. La \figref{fig:fig44} représente le modèle \eqref{BSS} sous la forme d'une décomposition dyadique, où les termes $ \a_{r}\circ\b_{r}=\a_{r}\b_{r}^{T} $ sont des matrices de rang 1.

Les indéterminations bien connues intrinsèques à ce modèle sont:
\begin{enumerate}
 \item
 la mise à l'échelle arbitraire des composantes et
 \item 
 la permutation des termes de rang 1.
\end{enumerate} 
\begin{figure}[!h]
	\centering
	\encad{\centering
		\includegraphics[width=0.5\linewidth]{"My figures/FIG44"}}
	\caption{L'analogie entre (a) décomposition dyadique et (b) les PD; le format de Tucker a un noyau diagonal.} %\cite[p.149]{cichocki2015tensor}.}
	\label{fig:fig44}
\end{figure}
Une autre indétermination est liée à la signification physique des facteurs: si le modèle \eqref{BSS} n'est pas contraint, il admet une infinité de combinaisons de A et B. Les factorisations matricielles standard en algèbre linéaire, telles que la factorisation QR, la décomposition en valeurs propres (EVD) et la SVD, ne sont que des méthodes spéciales de factorisation de \eqref{BSS}, et doivent leur unicité à des contraintes dures et restrictives telles que la triangularité et l'orthogonalité. D'autre part, certaines propriétés des facteurs de \eqref{BSS} peuvent être représentées par des contraintes appropriées, rendant possible l'estimation ou l'extraction unique de ces facteurs. Ces contraintes comprennent \textit{l'indépendance statistique, la sparsité, la non-négativité, la structure exponentielle, la non-corrélation, le module constant, l'alphabet fini, la régularité et l'unimodalité}. En effet, les quatre premières propriétés constituent la base de l'ICA, de la SCA , de la NMF  et de l'extraction harmonique.

\subsection{Tensorisation-blanchissement de la dimensionnalité}
Alors que les structures algébriques unidirectionnelles (vecteurs) et bidirectionnelles (matrices) ont été respectivement introduites comme des représentations naturelles des segments de mesures scalaires et des mesures sur une grille, les tenseurs ont été initialement utilisés uniquement pour les avantages mathématiques qu'ils procurent dans l'analyse des données; par exemple, il semblait naturel d'empiler les matrices de spectroscopie d'excitation-émission en chimiométrie dans un tenseur d'ordre 3.

\begin{figure}[!h]
	\encad{\centering \includegraphics[width=0.35\linewidth]{"My figures/AFIG3A"}}
	\caption{Construction des tenseurs: (a) Vectorisation des vecteurs ou des matrices  dans le format dit quantifié; en informatique scientifique, cela facilite la supercompression de vecteurs ou de matrices à grande échelle. (b) Le tenseur est formé par la discrétisation d'une fonction trivariée f(x,y,z).}
	\label{fig:afig3a}
\end{figure}

La procédure de création d'un tenseur de données à partir de données originales de dimension inférieure est appelée \textit{tensorisation}, et \cite[p.148]{cichocki2015tensor} propose la taxonomie suivante pour la génération de tenseurs:

\begin{enumerate}
\item[(1)]\textit{Réarrangement de structures de données de dimension inférieure}: les vecteurs ou matrices à grande échelle sont facilement tensorisés en tenseurs d'ordre supérieur et peuvent être compressés par des décompositions tensorielles s'ils admettent une approximation tensorielle de faible rang; ce principe facilite l'analyse des données volumineuses (voir \figref{fig:afig3a}(a)). Par exemple, un signal exponentiel unidirectionnel
\begin{equation}
x(k) = az^{k}
\end{equation} peut être réarrangé en une matrice de Hankel de rang 1 ou un tenseur de Hankel.

\begin{equation}\label{hankel}
H=\begin{pmatrix}
x(0)&x(1)&x(2)&\cdots\\
x(1)&x(2)&x(3)&\cdots\\
x(2)&x(3)&x(4)&\cdots\\
\vdots&\vdots&\vdots&
\end{pmatrix}=a\b\circ\b,
\end{equation}
où $ \b=\begin{bmatrix}
1,z,z^{2},\dots
\end{bmatrix}^{T} $. 

 De même, dans le traitement des réseaux de capteurs, des structures tensorielles apparaissent naturellement lors de la combinaison d'instantanés provenant de sous-réseaux identiques.
\item[(2)]
\textit{Construction mathématique}: Parmi de nombreux exemples, les moments d'ordre N (cumulants) d'une variable aléatoire vectorielle forment un tenseur d'ordre N, tandis que dans l'ICA d'ordre 2, les instantanés des statistiques de données (matrices de covariance) sont effectivement des tranches d'un tenseur d'ordre 3. De même, une matrice de données ($ canal\times temps $) peut être transformée en un tenseur ($ canal\times temps\times fréquence $) ou ($canal\times temps\times échelle $) via des représentations \textit{temps-fréquence} ou \textit{ondelettes}, une procédure puissante dans l'analyse des électroencéphalogrammes (EEG) multicanaux dans les sciences du cerveau.
\item[(3)]
\textit{Plan d'expérience}: Les données à facettes multiples peuvent être naturellement empilées dans un tenseur; par exemple, dans les communications sans fil, la diversité des signaux \textit{(temporelle, spatiale, spectrale, etc.}) correspond à l'ordre du tenseur. Dans le même esprit, les visages propres standard peuvent être généralisés en visages tensoriels en combinant des images avec différents éclairages, poses et expressions, tandis que les modes communs dans les enregistrements EEG entre les sujets, les essais et les conditions sont mieux analysés lorsqu'ils sont combinés ensemble dans un tenseur.
\item[(4)]
\textit{Données tensorielles naturelles}: Certaines sources de données sont facilement générées sous forme de tenseurs (par exemple, les images couleur RVB, les vidéos, les affichages de champs lumineux tridimensionnels (3D)). En outre, dans le domaine de l'informatique scientifique, on a souvent besoin d'évaluer une fonction multivariable discrétisée; il s'agit d'un tenseur naturel, comme l'illustre la  \figref{fig:afig3a}(b) pour une fonction trivariable $ f(x, y, z) $.
\end{enumerate}
La haute dimensionnalité du format tensoriel est donc associée à des avantages, qui incluent les possibilités d'obtenir des représentations compactes, l'unicité des décompositions, la flexibilité dans le choix des contraintes, et la généralité des composants qui peuvent être identifiés.

\subsection{Décomposition polyadique canonique}
On rappelle  qu'un tenseur $ \mbf{X} \in\RItn$ est dans le format CPD, lorqu'il s'écrit comme une combinaison linéaire de tenseurs de rang 1 de la forme
\begin{equation}\label{3.01}
\mbf{X}=\sum_{i=1}^{R}\lambda_{r}\b_{r}^{(1)}\circ\b_{r}^{(2)}\cdots\circ\b_{r}^{(N)}.
\end{equation}
 
De manière équivalente, $ \mbf{X} $ est exprimé comme un produit multilinéaire avec un noyau diagonal
\begin{eqnarray}
\mbf{X}&=&\mathcal{D}\times_{1}B^{(1)}\times_{2}B^{(2)}\cdots\times_{N}B^{(N)}\nonumber\\
&=&\begin{bmatrix}
\mathcal{D};B^{(1)},B^{(2)},\dots,B^{(N)}\label{3BSS}
\end{bmatrix},
\end{eqnarray}
où $ \mathcal{D}=diag_{N}(\lambda_{1},\lambda_{2},\dots,\lambda_{R}) $
d'ordre 3.

\textbf{Rang}: Dans les applications de traitement du signal, l'estimation du rang correspond le plus souvent à la détermination du nombre de composantes tensorielles qui peuvent être récupérées avec une précision suffisante, et souvent il n'y a que quelques composantes de données présentes. Une première évaluation pragmatique du nombre de composantes peut se faire par l'inspection du spectre des valeurs singulières multilinéaires, qui indique la taille du tenseur central dans la partie droite de la \figref{fig:fig44}(b). 

\textbf{Unicité}: 
Les conditions d'unicité donnent des limites théoriques pour les décompositions tensorielles exactes. Une condition d'unicité classique est due à Kruskal, qui affirme que pour les tenseurs du troisième ordre, la DPC est unique jusqu'aux ambiguïtés inévitables d'échelle et de permutation, à condition que $ k_{B^{(1)}}+k_{B^{(2)}}+k_{B^{(3)}}\geq 2R+2 $, où le rang de Kruskal $ k_{B} $ d'une matrice B est la valeur maximale garantissant que tout sous-ensemble de $ k_{B} $ colonnes est linéairement indépendant. En modélisation clairsemée, le terme ($ k_{B}+ 1 $) est également connu sous le nom d'\textit{étincelle}. Une généralisation aux tenseurs d'ordre N est due à Sidiropoulos et Bro  et est donnée par
\begin{equation*}
	\sum_{n=1}^{N}k_{B^{(n)}}\geq 2R+N-1.
\end{equation*}
Des conditions d'unicité plus relaxées peuvent être obtenues lorsqu'une matrice de facteurs a un rang de colonne complet. Tout ceci montre que, par rapport aux décompositions matricielles, la DPC est unique sous des conditions plus naturelles et relaxées, qui exigent seulement que les composantes soient suffisamment différentes et que leur nombre ne soit pas déraisonnablement grand. Ces conditions n'ont pas de contrepartie matricielle et sont au cœur de la séparation des signaux basée sur les tenseurs.\\

\textbf{Calcul}: Certaines conditions permettent le calcul explicite des matrices de facteurs dans \eqref{BSS} en utilisant l'algèbre linéaire. La présence de bruit dans les données signifie que la DPC est rarement exacte, et on doit adapter un modèle de DPC aux données en minimisant une fonction de coût appropriée. Ceci est généralement réalisé en minimisant la norme de Frobenius de la différence entre le tenseur de données donné et son approximation CP, ou, alternativement, par l'ajustement de la moindre erreur absolue lorsque le bruit est laplacien. 

Le calcul de la DPC étant intrinsèquement multilinéaire, on peut arriver à la solution par une séquence de sous-problèmes linéaires comme dans le cadre des moindres carrés alternatifs (ALS), où la fonction de coût des moindres carrés (LS) est optimisée pour une matrice composante à la fois, tout en gardant les autres matrices composantes fixes.

Bien que l'ALS soit attrayante pour sa simplicité et ses performances satisfaisantes pour quelques composantes bien séparées et pour un rapport signal/bruit (SNR) suffisamment élevé, elle hérite également des problèmes des algorithmes alternatifs et n'est pas garantie de converger vers un point stationnaire. Ce problème peut être corrigé en ne mettant à jour que la matrice de facteurs pour laquelle la fonction de coût a le plus diminué à une étape donnée, mais cela entraîne une augmentation de $ N $ fois du coût de calcul par itération. La convergence de l'ALS (disponible dans \cite[p.471]{Hong2008} ) n'est pas encore complètement comprise, elle est quasi-linéaire près du point stationnaire, alors qu'elle devient plutôt lente pour les cas mal conditionnés.\\

\textbf{Contraintes: }Comme mentionné précédemment, dans des conditions assez douces, la DPC est unique par elle-même, sans nécessiter de contraintes supplémentaires. Cependant, pour améliorer la précision et la robustesse par rapport au bruit, la connaissance préalable des propriétés des données (par exemple, l'indépendance statistique, la sparsité) peut être incorporée dans les contraintes sur les facteurs de manière à faciliter leur interprétation physique, à assouplir les conditions d'unicité et même à simplifier le calcul. De plus, les contraintes d'orthogonalité et de non-négativité assurent l'existence du minimum du critère d'optimisation utilisé.\\

\textbf{Applications: }La CPD s'est déjà imposée comme un outil avancé pour la séparation des signaux dans des branches très diverses du traitement du signal et de l'analyse des données, comme le traitement audio et vocal, l'ingénierie biomédicale, la chimiométrie et l'apprentissage automatique. Les algorithmes d'ICA algébriques sont effectivement basés sur la CPD d'un tenseur des statistiques des enregistrements; l'indépendance statistique des sources se reflète dans la diagonalité du tenseur central de la \figref{fig:fig44}, c'est-à-dire dans la disparition des statistiques croisées. La CPD est également très utilisée dans l'analyse exploratoire des données, où les termes de rang 1 capturent les propriétés essentielles des signaux dynamiquement complexes. Un autre exemple est celui des communications sans fil, où les signaux transmis par différents utilisateurs correspondent aux termes de rang 1 dans le cas d'une propagation en visibilité directe. De même, dans les applications de type extraction harmonique et direction d'arrivée, les exponentielles réelles ou complexes ont une structure de rang 1, pour laquelle l'utilisation de la DPC est naturelle. 
% A titre d'illustration, considérons l'exemple suivant  tiré de \cite[p.150]{cichocki2015tensor}
%
%\begin{ex}\emph{\\}
%Considérons un réseau de capteurs composé de $ K $ sous-réseaux déplacés mais par ailleurs identiques de $ I $ capteurs, avec $\tilde{I}=KI  $ capteurs  au total.\\
% Pour $ R $ sources en bande étroite dans le champ lointain, le modèle équivalent en bande de base de la sortie du réseau devient $ X=AS^{T}+E $, où $ A\in\C^{I\times R} $ est la réponse globale du réseau, $ S\in\C^{J\times R} $ contient J instantanés des sources, et E est le bruit. Une source unique ($ R = 1 $) peut être obtenue à partir de la meilleure approximation de rang 1 de la matrice X; cependant, pour $ R>1 $, la décomposition de X n'est pas unique et, par conséquent, la séparation des sources n'est pas possible sans incorporer des informations supplémentaires. Les contraintes sur les sources qui peuvent donner une solution unique sont, par exemple, un module constant et l'indépendance statistique.\\
% Considérons une matrice de sélection de lignes $ J_{k}\in\C^{I\times \tilde{I}} $ qui extrait les lignes de X correspondant au k-ième sous-réseau, $ k=1,\dots,K $. Pour deux sous-réseaux identiques, l'EVD généralisée des matrices $ J_{1}X $ et $ J_{2}X $ correspond à l'estimation bien connue des paramètres du signal par des techniques d'invariance rotationnelle (ESPRIT). Pour le cas $ K>2 $, nous considérerons $ J_{k}X $ comme des tranches du tenseur $ \mbf{X}\in\C^{I\times J\times K} $. On peut montrer que la partie signal de $ \mbf{X} $ admet un CPD comme dans \eqref{BSS} et \eqref{3.01}, avec $ \lambda_{1}=\cdots=\lambda_{R}=1 $,
%\begin{equation*}
%J_{k}A=B^{(1)}diag\left(b_{k1}^{(3)},\dots,b_{kR}^{(3)}\right), \text{ et }B^{(2)}=S,
%\end{equation*} et la séparation des sources qui en découle dans des conditions plutôt légères, son unicité ne nécessite pas de contraintes telles que l'indépendance statistique ou un module constant. De plus, la décomposition est unique même dans les cas où le nombre de sources, $ R $, dépasse le nombre de capteurs de sous-réseaux, $ I $, ou même le nombre total de capteurs, $ \tilde{I} $. Il est à noter que des géométries particulières de réseaux, telles que des sous-réseaux linéairement et uniformément déplacés, peuvent être converties en une contrainte sur la DPC, ce qui entraîne une plus grande relaxation des conditions d'unicité, une sensibilité réduite au bruit et un calcul souvent plus rapide.
%\end{ex}
\subsection{Décomposition de Tucker}
On rappelle que le principe de la TKD est de traiter un tenseur $ \mbf{X}\in\RItn $ comme une transformation multilinéaire d'un tenseur central (typiquement dense mais petit) $ \mathcal{G}\in\RItn $ par les matrices de facteurs 
$$ B^{(n)}=\left[\b_{1}^{(n)},\b_{2}^{(n)},\dots,\b_{R_{n}}^{(n)}\right],\: n=1,2,\dots,N, $$ donnée par
\begin{equation}\label{3TKD}
\mbf{X}=\sum_{r_{1}=1}^{R_{1}}\sum_{r_{2}=1}^{R_{2}}\cdots\sum_{r_{N}=1}^{R_{N}}g_{r_{1}r_{2}\cdots r_{N}}\left(\b_{r_{1}}^{(1)}\circ\b_{r_{2}}^{(2)}\circ\cdots\circ\b_{r_{N}}^{(N)}\right),
\end{equation}
ou de manière équivalente
\begin{eqnarray}
\mbf{X}&=&\mathcal{G}\times_{1}B^{(1)}\times_{2}B^{(2)}\cdots\times_{N}B^{(N)}\nonumber\\
&=&\left[\mathcal{G};B^{(1)},B^{(2)},\dots,B^{(N)}\right].\label{TKD8}
\end{eqnarray}

%\textbf{SVD multilinéaire}: 
%Les bases orthonormales dans une représentation contrainte de Tucker peuvent être obtenues via la SVD du tenseur matricialicisé mode-n
%\begin{equation*}
%X^{(n)}=U_{n}\Sigma_{n}V_{n}^{T}\left(\text{ c'est-à-dire,  }B^{(n)}=U_{n},\; n=1,2,\dots,N\right).
%\end{equation*}
% En raison de l'orthonormalité, le tenseur central correspondant devient
% \begin{equation}
% \mbf{S}=\mbf{X}\times_{1}U_{1}^{T}\times_{2}U_{2}^{T}\cdots\times_{N}U_{N}^{T}.
% \end{equation}
% 
% Ensuite, les valeurs singulières de $ X^{(n)} $ sont les normes de Frobenius des tranches correspondantes du tenseur noyau 
% \begin{equation*}
% 	\mbf{S}:\left(\Sigma_{n}\right)_{r_{n},r_{n}}=\begin{Vmatrix}
% 	\mbf{S}_{:,:,\dots,r_{n},:,\dots,:}
% 	\end{Vmatrix}_{F},
% \end{equation*} les tranches d'un même mode étant mutuellement orthogonales, c'est-à-dire que leurs produits internes sont nuls. Les colonnes de $ U_{n} $ peuvent donc être vues comme des vecteurs singuliers multilinéaires, tandis que les normes des tranches du noyau sont des valeurs singulières multilinéaires. Comme dans le cas des matrices, les valeurs singulières multilinéaires régissent le rang multilinéaire, tandis que les vecteurs singuliers multilinéaires permettent, pour chaque mode séparément, une interprétation comme dans l'ACP.\\
 
% \textbf{Approximation de rang multilinéaire faible }:
% Analogiquement à l'ACP, un tenseur de données à grande échelle $ \mbf{X} $ peut être approximé en écartant les vecteurs singuliers multilinéaires et les tranches du tenseur central qui correspondent à des petites valeurs singulières multilinéaires, c'est-à-dire par des SVD de matrices tronquées. L'approximation de rang multilinéaire faible est toujours bien posée; cependant, la troncature n'est pas nécessairement optimale au sens LS, bien qu'une bonne estimation puisse souvent être faite car l'erreur d'approximation correspond au degré de troncature. Lorsqu'il s'agit de trouver la meilleure approximation, les algorithmes de type ALS présentent les mêmes avantages et inconvénients que ceux utilisés pour la DPC. Des algorithmes basés sur l'optimisation et exploitant l'information du second ordre ont également été proposés.\\
 
 \textbf{Les contraintes et l'analyse en composantes multiples basée sur la TKD}: Outre l'orthogonalité, les contraintes qui peuvent aider à trouver des vecteurs de base uniques dans une représentation de Tucker comprennent \textit{l'indépendance statistique, la sparsité, le lissage et la non-négativité}. Les composants d'un tenseur de données ont rarement les mêmes propriétés dans ses modes, et pour une représentation physiquement significative, différentes contraintes peuvent être requises dans différents modes afin de correspondre aux propriétés des données en question. La \figref{fig:fig316} illustre le concept de l'analyse en composantes multivoie (ACVM) et sa flexibilité dans le choix des contraintes modales; une représentation de Tucker de l'ACVM tient naturellement compte de ces diversités dans les différents modes.
 
 \begin{figure}[!h]
 \encad{\centering
 	\includegraphics[width=0.7\linewidth]{"My figures/FIG316"}}
 	\caption{MWCA pour un tenseur d'ordre 3, en supposant que les composantes sont (a) principales et orthogonales dans le premier mode, (b) non négatives et éparses dans le deuxième mode, et (c) statistiquement indépendantes dans le troisième mode  \cite[p.147]{cichocki2015tensor}.}
 	\label{fig:fig316}
 \end{figure}
 
 \textbf{Autres applications}: La TKD peut être considérée comme une extension multilinéaire de l'ACP; elle généralise donc les techniques de sous-espace du signal, avec des applications telles que \textit{la classification, l'extraction de caractéristiques et la recherche harmonique basée sur le sous-espace}. Par exemple, une approximation de rang multilinéaire faible obtenue par la TKD peut donner un SNR plus élevé que le SNR du tenseur de données brutes d'origine, ce qui fait de la TKD un outil très naturel pour la compression et l'amélioration du signal.\\
 
 
 \textbf{Décompositions des termes en blocs}: Il a été déjà montré que la DPC est unique dans des conditions assez douces. Un autre avantage des tenseurs par rapport aux matrices est qu'il est même possible de relâcher la contrainte de rang 1 sur les termes, ce qui ouvre des possibilités totalement nouvelles, par exemple dans le cas de la BSS. Pour des raisons de clarté, en  considérant le cas du troisième ordre, dans lequel, en remplaçant les matrices de rang 1; $ \b_{r}^{(1)}\circ\b_{r}^{(2)}=\b_{r}^{(1)}\b_{r}^{(2)^{T}} $ dans \eqref{3.01} par des matrices de rang faible
 $ A_{r}B_{r}^{T} $, le tenseur $ \mbf{X} $ peut être représenté comme [\figref{fig:fig317}(a)]
 \begin{equation}\label{11}
 \mbf{X}=\sum_{r=1}^{R}\left(A_{r}B_{r}^{T}\right)\circ\mbf{c}_{r}.
 \end{equation}
 La \figref{fig:fig317}(b) montre qu'on peut même utiliser des termes qui ne sont tenus que d'avoir un rang multilinéaire faible pour donner
 \begin{equation}\label{12}
 \mbf{X}=\sum_{r=1}^{R}\mathcal{G}_{r}\times_{1}A_{r}\times_{2}B_{r}C_{r}.
 \end{equation}
 
  Ces décompositions en termes de blocs (BTD) dans \eqref{11} et \eqref{12} permettent de modéliser des composantes de signaux plus complexes que la DPC et sont uniques dans des conditions plus restrictives mais encore assez naturelles.

 \begin{figure}[!h]
 	\encad{\centering
 	\includegraphics[width=0.4\linewidth]{"My figures/FIG317"}}
 	\caption{Les BTD trouvent des composantes de données qui sont structurellement plus complexes que les termes de rang 1 dans le CPD. (a) Décomposition en termes de rang multilinéaire $ (L_{r},L_{r},1) $. (b) Décomposition en termes de rang multilinéaire $ (L_{r},M_{r},N_{r}) $.}
 	\label{fig:fig317}
 \end{figure}

\begin{ex}\label{exoTKD}
\emph{\\} 
Afin de comparer certaines approches standard et tensorielles pour la séparation de sources corrélées de courte durée, la BSS a été réalisée sur cinq mélanges linéaires des sources 

$ \s_{1}(t)=\sin(6\pi t) $ et $ \s_{2}(t)=\exp(10t)\sin(20\pi t) $, qui ont été contaminées par un bruit blanc gaussien, pour donner les mélanges $ X=AS+E\in\R^{5\times 60} $, où $ S(t)=[\s_{1}(t),\s_{2}(t)]^{T} $  et $ A\in\R^{5\times 2} $ était une matrice aléatoire dont les colonnes (vecteurs de mélange) satisfont $ \a_{1}^{T}\a_{2}=0.1 $, $ \parallel\a_{1} \parallel_{2}=\parallel\a_{2}\parallel_{2}=1$. L'onde sinusoïdale de $ 3 $Hz n'a pas complété une période complète sur les 60 échantillons de sorte que les deux sources avaient un degré de corrélation de $ \left(\mid \s_{1}^{T}\s_{2}\mid\right)/\left(\parallel\s_{1}\parallel_{2}\parallel\s_{2}\parallel_{2}\right)=0.35 $. Les approches tensorielles, CPD, TKD, et BTD ont employé un tenseur $ \mbf{X} $ de troisième ordre de taille $ 24 \times 37 \times  5 $ généré à partir de cinq matrices de Hankel dont les éléments obéissent à \begin{equation*}
\mbf{X}(i,j,k)=X(k,i+j-1).
\end{equation*}
L'erreur angulaire quadratique moyenne (SAE) a été utilisée comme mesure de performance. La \figref{fig:fig318} montre les résultats de la simulation, illustrant les points suivants.

\begin{list}{$ \ast $}{}
\item
L'ACP a échoué car les vecteurs de mélange n'étaient pas orthogonaux et les signaux sources étaient corrélés, ce qui enfreint les hypothèses de l'ACP.
\item
L'ICA [utilisant l'algorithme JADE (joint approximate diagonalization of eigenmatrices)] a échoué car les signaux n'étaient pas statistiquement indépendants, comme le suppose l'ICA.
\item
L'approximation du tenseur de faible rang via une CPD de rang $ 2 $ a été utilisée pour estimer A comme la matrice du troisième facteur, qui a ensuite été inversée pour donner les sources. La précision de la CPD a été compromise car les composantes du tenseur $ \mbf{X} $ ne peuvent pas être représentées par des termes de rang 1.
\item
L'approximation de rang multilinéaire faible via la TKD pour le rang multilinéaire $ (4, 4, 2) $ a permis de retrouver l'espace colonne de la matrice de mélange mais n'a pas pu trouver les vecteurs de mélange individuels en raison de la non-unicité de la TKD.
\item 
La BTD en termes de rang multilinéaire$ -(2, 2, 1) $ correspondait à la structure des données; il est remarquable que les sources aient été récupérées en utilisant aussi peu que six échantillons dans le cas sans bruit.
\end{list}

\begin{figure}[!h]
\encad{\centering
	\includegraphics[width=0.9\linewidth]{"My figures/FIG318"}}
	\caption{Séparation aveugle du mélange d'une onde sinusoïdale pure et d'une onde sinusoidale à modulation exponentielle à l'aide PCA, ICA, CPD, TKD et BTD. Les sources $ \s_{1} $ et $ \s_{2} $ sont corrélées et de courte durée; les symboles $ \hat{\s}_{1} $ et $ \hat{\s}_{2} $  désignent les sources estimées. (a)--(c) sources $ \s_{1}(t) $ et $ \s_{2}(t) $ et leurs estimations à l'aide de l'ACP, de l'ICA, de la CPD, de la TKD et de la BTD; (d) erreurs angulaires quadratiques moyennes (EAC) dans l'estimation des sources \cite[p.153]{cichocki2015tensor}.}
	\label{fig:fig318}
\end{figure}

\end{ex}

 

\subsection{Les données à grande échelle et la coupe de la dimensionnalité}

La taille des données tensorielles dépasse facilement la mémoire ou sature la capacité de traitement des ordinateurs standards; il est donc naturel de se demander comment les décompositions tensorielles peuvent être calculées si les dimensions des tenseurs dans tous ou certains modes sont grandes ou, pire encore, si l'ordre des tenseurs est élevé. L'expression "\textit{malédiction de la dimensionnalité}", dans un sens général, a été introduite par Bellman pour faire référence à divers goulots d'étranglement informatiques lorsque l'on traite des paramètres à haute dimension. Dans le contexte des tenseurs, la malédiction de la dimensionnalité fait référence au fait que le nombre d'éléments d'un tenseur d'ordre $ N $ $ (I\times I\times\cdots\times I) $, $ I^{N} $, évolue de manière exponentielle avec l'ordre $ N $ du tenseur. Par exemple, le nombre de valeurs d'une fonction discrétisée dans la 
\figref{fig:afig3a}(b) devient rapidement ingérable en termes de calculs et de stockage lorsque $ N $ augmente. En plus de leur utilisation standard (séparation du signal, amélioration, etc.), les décompositions tensorielles peuvent être employées de manière élégante dans ce contexte comme outils de représentation efficaces. Une des questions est de savoir quel type de décomposition tensorielle est approprié.

La \tabref{tab:tab3.1} essaie de présenter le coût de stockage pour quelques décompositions tensorielles données.
\begin{table}[htbp]
\centering
\caption{Coût de stockage des modèles tensoriels pour un tenseur  $ \mbf{X}\in\RItn $ d'ordre $ N $ pour lequel le besoin de stockage des données brutes est de $ \op(I^{N}) $. }
\label{tab:tab3.1}
\begin{tabular}{|cll|}
	\hline\hline
	Numéro&Décomposition&Stockage\\
	\hline
	(1)&Décomposition polyadique canonique (CPD)&$ \op(NIR) $\\
	(2)&Décomposition de Tucker (TKD)&$ \op(NIR+R^{N}) $\\
(3)&Décomposition en Trains de Tenseurs (TT)&$ \op(NIR^{2}) $\\
(4)&Train des Tenseurs Quantifié (QTT)&$\op\left(NR^{2}+\log_{2}(I)\right)  $\\
\hline
\hline
\end{tabular}
\end{table}


\subsection{Traitement efficace des données}
Si tous les calculs sont effectués sur une représentation CP et non sur le tenseur de données brutes lui-même, alors, au lieu des $ I^{N} $ entrées de données brutes originales, le nombre de paramètres dans une représentation CP se réduit à $ NIR $, qui s'échelonne linéairement en $ N $ (voir \tabref{tab:tab3.1}). Cela permet de contourner efficacement la malédiction de la dimensionnalité, tout en donnant la liberté de choisir le rang, $ R $, en fonction de la précision souhaitée; d'autre part, l'approximation CP peut impliquer des problèmes numériques.\\
La compression est également inhérente à la TKD, car elle réduit la taille d'un tenseur de données donné de l'original $ I^{N} $ à $ (NIR + R^{N}) $, présentant ainsi un taux de compression approximatif de $ (I/R)^{N} $. On peut alors bénéficier de l'approximation fiable et bien comprise au moyen de la matrice SVD; toutefois, celle-ci n'est utile que pour les faibles $ N $.
\subsection{Réseaux tensoriels}

Une façon numériquement fiable de s'attaquer à la malédiction de la dimensionnalité est de recourir à un concept issu de l'informatique scientifique et de la théorie de l'information quantique, appelé \textit{réseaux tensoriels}, qui représente un tenseur d'un ordre éventuellement très élevé sous la forme d'un ensemble de matrices et de noyaux tensoriels d'ordre faible (généralement d'ordre 3) faiblement interconnectés. Ces noyaux de faible dimension sont interconnectés via des contractions tensorielles pour fournir une représentation hautement compressée d'un tenseur de données. En outre, les algorithmes existants pour l'approximation d'un tenseur donné par un réseau de tenseurs ont de bonnes propriétés numériques, ce qui en fait une solution de choix.
\par Il est possible de contrôler l'erreur et d'obtenir la précision d'approximation souhaitée. Par exemple, les réseaux tensoriels permettent de représenter une large classe de fonctions multivariées discrétisées, même dans les cas où le nombre de valeurs de la fonction est supérieur au nombre d'atomes dans l'univers \cite[p.156]{cichocki2015tensor}.
\par Des exemples de réseaux tensoriels sont la TKD hiérarchique et les trains tensoriels (TT) (voir \figref{TT-formatFig}). Les TT sont également connus sous le nom d'états de produits matriciels et sont utilisés par les physiciens depuis plus de deux décennies.\\
 L'algorithme PARATREE a été développé dans le domaine du traitement du signal et suit une idée similaire; il utilise une représentation polyadique d'un tenseur de données (en un nombre de termes éventuellement non minimal), dont le calcul ne nécessite ensuite que la SVD matricielle.
\par Pour les données à très grande échelle qui présentent une structure bien définie, une approche encore plus radicale pour obtenir une représentation parcimonieuse peut être le concept de réseaux tensoriels quantifiés ou quantics (QTNs). Par exemple, un énorme vecteur $ \x\in\R^{I} $ avec $ I = 2^{L} $ éléments peut être quantifié et tensorisé
en un tenseur $ \mbf{X} $ $ (2\times 2\times\cdots \times 2) $  d'ordre L, comme l'illustre la \figref{fig:afig3a}(a). Si $ x $ est un signal exponentiel, $ x(k) = az^{k} $, alors $ \mbf{X} $ est un tenseur symétrique de rang-1 qui peut être représenté par deux paramètres: le facteur d'échelle $ a $ et le générateur $ z $ (voir relation \eqref{hankel}). Les termes non symétriques offrent d'autres possibilités, au-delà de la représentation de la somme des exponentielles par des tenseurs symétriques de bas rang. Les matrices et les tenseurs de grande taille peuvent être traités de la même manière. Par exemple, un tenseur $ \mbf{X}\in\RItn $ d'ordre N, avec $ I_{n} = q^{L_{n}} $, peut être quantifié dans tous les modes simultanément pour donner un  tenseur $ (q\times q\times\cdots\times q) $ quantifié d'ordre supérieur. Dans QTN, $ q $ est petit, typiquement $ q = 2,3,4 $, par exemple, le codage binaire ($ q=2 $) remodèle un tenseur d'ordre $ N $ avec $ (2^{L_{1}}\times 2^{L_{2}}\times\cdots\times2^{L_{N}}) $ éléments en un tenseur d'ordre $ (L_{1} + L_{2} +\cdots+ L_{N}) $ avec le même nombre d'éléments. La décomposition TT appliquée aux tenseurs quantifiés est appelée TT quantifié (QTT); et admet autant des variantes pour d'autres représentations de tenseurs. Dans le domaine du calcul scientifique, ces formats permettent ce que l'on appelle la \textit{supercompression}, c'est-à-dire une réduction logarithmique des besoins de stockage: 
$$ \op(I^{N})\longrightarrow\op(N\log_{q}(I)) .$$
%\subsection{Régression multivariée}
%
%La régression se réfère à la modélisation d'une ou plusieurs variables dépendantes (réponses), \textit{Y}, par un ensemble de données indépendantes (prédicteurs), \textit{X}. Dans le cas le plus simple de l'estimation de la moyenne quadratique conditionnelle (MSE), où $ \hat{y}=E(y|x) $, la réponse y est une combinaison linéaire des éléments du vecteur des prédicteurs $ \x $; pour les données multivariées, la régression linéaire multivariée (MLR) utilise un modèle matriciel, 
%\begin{equation}
% \mathrm{Y=XP+E},
%\end{equation}
% où P est la matrice des coefficients (chargements) et E est la matrice résiduelle. La solution MLR donne 
% $$\mathrm{P=\left(X^{T}X\right)^{-1}X^{T}Y }$$et implique l'inversion de la matrice des moments $ \mathrm{X^{T}X} $. Une technique courante pour stabiliser l'inverse de la matrice des moments $ \mathrm{X^{T}X} $ est la régression en composantes principales (PCR), qui utilise une approximation de faible rang de $ \mathrm{X} $.
% \subsection{Modélisation de la structure des données-LS partiel}
%Dans la régression multivariée stabilisante, la PCR n'utilise que les informations des variables \textit{X}, sans aucune rétroaction des variables \textit{Y}. L'idée derrière la méthode PLS (partial-LS) est de tenir compte de la structure des données en supposant que le système sous-jacent est régi par un petit nombre, $ R $, de variables latentes spécifiquement construites, appelées scores, qui sont partagées entre les variables \textit{X} et \textit{Y}; en estimant le nombre R, PLS fait un compromis entre l'ajustement de X et la prévision de Y. 
%
%\begin{figure}[!h]
%\encad{ \centering
%	\includegraphics[width=0.5\linewidth]{"My figures/FIG312"}}
%	\caption{Le modèle basique PLS effectue une approximation  séquentielle jointe de faible rang de la matrice des prédicteurs X et de la matrice des réponses Y de manière à partager (jusqu'à l'ambiguïté d'échelle) les composantes latentes-colonnes des matrices de score T et U. Les matrices P et Q sont les matrices de chargement pour les prédicteurs et les réponses, et E et F sont les matrices résiduelles correspondantes.}
%	\label{fig:fig312}
%\end{figure}
%
%
%La \figref{fig:fig312} montre que la procédure PLS: 
%\begin{enumerate}
%\item
%utilise l'analyse propre pour effectuer la contraction de la matrice de données $ \mathrm{X} $ en une matrice principale de scores de vecteurs propres $ T= [\mbf{t}_{1},\dots,\mbf{t}_{R}] $ de rang R et 
%\item 
%s'assure que les composantes $ \mbf{t}_{r} $ sont corrélées au maximum avec les composantes $ \mbf{u}_{r} $ dans l'approximation des réponses $ \mathrm{Y} $, ce qui est obtenu lorsque les $ \mbf{u}'_{r} $ sont des versions mises à l'échelle des $ \mbf{t}'_{r} $.
%\end{enumerate}
% Les variables \textit{Y} sont ensuite régressées sur la matrice $ U= [\mbf{u}_{1},\dots,\mbf{u}_{R}] $. Par conséquent, PLS est un modèle multivarié avec une capacité inférentielle qui vise à trouver une représentation de $ \mathrm{X} $ (ou une partie de $ \mathrm{X} $) qui est pertinente pour prédire $ \mathrm{Y} $, en utilisant le modèle
% \begin{eqnarray}
% \mathrm{X}&=&TP^{T}+E=\sum_{r=1}^{R}\mbf{t}_{r}\mbf{p}_{r}^{T}+E,\\
% \mathrm{Y}&=&UQ^{T}+F=\sum_{r=1}^{R}\mbf{u}_{r}\mbf{q}_{r}^{T}+F.
% \end{eqnarray}
% Les vecteurs de score $\mbf{t}_{r}  $ fournissent un ajustement LS des données $ \mathrm{X} $, alors que dans le même temps, la corrélation maximale entre les scores $ \mbf{t} $ et $ \mbf{u} $ assure un bon modèle prédictif pour les variables \textit{Y}. Les réponses prédites $ \mathrm{Y}_{new} $ sont alors obtenues à partir des nouvelles données $ \mathrm{X}_{new} $ et des chargements P et Q.\\
% 
% En pratique, les vecteurs de score $ \mbf{t}_{r} $, sont extraits séquentiellement, par une série de projections orthogonales suivies de la déflation de $\mathrm{X}  $. Puisque le rang de $ \mathrm{Y} $ n'est pas nécessairement diminué à chaque nouveau $ \mbf{t}_{r} $, nous pouvons continuer à dégonfler jusqu'à ce que le rang du bloc $ \mathrm{X} $ soit épuisé de manière à trouver un équilibre entre la précision de la prédiction et l'ordre du modèle.
% 
% Le concept PLS peut être généralisé aux tenseurs de la manière suivante:
% \begin{enumerate}
% \item
% \textit{Déploiement de données multidimensionnelles}
% 
%  Par exemple, les tenseurs $ \mbf{X}(I\times J\times K) $ et  $ \mbf{Y}(I\times M\times N) $ peuvent être aplatis en des longues matrices $ X(I\times JK) $ et $ Y(I\times MN) $ de manière à admettre la PLS matricielle (voir \figref{fig:fig312}). Cependant, un tel déploiement avant la PLS bilinéaire standard masque la structure des données multidimensionnelles et compromet l'interprétation des composantes latentes.
%  
%  \item
% \textit{Approximation des tenseurs de bas rang}
% 
%  La méthode dite N-PLS tente de trouver des vecteurs de score ayant une covariance maximale avec les variables de réponse, sous la contrainte que les tenseurs $ \mbf{X} $ et $ \mbf{Y} $ sont décomposés en une somme de tenseurs de rang-1.
%  \item
%   \textit{Une approximation de type BTD}
%   
%    Comme dans le modèle PLS d'ordre supérieur (HOPLS) illustré à la \figref{fig:fig313}, l'utilisation de termes de bloc dans le modèle HOPLS lui confère une flexibilité supplémentaire, ainsi qu'une analyse plus significative sur le plan physique que les modèles PLS dépliants et N-PLS.
%    
%    \begin{figure}[!h]
%    \encad{	\centering
%    	\includegraphics[width=0.3\linewidth]{"My figures/FIG313"}}
%    	\caption{Principe de HOPLS pour les tenseurs tridimensionnels. Les tenseurs noyaux  $ \mathcal{G}_{X} $ et $ \mathcal{G}_{Y} $ sont diagonaux par blocs. La structure de type BTD permet de modéliser des composantes générales qui sont fortement corrélées dans le premier mode}
%    	\label{fig:fig313}
%    \end{figure}
%    
%     \end{enumerate}
% 
% Le principe de HOPLS peut être formalisé comme un ensemble de décompositions approximatives séquentielles du tenseur indépendant $ \mbf{X}\in\RItn $ et le tenseur dépendant $ \mbf{Y}\in\R^{J_{1}\times J_{2}\times\cdots\times J_{M}} $ (avec $ I_{1} = J_{1} $) de manière à assurer une similarité (corrélation) maximale entre les scores $ \mbf{t}_{r} $ et $ \mbf{u}_{r} $ dans les matrices T et U, sur la base des critères suivants
% \begin{eqnarray}
% \mbf{X}&\approx&\sum_{r=1}^{R}\mathcal{G}_{X}^{(r)}\times_{1}\mbf{t}_{r}\times_{2}P_{r}^{(1)}\cdots\times_{N}P_{r}^{(N-1)}\\
%  \mbf{Y}&\approx&\sum_{r=1}^{R}\mathcal{G}_{Y}^{(r)}\times_{1}\mbf{u}_{r}\times_{2}Q_{r}^{(1)}\cdots\times_{N}Q_{r}^{(M-1)}.
% \end{eqnarray}
%Un certain nombre de problèmes d'analyse de données peuvent être reformulés en tant qu'analyse de régression ou de similarité [analyse de la variance (ANOVA), modélisation des moyennes mobiles autorégressives (ARMA), analyse discriminante linéaire (LDA) et analyse de corrélation canonique (CCA)], de sorte que les solutions PLS matricielles et tensorielles peuvent être généralisées à l'analyse exploratoire des données.
%\begin{ex}
%\emph{\\}
%Le pouvoir prédictif de la PLS basée sur les tenseurs est illustré sur un exemple réel de prédiction de la trajectoire du mouvement du bras à partir de l'électrocorticogramme (ECoG). La \figref{fig:fig314}(a) illustre la configuration expérimentale, dans laquelle le mouvement du bras en 3D d'un singe a été capturé par un système de capture de mouvement optique avec des marqueurs réfléchissants fixés à l'épaule, au coude, au poignet et à la main gauches; construisent naturellement un tenseur $ \mbf{X} $ d'ordre $ 4 $ $ (temps\times canal\_ no\times époque\_ longueur\times fréquence) $ tandis que les trajectoires de mouvement des quatre marqueurs (réponse) peuvent être représentées comme un tenseur $ \mbf{Y} $ d'ordre $ 3 $ $ (temps\times 3D\_ marqueur\_ position\times marqueur nà) $. L'objectif de l'étape de formation est d'identifier les paramètres HOPLS:
%$ \mathcal{G}_{X}^{(r)}, \mathcal{G}_{Y}^{(r)}, P_{r}^{(n)},Q_{r}^{(n)} $ (voir \figref{fig:fig313}). Dans la phase de test, les trajectoires de mouvement, $ \mbf{Y}^{*} $, pour les nouvelles données ECoG, $ \mbf{X}^{*} $, sont prédites par des projections multilinéaires:
%\begin{enumerate}
%\item
%les nouveaux scores, $ \mbf{t}_{r}^{*} $, sont trouvés à partir des nouvelles données, $ \mbf{X}^{*} $, et des paramètres du modèle existant:
%$ \mathcal{G}_{X}^{(r)},P_{r}^{(1)},P_{r}^{(2)},P_{r}^{(3)} $, et
%\item
%la trajectoire prédite est calculée comme suit
%\begin{equation}
%\mbf{Y}^{*}\approx\sum_{r=1}^{R}\mathcal{G}_{Y}^{(r)}\times_{1}\mbf{t}_{r}^{*}\times_{2}Q_{r}^{(1)}\times_{3}Q_{r}^{(2)}\times_{4}Q_{r}^{(3)}.
%\end{equation} Dans les simulations, le PLS standard a été appliqué de la même manière aux tenseurs dépliés.
%\end{enumerate}
% 
%La \figref{fig:fig314}(c) montre que, bien que la PLS standard ait été capable de prédire le mouvement correspondant à chaque marqueur individuellement, une telle prédiction est assez grossière car la PLS bidirectionnelle ne tient pas compte de manière adéquate de l'information mutuelle entre les quatre marqueurs. L'amélioration des performances de prédiction de la PLS basée sur le BTD [la ligne rouge de la \figref{fig:fig314}(c)] est donc attribuée à sa capacité à modéliser les interactions entre les composantes latentes complexes des prédicteurs et des réponses.
%\end{ex}
%\newpage
%\begin{figure}[!h]
%\encad{ \centering
%	\includegraphics[width=0.7\linewidth]{"My figures/FIG314"}}
%	\caption{La prédiction du mouvement du bras à partir des réponses électriques du cerveau. (a) La configuration de l'expérience. (b) La construction des tenseurs de données et de réponses et l'entrainement. (c) Le nouveau tenseur de données (en bas) et les trajectoires prédites du mouvement du bras en 3-D; (coordonnées XYZ) obtenues par HOPLS basé sur le tenseur et PLS basé sur la matrice standard (en haut) }
%	\label{fig:fig314}
%\end{figure}
%
%\subsection{Analyse en composantes multivoies liées et fusion de données tensorielles}
%La fusion de données concerne l'analyse conjointe d'un ensemble d'ensembles de données, tels que des vues multiples d'un phénomène particulier, où certaines parties de la scène peuvent être visibles dans un seul ou quelques ensembles de données. On peut citer comme exemples la fusion d'images visuelles et thermiques dans des conditions de faible visibilité et l'analyse de signaux électrophysiologiques humains en réponse à un certain stimulus mais provenant de sujets et d'essais différents; ces données sont naturellement analysées ensemble au moyen de factorisations matricielles/tensorielles. La nature couplée de l'analyse de ces multiples ensembles de données nous permet de tenir compte des facteurs communs à tous les ensembles de données et, en même temps, de garantir que les composantes individuelles ne sont pas partagées (par exemple, les processus indépendants des excitations ou des stimuli/tâches).\\
%L'analyse en composantes multivoie liée (LMWCA), illustrée à la \figref{fig:fig315}, effectue une telle décomposition en facteurs partagés et individuels et est formulée comme un ensemble de TKD conjointes approximatives d'un ensemble de tenseurs de données $ \mbf{X}^{(k)} \in\RItn$ $ (k=1,2,\dots,K) $
%\begin{equation}
%\mbf{X}^{(k)}\cong\mathcal{G}^{(k)}\times_{1}B^{(1,k)}\times_{2}B^{(2,k)}\cdots\times_{N}B^{(N,k)},
%\end{equation}
%où chaque matrice de facteurs $ B^{(n,k)}=\left[B_{C}^{(n)},B_{I}^{(n,k)}\right] $ a
%
%\begin{enumerate}
%\item
%des composantes $ B_{C}^{(n)}\in\R^{I_{n}\times C_{n}} $ (avec $ 0\leq  C_{n}\leq  R_{n} $) qui sont communes (c'est-à-dire maximalement corrélées) à tous les tenseurs et 
%\item des composantes $ B_{I}^{(n,k)}\in\R^{I_{n}\times (R_{n}-C_{n})} $ qui sont spécifiques aux tenseurs.
%\end{enumerate}
%L'objectif est d'estimer les composantes communes $ B_{C}^{(n)} $, les composantes individuelles $ B_{I}^{(n,k)} $, et, via les tenseurs de base $ \mathcal{G}^{(k)} $, leurs interactions mutuelles. Comme dans la MWCA, des contraintes peuvent être imposées pour correspondre aux propriétés des données. Cela permet un cadre plus général et plus souple que l'ICA de groupe et l'analyse vectorielle indépendante, qui effectuent également une analyse liée de plusieurs ensembles de données mais supposent que
%\begin{enumerate}
%\item
% il n'existe que des composantes communes et 
% \item les variables latentes correspondantes sont statistiquement indépendantes.
%\end{enumerate}
%Ces deux hypothèses sont assez strictes et limitatives. Comme alternative à la TKD, les décompositions tensorielles couplées peuvent être de type polyadique ou même à terme de bloc.
%
%
%\begin{figure}[!h]
%\encad{	\centering
%	\includegraphics[width=0.3\linewidth]{"My figures/FIG315"}}
%	\caption{TKD couplé pour LMWCA. Les tenseurs de données ont des composantes partagées et individuelles. Des contraintes telles que l'orthogonalité, l'indépendance statistique, la rareté et la non-négativité peuvent être imposées le cas échéant.}
%	\label{fig:fig315}
%\end{figure}
%\subsection{Logiciel}
%Les ressources logicielles actuellement disponibles pour les décompositions tensorielles incluent:
%\begin{list}{$ \ast $}{}
%\item \verb|tensor toolbox|, un cadre polyvalent pour les opérations de base sur les tenseurs épars et denses, y compris les formats CPD et Tucker.
%\item
%\verb|TDALAB| et \verb|TENSORBOX|, qui fournissent une interface conviviale et des algorithmes avancés pour la DPC, le TKD non négatif et le MWCA.
%\item \verb|Tensorlab toolbox| s'appuie sur le cadre d'optimisation complexe et propose des algorithmes numériques pour calculer le CPD, le BTD et le TKD; la boîte à outils comprend une bibliothèque de contraintes (par exemple, la non négativité et l'orthogonalité) et la possibilité de combiner et de factoriser conjointement des tenseurs denses, épars et incomplets.
%\item
% \verb|N-way toolbox|, qui comprend la CPD (contrainte), la TKD et la PLS dans le contexte d'applications chimiométriques; beaucoup de ces méthodes peuvent gérer les contraintes (par exemple, la non-négativité et l'orthogonalité) et les éléments manquants.
% \item
%\verb|TT toolbox|, \verb|Hierarchical Tucker toolbox| et \verb|Tensor Calculus library| fournissent des outils tensoriels pour le calcul scientifique.
%\item etc.
%\end{list}
%\newpage
\vspace{3cm}

\section*{Conclusion partielle}
Ce chapitre consistait à présenter en grande partie l'apport des décompositions tensorielles dans le traitement multidimensionnel du signal. Pour ce faire, nous avons  présenté d'une part un état de l'art de l'analyse multidimensionnelle. Et d'autre part, le traitement multidimensionnel du signal. Dans le traitement des signaux multidimensionnels, une attention particulière a été portée sur les décompositions CPD et TKD, mais, selon les auteurs (\cite{MUTIfrance} et \cite{cichocki2015tensor}), la TKD paraît un outil très naturel pour la compression et l'amélioration du signal (comme on peut le voir dans l'\exoref{exoTKD}). La  taille des données tensorielles dépasse facilement la mémoire ou sature la capacité de traitement des ordinateurs standards; il est donc naturel de se demander comment les décompositions tensorielles peuvent être calculées si les dimensions des tenseurs dans tous ou certains modes sont grandes ou, pire encore, si l'ordre des tenseurs est élevé. L'expression "\textit{malédiction de la dimensionnalité}" inventée par Bellman fait face et, une façon numériquement fiable de s'attaquer à ce fléau de la dimensionnalité est de recourir à un concept issu de l'informatique scientifique et de la théorie de l'information quantique, appelé \textit{réseaux tensoriels}, qui représente un tenseur d'un ordre éventuellement très élevé sous la forme d'un ensemble de matrices et de noyaux tensoriels d'ordre faible (généralement d'ordre 3) faiblement inter-connectés; c'est ainsi qu'on peut recourir aux décompositions tensorielles et, en particulier, la décomposition hiérarchique de Tucker et  le format train  des tenseurs (TT).\\
 Certains exemples intéressants ont été présentés de la part des auteurs et nécessitaient des implémentations numériques de notre part. Nous en avons présenté théoriquement un échantillon sans trop de détails, nous aurions dû faire autrement mais, hélas, les conditions ne l'ont pas favorisé et ce n'était pas la priorité de ce travail!!!. C'est pourquoi nous supposons que les prochains travaux relatifs à cette thématique dans le département ne laisseront pas inaperçue cette préoccupation.
 
{\newgeometry{left=1.7cm,bottom=0.7cm}
\chapter*{Conclusion générale}
\addcontentsline{toc}{chapter}{Conclusion générale}
Ce travail a porté sur les traitements des tenseurs numériques. Pour y parvenir, nous l'avons reparti en trois principaux chapitres.
\par D'abord, dans le chapitre \ref{chap1}, il a été question de présenter les fondements algébriques des espaces tensoriels. Nous avons dans un premier temps rappelé les notions relatives aux espaces vectoriels qui sont jugées incontournables à la construction des espaces tensoriels et avons dans un autre temps explicité les résultats caractéristiques de ces derniers.
\par Ensuite, dans le chapitre \ref{chap2}, nous avons présenté les représentations et traitements des tenseurs numériques; ces derniers sont considérés comme des tableaux multidimensionnels ou \textit{hypermatrices} et permettent de représenter des applications multilinéaires et des polynômes multivariables. Ils ont été visuellement représentés en utilisant un point avec des lignes qui représentent les différentes directions du tenseur ou des \textit{boîtes} et des \textit{tranches}. Numériquement, ils peuvent être représentés suivant des formats; c'est ainsi que nous avons présenté le format plein, le format polyadique canonique, le format de Tucker, le format hiérarchique de Tucker et le format en train tensoriel. L'algèbre linéaire constitue le socle du calcul tensoriel sauf que certaines notions ne se généralisent pas de manière unique en dimension élevée, le rang tensoriel en est un paradigme.
 \par Enfin, dans le chapitre \ref{chap3}, nous avons présenté le traitement multidimensionnel du signal par décomposition tensorielle. Il s'agissait surtout de montrer l'apport significatif des décompositions tensorielles dans le traitement des signaux multidimensionnels. Après présentation d'une approche de taxonomie de construction d'un tenseur à partir des données brutes, on a montré que l'on peut recourir au calcul tensoriel pour le traitement des signaux. Cependant, il a été dit que la complexité calculatoire de la plupart des algorithmes en dimension $d>2$ croît de manière exponentielle en fonction de la dimension $d$, une façon numériquement fiable de s'attaquer à ce fléau de la dimensionnalité est de recourir à un concept issu de l'informatique scientifique et de la théorie de l'information quantique, appelé \textit{réseaux tensoriels}, qui représente un tenseur d'un ordre éventuellement très élevé sous la forme d'un ensemble de matrices et de noyaux tensoriels d'ordre faible (généralement d'ordre 3) faiblement inter-connectés. Dans ce cas, on peut recourir aux décompositions tensorielles. En particulier, la décomposition hiérarchique de Tucker et le format train  des tenseurs (TT) sont le choix idéal pour faire face à ce fléau. De manière spécifique, les auteurs (\cite{MUTIfrance} et \cite{cichocki2015tensor}) ont montré que la TKD paraît un outil très naturel pour la compression et l'amélioration du signal.
\par Les résultats sus-cités traduisent que nos objectifs ont été atteints et constituent notre contribution dans ce travail. Nous ne prétendons
pas avoir traité exhaustivement cette thématique. Une brèche est ouverte
pour ceux ou celles qui sont intéressé(e)s par ce domaine. L'étude pourrait
par exemple considérer l'application des tenseurs numériques au traitement  des images, la reconnaissance de visage basée sur l’analyse multidimensionnelle, l'étude des tenseurs topologiques, etc.
\par Ce travail  est certainement perfectible. Aucune œuvre humaine n’échappe
à cette règle, aussi meilleure qu’elle soit en apparence ou en quintessence. Nous vous
remercions anticipativement pour les critiques, remarques et suggestions constructives susceptibles de conduire à son amélioration.



}





%\listof{algorithm}{\textsc{Liste des algorithmes}}
	
	
	%\listoffigures
	
	%\listoftables
	%\nocite{*}
	

%\theoremlisttype{opt}
%\listtheorems{defi}




\nocite{*}
\bibliography{EMMANUELAOUT}
\end{document}
